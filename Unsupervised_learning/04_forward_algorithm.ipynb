{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07342709",
   "metadata": {},
   "source": [
    "# 4: Forward algorithm\n",
    "\n",
    "In this notebook we will implement the forward algorithm together. \n",
    "\n",
    "## 1. Loop-based forward\n",
    "\n",
    "We will first implement the algorithm using loops, and then explore vectorized implementations which are more efficient.\n",
    "\n",
    "For this example, we won't train the HMM. We will simply use mad up values for initial, emission and transition probabilities.\n",
    "\n",
    "Our `SimpleHMM` recognizes only three words `\"I\"`, `\"am\"` and `\"Sam\"`. It has three states (or tags) `\"PRP\"`, `\"NN\"` and `\"VBN\"`. \n",
    "\n",
    "Initial probabilities:\n",
    "\n",
    "```\n",
    "PRP   NN   VBN\n",
    "0.9   0.05 0.05\n",
    "```\n",
    "\n",
    "Emission probabilities:\n",
    "\n",
    "```\n",
    "     PRP   NN    VBN\n",
    "I    0.95  0.025 0.025\n",
    "am   0.025 0.025 0.95\n",
    "Sam  0.025 0.95  0.025\n",
    "```\n",
    "\n",
    "Transition probabilities:\n",
    "\n",
    "```\n",
    "     PRP    NN    VBN\n",
    "PRP  0.05   0.05  0.9\n",
    "NN   0.05   0.05  0.9\n",
    "VBN  0.05   0.9   0.05\n",
    "```\n",
    "\n",
    "The forward algorithm takes a sentence as input and returns its total probability. We will need to:\n",
    "\n",
    "1. Initialize a trellis (np.array of dimension |tag set size| x |sentence length|)\n",
    "1. Initialize the first column of alpha values in the trellis\n",
    "1. Iteratively fill all the rest of the columns in the trellis\n",
    "1. Return the sum of the probabilities in the last column.\n",
    "\n",
    "Remember that for the sentence \"I am Sam\":\n",
    "\n",
    "$$\\alpha_{2}(NN) = \\sum_{t} \\alpha_{1}(t) \\cdot tr_{t,NN} \\cdot em_{NN,Sam}$$\n",
    "\n",
    "**Indexing in our code starts at 0, which is why $\\alpha_2$ refers to the last (third) column in the trellis!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e6dbc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Definition of vocabulary and tagset.\n",
    "TAGS = [\"PRP\", \"NN\", \"VBN\"]\n",
    "TAGCOUNT = 3\n",
    "VOCAB = {\"I\":0, \"am\": 1, \"Sam\":2}\n",
    "\n",
    "class SimpleHMM:\n",
    "    def __init__(self):\n",
    "        self.init_prob = np.array([0.9, 0.05, 0.05]) # 90% chance to start in the PRON state\n",
    "\n",
    "        self.em_prob = np.array([[0.95,  0.025, 0.025],\n",
    "                                 [0.025, 0.025, 0.95],\n",
    "                                 [0.025, 0.95,  0.025]])\n",
    "\n",
    "        self.tr_prob = np.array([[0.05, 0.05, 0.9],\n",
    "                                 [0.05, 0.05, 0.9],\n",
    "                                 [0.05, 0.9,  0.05]])\n",
    "\n",
    "    def forward(self, s):\n",
    "        s = [VOCAB[w] for w in s]\n",
    "        \n",
    "        trellis = np.zeros((len(TAGS), len(s)))\n",
    "\n",
    "        # Fill in the first column of the trellis\n",
    "        for t in range(TAGCOUNT):\n",
    "            alpha_0_tag = self.init_prob[t] * self.em_prob[t,s[0]]\n",
    "            trellis[t,0] = alpha_0_tag\n",
    "\n",
    "        # Compute alpha n based on alpha n-1\n",
    "        for i in range(1,len(s)):\n",
    "            for t1 in range(TAGCOUNT):\n",
    "                alpha_n_t1 = 0\n",
    "                for t2 in range(TAGCOUNT):\n",
    "                    alpha_n_1_t2 = trellis[t2, i-1]\n",
    "                    tr_prob = self.tr_prob[t2, t1]\n",
    "                    em_prob = self.em_prob[t1,s[i]] \n",
    "                    alpha_n_t1 += alpha_n_1_t2 * tr_prob * em_prob\n",
    "                trellis[t1,i] = alpha_n_t1\n",
    "        \n",
    "        total_prob = 0\n",
    "        for t in range(len(TAGS)):      # sum up the last column\n",
    "            total_prob += trellis[t,len(s)-1]\n",
    "\n",
    "        return total_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb35a686",
   "metadata": {},
   "source": [
    "Let's initialize a `SimpleHMM` and compute the total probability of the sentences \"I am Sam\" and \"am I Sam\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "230a1de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6279759394531248\n",
      "0.0008285410156250001\n"
     ]
    }
   ],
   "source": [
    "hmm = SimpleHMM()\n",
    "\n",
    "print(hmm.forward(\"I am Sam\".split()))\n",
    "print(hmm.forward(\"am I Sam\".split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be70754",
   "metadata": {},
   "source": [
    "Loops are actually a very slow way to process Numpy arrays. Using the predefined operations like addition and dot (or inner) product can substantially speed up your code. \n",
    "\n",
    "Let's try to add the values in a 10,000,000-dimensional first vector using a loop and then using the built-in `sum()` member function. We'll time the execution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04fd6b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000000\n",
      "CPU times: user 1.07 s, sys: 45.7 ms, total: 1.11 s\n",
      "Wall time: 1.17 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "a = np.ones((1,10000000))\n",
    "\n",
    "tot = 0\n",
    "\n",
    "for i in range(10000000):\n",
    "    tot += 1\n",
    "    \n",
    "print(tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abb739e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000000.0\n",
      "CPU times: user 34 ms, sys: 47 ms, total: 80.9 ms\n",
      "Wall time: 93.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "a = np.ones((1,10000000))\n",
    "print(a.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e25d95c",
   "metadata": {},
   "source": [
    "## 2. Vectorizing the computation of $alpha_n(tag)$\n",
    "\n",
    "We can speed the forward algorithm up quite a bit by noticing that all the `alpha_n` probabilities can be computed at once using the following formula:\n",
    "\n",
    "$$\\alpha_{2}(NN) = \\alpha_1 \\cdot tr_{:,NN} \\cdot em_{NN, Sam}$$\n",
    "\n",
    "$\\alpha_1$ refers to the vector containing all the `alpha_1(tag)` probabilities. This is in fact column `1` of our trellis. `tr_{:,NN}` refers to the transition probabilites from any tag to the tag `NN` which is column 2 of our transition probability matrix (2 because the index of `NN` in `TAGS` is 2):\n",
    "\n",
    "$$\\alpha_{2}(NN) =\\begin{bmatrix} \\alpha_{1}(PRP) & \\alpha_{1}(NN) & \\alpha_{1}(VBN)\\end{bmatrix} \\cdot \\begin{bmatrix}tr_{PRP,NN} \\\\ tr_{NN,NN} \\\\ tr_{VBN,NN}\\end{bmatrix} \\cdot em_{NN, Sam}$$\n",
    "\n",
    "Notice that the code is much shrter than in the loop-based implementation.\n",
    "\n",
    "Let's spend some time computing and making sure that this gives us the correct result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5e06387",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorizedHMM:\n",
    "    def __init__(self):\n",
    "        self.init_prob = np.array([0.9, 0.05, 0.05]) # 90% chance to start in the PRON state\n",
    "\n",
    "        self.em_prob = np.array([[0.95,  0.025, 0.025],\n",
    "                                 [0.025, 0.025, 0.95],\n",
    "                                 [0.025, 0.95,  0.025]])\n",
    "\n",
    "        self.tr_prob = np.array([[0.05, 0.05, 0.9],\n",
    "                                 [0.05, 0.05, 0.9],\n",
    "                                 [0.05, 0.9,  0.05]])\n",
    "\n",
    "    def forward(self, s):\n",
    "        s = [VOCAB[w] for w in s]\n",
    "        \n",
    "        trellis = np.zeros((len(TAGS), len(s)))\n",
    "        \n",
    "        trellis[:,0] = self.init_prob * self.em_prob[:,s[0]]\n",
    "\n",
    "        for i in range(1,len(s)):\n",
    "            alpha_n_1 = trellis[:,i-1]\n",
    "            for t in range(TAGCOUNT):\n",
    "                # @ refers to the inner or dot product of two arrays.\n",
    "                alpha_n_t = (alpha_n_1 @ self.tr_prob[:,t]) * self.em_prob[t,s[i]]  \n",
    "                # self.em_prob[t,s[i]]: given the word s[i], the probability of emitting the tag t\n",
    "                trellis[t,i] = alpha_n_t\n",
    "\n",
    "        return trellis[:,len(s)-1].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59f475a",
   "metadata": {},
   "source": [
    "Let's initialize a `VectorizedHMM` and compute the total probability of the sentences \"I am Sam\" and \"am I Sam\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfb0d96b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6279759394531249\n",
      "0.000828541015625\n"
     ]
    }
   ],
   "source": [
    "hmm = VectorizedHMM()\n",
    "\n",
    "print(hmm.forward(\"I am Sam\".split()))\n",
    "print(hmm.forward(\"am I Sam\".split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c139c293",
   "metadata": {},
   "source": [
    "## 3. Vectorizing the computation of $\\alpha_n$\n",
    "\n",
    "We can actually optimize this even further by noticing that all the $\\alpha_n(tag)$ probabilites can be computed at once. \n",
    "\n",
    "First, a bit of notation. The operator $\\odot$ is the pointwise product of two matrices/vectors:\n",
    "\n",
    "$$[x_1\\ x_3\\ x_3] \\odot [y_1\\ y_2\\ y_3] = [x_1\\cdot y_1\\ x_2\\cdot y_2\\ x_3\\cdot y_3]$$\n",
    "\n",
    "For two numpy arrays `x` and `y`, the pointwise product is just `x * y`.\n",
    "\n",
    "Now, we can compute the entire 3rd column of the trellis $\\alpha_2$ in one step:\n",
    "\n",
    "$$\\alpha_{2} = \\alpha_1 \\cdot tr \\cdot em_{:, Sam}$$\n",
    "\n",
    "$$=\\Big(\\begin{bmatrix} \\alpha_{1}(PRP) & \\alpha_{1}(NN) & \\alpha_{1}(VBN)\\end{bmatrix} \\begin{bmatrix}tr_{PRP,PRP} & tr_{PRP,NN} & tr_{PRP,VBN}\\\\ tr_{NN,PRP} & tr_{NN,NN} & tr_{NN,VBN}\\\\ tr_{VBN,PRP} & tr_{VBN,NN} & tr_{VBN,VBN}\\end{bmatrix}\\Big) \\odot \\begin{bmatrix}e_{PRP,Sam} & e_{NN,Sam} & e_{VBN,Sam}\\end{bmatrix}$$\n",
    "\n",
    "Let's spend some time computing and making sure that this gives us the correct result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "222ed6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyVectorizedHMM:\n",
    "    def __init__(self):\n",
    "        self.tags = {\"PRP\":0, \"NN\":1, \"VBN\":2}\n",
    "        self.vocabulary = {\"I\":0, \"am\": 1, \"Sam\":2}\n",
    "        \n",
    "        self.init_prob = np.array([0.9, 0.05, 0.05]) # 90% chance to start in the PRON state\n",
    "\n",
    "        self.em_prob = np.array([[0.95,  0.025, 0.025],\n",
    "                                 [0.025, 0.025, 0.95],\n",
    "                                 [0.025, 0.95,  0.025]])\n",
    "\n",
    "        self.tr_prob = np.array([[0.05, 0.05, 0.9],\n",
    "                                 [0.05, 0.05, 0.9],\n",
    "                                 [0.05, 0.9,  0.05]])\n",
    "\n",
    "    def forward(self, s):\n",
    "        s = [self.vocabulary[w] for w in s]\n",
    "        \n",
    "        trellis = np.zeros((len(self.tags), len(s)))\n",
    "        \n",
    "        trellis[:,0] = self.init_prob * self.em_prob[:,s[0]]\n",
    "\n",
    "        for i in range(1,len(s)):\n",
    "            alpha_n_1 = trellis[:,i-1]\n",
    "            # @ refers to the inner or dot product of two arrays. \n",
    "            # When the arrays are matrices, this is just regular matrix product.\n",
    "            alpha_n = (alpha_n_1 @ self.tr_prob) * self.em_prob[:,s[i]]\n",
    "            # this time self.em_prob[:,s[i]] gives all the column of a given tag\n",
    "            trellis[:,i] = alpha_n   # assign the whole column to trellis\n",
    "\n",
    "        return trellis[:,len(s)-1].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322c5983",
   "metadata": {},
   "source": [
    "Let's initialize a `FullyVectorizedHMM` and compute the total probability of the sentences \"I am Sam\" and \"am I Sam\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb366f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6279759394531249\n",
      "0.000828541015625\n"
     ]
    }
   ],
   "source": [
    "hmm = FullyVectorizedHMM()\n",
    "\n",
    "print(hmm.forward(\"I am Sam\".split()))\n",
    "print(hmm.forward(\"am I Sam\".split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c698ab59",
   "metadata": {},
   "source": [
    "## 4. Using log-probabilities\n",
    "\n",
    "Very often we want to convert all probabilities to log-probabilities in initial, emission and transition matrices.\n",
    "\n",
    "We need to remember that product of probabilites $p\\cdot q$ corresponds to addition of log-probabilities $\\log p + \\log q$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1ac290c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.321928094887362\n",
      "-3.321928094887362\n"
     ]
    }
   ],
   "source": [
    "p = 0.5\n",
    "q = 0.2\n",
    "\n",
    "print(np.log2(p) + np.log2(q))\n",
    "print(np.log2(p*q))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd684be",
   "metadata": {},
   "source": [
    "Going over to log-probabilities does not influence maximization which is what we'll need to do in the Viterbi algorithm: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f614385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8 0.7]\n",
      "[1 0]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[0.1, 0.8],[0.7,0.2]])\n",
    "\n",
    "print(a.max(axis=1))\n",
    "print(a.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12ede81a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.32192809 -0.51457317]\n",
      "[1 0]\n"
     ]
    }
   ],
   "source": [
    "b = np.log2(a)\n",
    "print(b.max(axis=1))\n",
    "print(b.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7b7f535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8 0.7]\n"
     ]
    }
   ],
   "source": [
    "print(np.exp2(b.max(axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e821f90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
