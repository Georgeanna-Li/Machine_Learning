{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Boolean IR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* We will be learning how to use the open-source [Elasticsearch](https://www.elastic.co/) search engine, which is turn based on [Lucene](https://lucene.apache.org/).\n",
    "* But the underlying methods (often unsupervised) are worth knowing and can be implemented efficiently in Python for small-to-medium sized applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up\n",
    "\n",
    "ElasticSearch is a free java-based IR system. You'll need to install an recent version of [Java](https://java.com/en/download/manual.jsp), and get the latest version (v7+) of [Elasticsearch](https://www.elastic.co/downloads/elasticsearch) for your operating system. After you have unzipped/installed Elasticsearch (I recommend downloading an archived version rather than installing), run `elasticsearch` in the `bin directory` to start the Elasticsearch server. \n",
    "\n",
    "In Python, you will need the elasticsearch_dsl package.\n",
    "\n",
    "`!pip install elasticsearch_dsl`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up an Elasticsearch index\n",
    "\n",
    "Elasticsearch is written in Java, but it runs as an REST API that we can access using Python. \n",
    "\n",
    "The code below assumes that the Elasticsearch API is running at the default port.\n",
    "\n",
    "There are actually two packages for Elasticsearch, one which provides high-level Pythonic access to Elasticsearch, ([elasticsearch_dsl](https://elasticsearch-dsl.readthedocs.io/en/latest/index.html)), and one which provides lower-level, more flexible access ([elasticsearch](https://elasticsearch-py.readthedocs.io/en/7.10.0/index.html)). \n",
    "\n",
    "We'll mostly be using `elasticsearch_dsl` here. We'll start by setting up the connection to the Java API, and testing to see if it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.9/site-packages/elasticsearch/connection/base.py:200: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.16/security-minimal-setup.html to enable security.\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'name': 'seventypercent.local',\n",
       " 'cluster_name': 'elasticsearch',\n",
       " 'cluster_uuid': 'QvrhnTD5SEWUK7gx3wVCBA',\n",
       " 'version': {'number': '7.16.3',\n",
       "  'build_flavor': 'default',\n",
       "  'build_type': 'tar',\n",
       "  'build_hash': '4e6e4eab2297e949ec994e688dad46290d018022',\n",
       "  'build_date': '2022-01-06T23:43:02.825887787Z',\n",
       "  'build_snapshot': False,\n",
       "  'lucene_version': '8.10.1',\n",
       "  'minimum_wire_compatibility_version': '6.8.0',\n",
       "  'minimum_index_compatibility_version': '6.0.0-beta1'},\n",
       " 'tagline': 'You Know, for Search'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from elasticsearch_dsl.connections import connections\n",
    "\n",
    "connections.create_connection(hosts=['localhost'])\n",
    "\n",
    "connections.get_connection().info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For us, the important unit of Elasticsearch is the *index*, which contains a document collection as well as information needed to search it efficiently. The first step in setting up an Elasticsearch index is to define what a document in our collection will consist of. As usual, we are going to use the Brown corpus as an example. We define a BrownDocument as a subclass of the Document, and define that it will contain the `text` as well as the `genre`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch_dsl import Document, Text, Keyword, analyzer, tokenizer\n",
    "\n",
    "class BrownDocument(Document):\n",
    "    text = Text()        # TEXT contains some word pre-processing steps\n",
    "    genre = Keyword()   # this is a single term (treated like an atomic unit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we are treating the main text a bit differently from the genre, because we might prefer to store the genre as a single indivisible variable rather than as a text that will be processed into tokens. In Elasticsearch, the `Text` fields are associated with an analyzer that coverts a text raw string to a sequence of tokens. \n",
    "\n",
    "The default analyzer for Elasticsearch does a simple form of tokenization which removes punctuation, followed by \"filtering\" consisting of lowercasing, stopword removal, and (Porter) stemming. This is probably fine, but there might be better options, see [here](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-analyzers.html) for a list of built-in analyzers. We could access one by adding analyzer=\"\", keyword to our `Text`. \n",
    "\n",
    "As it turns out, the built-in `classic` analyzer does not lowercase, which is something we'd like. Let's just built a custom analyzer, which allows us to choose exactly the preprocessing steps we want (though unfortunately no lemmatization!), and test it out on the fly using the `simulate` method. Here's a list of build-in [tokenizers](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-tokenizers.html) and possible [filters](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-tokenfilters.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'token': 'test', 'start_offset': 10, 'end_offset': 14, 'typ...}\n",
      "{'token': 'your', 'start_offset': 18, 'end_offset': 22, 'typ...}\n",
      "{'token': 'elasticsearch', 'start_offset': 23, 'end_offset':...}\n",
      "{'token': 'custom', 'start_offset': 37, 'end_offset': 43, 't...}\n",
      "{'token': 'analyzer', 'start_offset': 44, 'end_offset': 52, ...}\n",
      "{'token': 'how', 'start_offset': 54, 'end_offset': 57, 'type...}\n",
      "{'token': 'did', 'start_offset': 58, 'end_offset': 61, 'type...}\n",
      "{'token': 'go', 'start_offset': 65, 'end_offset': 67, 'type'...}\n"
     ]
    }
   ],
   "source": [
    "from elasticsearch_dsl import Document, Text, Keyword, analyzer, tokenizer\n",
    "\n",
    "brown_analyzer = analyzer('brown', tokenizer=\"classic\", filter=[\"lowercase\",\"stop\"])     # 'brown' is the name of the analyzer\n",
    "\n",
    "analyzed = brown_analyzer.simulate(\"This is a test of your Elasticsearch custom analyzer. How did it go?\")['tokens']\n",
    "for token in analyzed:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also create [a custom tokenizer](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-pattern-tokenizer.html), for instance the one below which tokenizes into character 3-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'token': 'wha', 'start_offset': 0, 'end_offset': 3, 'type':...}\n",
      "{'token': 'hat', 'start_offset': 1, 'end_offset': 4, 'type':...}\n",
      "{'token': \"at'\", 'start_offset': 2, 'end_offset': 5, 'type':...}\n",
      "{'token': \"t's\", 'start_offset': 3, 'end_offset': 6, 'type':...}\n",
      "{'token': \"'s \", 'start_offset': 4, 'end_offset': 7, 'type':...}\n",
      "{'token': 's g', 'start_offset': 5, 'end_offset': 8, 'type':...}\n",
      "{'token': ' go', 'start_offset': 6, 'end_offset': 9, 'type':...}\n",
      "{'token': 'goi', 'start_offset': 7, 'end_offset': 10, 'type'...}\n",
      "{'token': 'oin', 'start_offset': 8, 'end_offset': 11, 'type'...}\n",
      "{'token': 'ing', 'start_offset': 9, 'end_offset': 12, 'type'...}\n",
      "{'token': 'ng ', 'start_offset': 10, 'end_offset': 13, 'type...}\n",
      "{'token': 'g o', 'start_offset': 11, 'end_offset': 14, 'type...}\n",
      "{'token': ' on', 'start_offset': 12, 'end_offset': 15, 'type...}\n",
      "{'token': 'on ', 'start_offset': 13, 'end_offset': 16, 'type...}\n",
      "{'token': 'n h', 'start_offset': 14, 'end_offset': 17, 'type...}\n",
      "{'token': ' he', 'start_offset': 15, 'end_offset': 18, 'type...}\n",
      "{'token': 'her', 'start_offset': 16, 'end_offset': 19, 'type...}\n",
      "{'token': 'ere', 'start_offset': 17, 'end_offset': 20, 'type...}\n",
      "{'token': 're?', 'start_offset': 18, 'end_offset': 21, 'type...}\n"
     ]
    }
   ],
   "source": [
    "tri_analyzer = analyzer('my_analyzer',\n",
    "    tokenizer=tokenizer('trigram', 'ngram', min_gram=3, max_gram=3),\n",
    "    filter=['lowercase']\n",
    ")\n",
    "for token in tri_analyzer.simulate(\"what's going on here?\")['tokens']:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's redefine BrownDocument using the `brown_analyzer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BrownDocument(Document):\n",
    "    text = Text(analyzer=brown_analyzer)\n",
    "    genre = Keyword()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to setup up our index. First, we will initialize an index class and tie it to the `BrownDocument` type defined above using the its `document` method. The `create` method actually creates the index on the Elasicsearch cluster by calling the API with the relevant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acknowledged': True, 'shards_acknowledged': True, 'index': 'brown'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from elasticsearch_dsl import Index\n",
    "\n",
    "brown_index = Index(\"brown\")\n",
    "brown_index.document(BrownDocument)\n",
    "brown_index.create()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can fill the index by creating each BrownDocument and saving it to the index. We use keyword arguments to define the fields for each document. Every document in an Elasticsearch index has a unique id, which is contained in a special `meta.id` field, here we are assigning the ids to be the original Brown filenames. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.9/site-packages/elasticsearch/connection/base.py:200: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.16/security-minimal-setup.html to enable security.\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "\n",
    "for fileid in brown.fileids():\n",
    "    text = \" \".join(brown.words(fileid))\n",
    "    genre = brown.categories(fileid)[0]\n",
    "    doc = BrownDocument(text=text, genre=genre)    # feeding the text and genre into the BrownDocument class\n",
    "    doc.meta.id = fileid\n",
    "    doc.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the index you create in Elasticsearch will persist after you close your ipynb and even shut down Elasticsearch. If you want to start again from scratch on a index, you will need to delete your old index first.\n",
    "\n",
    "**Don't run unless you want to remove the index** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# brown_index.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do boolean filtering in ElasticSearch. In order to do a search of any kind, we start by creating a search object using the `search` method of the index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = brown_index.search()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two basic methods which are used to define the search: `filter` and `query`. You use filter if you ONLY want to do boolean filtering. The `query` method does both boolean filtering and relevance ranking, which is slightly inefficient if you don't care about the ranking of documents.\n",
    "\n",
    "The other thing we need to set up a query using elasticsearch_dsl are `Match` objects. `Match` objects serve two purposes, they let us define the document field where we will be searching, and they can be combined into complex boolean expressions. Note you don't have to use `Match` objects, there is other syntax in Elasticsearch to do queries, but they are very handy for boolean filtering. \n",
    "\n",
    "Let's up a simple query which demonstrates the use of `query` and `Match`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch_dsl.query import Match\n",
    "\n",
    "s = s.query(Match(text=\"brown\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few important points here. \n",
    "\n",
    "1. Using `s.query` doesn't do anything at all unless we assign it to s (queries can be considered an immutable datatype like strings in this regard). \n",
    "2. the Match object is generated on the fly and passed as the argument to the `query` method\n",
    "3. We indicate which field we are searching in by using keyword arguments to the match constructor\n",
    "4. The above does not actually do the search. It is simply constructing a API call, which will be passed to the Java backend in JSON format when we execute the search. We can look at the underlying API call for a search by using the `to_dict` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': {'match': {'text': 'brown'}}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's `execute` the query, storing the result in a response variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = s.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response: [BrownDocument(index='brown', id='ch26'), BrownDocument(index='brown', id='cj58'), BrownDocument(index='brown', id='cj15'), BrownDocument(index='brown', id='ce14'), BrownDocument(index='brown', id='ch29'), BrownDocument(index='brown', id='ce15'), BrownDocument(index='brown', id='ce13'), BrownDocument(index='brown', id='cf32'), BrownDocument(index='brown', id='cn22'), BrownDocument(index='brown', id='cc14')]>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We didn't get all the results (there are 70 in total). Like any good search engine, Elasticsearch only likes to show 10 results at a time. We can override this by using slicing syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': {'match': {'text': 'brown'}}, 'from': 0, 'size': 500}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = s[:500]\n",
    "print(s.to_dict())\n",
    "response = s.execute()\n",
    "len(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we do a match on the genre name instead we can quickly pull out documents from particular genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 The Office of Business Economics ( OBE ) of the U.S. Department of Commerce provides basic measures \n",
      "2 In most of the less developed countries , however , such programing is at best inadequate and at wor\n",
      "3 You have heard him tell these young people that during his almost 50 years of service in the Congres\n",
      "4 Origin of state automobile practices . The practice of state-owned vehicles for use of employees on \n",
      "5 The Rhode Island property tax There was a time some years ago when local taxation by the cities and \n",
      "6 Local industry's investment in Rhode Island was the big story in 1960's industrial development effor\n",
      "7 Special districts in Rhode island . It is not within the scope of this report to elaborate in any gr\n",
      "8 Rhode Island Heritage Week proclamation by John A. Notte , Jr. , governor The theme of Rhode Island \n",
      "9 Be it enacted by the Senate and House of Representatives of the United States of America in Congress\n",
      "10 In the same period , 431 presentations by members of the staff were made to local , national , and i\n",
      "11 Another recent achievement was the successful development of a method for the complete combustion in\n",
      "12 ( E ) In addition to the penalties provided in Title 18 , United States Code , Section 1001 , any pe\n",
      "13 Mr. Dooley . Mr. Speaker , for several years now the commuter railroads serving our large metropolit\n",
      "14 From its inception in 1920 with the passage of Public Law 236 , 66th Congress , the purpose of the v\n",
      "15 At the entrance side of the shelter , each roof beam is rested on the inside 4 inches of the block w\n",
      "16 A former Du Pont official became a General Motors vice president and set about maximizing Du Pont's \n",
      "17 It is not a medieval mental quirk or an attitude `` unnourished by sense '' to believe that husbands\n",
      "18 Foreign policy in its total context With this enlarged role in mind , I should like to make a few su\n",
      "19 While there should be no general age limit or restriction to one sex , there will be particular proj\n",
      "20 Wildlife habitat resources In 1960 one-quarter of the 92.5 million recreation visits to the National\n",
      "21 Strategy and tactics of the U.S. military forces are now undergoing one of the greatest transitions \n",
      "22 Purchase authorizations will include provisions relating to the sale and delivery of commodities , i\n",
      "23 This broad delegation leaves within our discretion ( subject to the always-present criterion of the \n",
      "24 If you elect to use the Standard Deduction or the Tax Table , and later find you should have itemize\n",
      "25 The one- or two-season hunt , of which there have been too many recently , may do more harm than goo\n",
      "26 When the Brown & Sharpe Manufacturing Company reached its 125th year as a going industrial concern d\n",
      "27 Sales and net income for the year ended December 31 , 1960 showed an improvement over 1959 . Net inc\n",
      "28 Following the term of service in Japan , each emissary returns for a brief visit to the campus to in\n",
      "29 In recent months , much attention has been given to the probable extent of the current downtrend in \n",
      "30 Between meetings he helps the president keep track of delegated matters . Since these duties fit nea\n"
     ]
    }
   ],
   "source": [
    "s = brown_index.search()\n",
    "s = s.query(Match(genre=\"government\"))\n",
    "s = s[:500]\n",
    "response = s.execute()\n",
    "for i, hit in enumerate(response):\n",
    "    print(i+1, hit.text[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Notice we only have 30 matches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elasticsearch Match instances support three boolean operators: & (and), | (or), and ~ (not). We can simply combine Match objects to create new Match objects, in the same way we would combine sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can have the same flexibility with Match objects boolean operators as we do with Python boolean operators. For example, let's find documents which have neither the word *black* nor *blue*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "331\n"
     ]
    }
   ],
   "source": [
    "s = brown_index.search()\n",
    "s = s.query(~(Match(text=\"black\") | Match(text=\"blue\")))\n",
    "s = s[:500]\n",
    "response = s.execute()\n",
    "print(len(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And documents containing both \"black\" and \"blue\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n"
     ]
    }
   ],
   "source": [
    "s = brown_index.search()\n",
    "s = s.query(Match(text=\"black\") & Match(text=\"blue\"))\n",
    "\n",
    "s = s[:500]\n",
    "response = s.execute()\n",
    "print(len(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Boolean Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We'll now reimplement some of the Elasticsearch functionality in native Python.\n",
    "\n",
    "The simplest kind of query involves looking for texts that contain particular words. For example, we can look for texts in the Brown corpus that contain the words \"black\" and \"blue\". However, even for a corpus of only 500 texts, iterating over the texts is pretty slow. \n",
    "\n",
    "This corresponds to the grep approach to IR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ca18', 'ca25', 'ca33', 'cb13', 'ce23', 'ce25', 'cf36', 'cg27', 'cg40', 'cg41', 'cg50', 'ck06', 'ck10', 'ck13', 'ck15', 'cl10', 'cl19', 'cl21', 'cn15', 'cn19', 'cn20', 'cn28', 'cp01', 'cp04', 'cp05', 'cp15', 'cp21', 'cp23', 'cp26', 'cp28']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "\n",
    "def get_texts_with_words(word1,word2):\n",
    "    '''returns a list of brown fileids that contain the provided words'''\n",
    "    texts = []\n",
    "    for filename in brown.fileids():\n",
    "        has_word1 = False\n",
    "        has_word2 = False\n",
    "        for word in brown.words(filename):\n",
    "            if word.lower() == word1:\n",
    "                has_word1 = True\n",
    "            if word.lower() == word2:\n",
    "                has_word2 = True\n",
    "        if has_word1 and has_word2:\n",
    "            texts.append(filename)\n",
    "    return texts\n",
    "\n",
    "print(get_texts_with_words(\"black\",\"blue\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Searching through a corpus in response to a query is not practical. That's why we need to build an index. The underlying data structure used in IR  is known as an *inverted index*. The classic setup for an inverted index is a hash map (i.e. a Python dict) from the word to a list of document ids. Let's create this for the brown corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def create_inverted_index(nltk_corpus):\n",
    "    inverted_index = defaultdict(set)\n",
    "    sorted_ids = nltk_corpus.fileids()\n",
    "    sorted_ids.sort()\n",
    "\n",
    "    for filename in sorted_ids:\n",
    "        for word in brown.words(filename):\n",
    "            inverted_index[word.lower()].add(filename)\n",
    "\n",
    "    return inverted_index\n",
    "\n",
    "brown_inverted_index = create_inverted_index(brown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Finding the documents which contain one specific word is now very fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cj14', 'cc08', 'cn27', 'ck18', 'cp02', 'ch06', 'ck25', 'cl14', 'cn16', 'cj61', 'ce15', 'cp10', 'cb14', 'cf22', 'cg51', 'cn22', 'cf35', 'cp16', 'ce14', 'cl18', 'cb02', 'cb11', 'cf34', 'ca18', 'cr07', 'cn07', 'cn06', 'ce11', 'cn26', 'cb04', 'cg03', 'ck29', 'cn10', 'cp05', 'cl17', 'ca17', 'cg12', 'ca21', 'cf32', 'ca29', 'cc04', 'cg55', 'cg47', 'cn15', 'cc14', 'cj15', 'ca11', 'cj58', 'cf26', 'cn23', 'cp12', 'ck01', 'ca24', 'cp14', 'ch26', 'cg14', 'ce18', 'cn20', 'ck16', 'cp26', 'ck13', 'cl13', 'cp04', 'cb24', 'cn17', 'ch29', 'ce13', 'cf30', 'ch25'}\n"
     ]
    }
   ],
   "source": [
    "print(brown_inverted_index[\"brown\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can now use our index to do Boolean filtering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "85\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ca18',\n",
       " 'ca25',\n",
       " 'ca33',\n",
       " 'cb13',\n",
       " 'ce23',\n",
       " 'ce25',\n",
       " 'cf36',\n",
       " 'cg27',\n",
       " 'cg40',\n",
       " 'cg41',\n",
       " 'cg50',\n",
       " 'ck06',\n",
       " 'ck10',\n",
       " 'ck13',\n",
       " 'ck15',\n",
       " 'cl10',\n",
       " 'cl19',\n",
       " 'cl21',\n",
       " 'cn15',\n",
       " 'cn19',\n",
       " 'cn20',\n",
       " 'cn28',\n",
       " 'cp01',\n",
       " 'cp04',\n",
       " 'cp05',\n",
       " 'cp15',\n",
       " 'cp21',\n",
       " 'cp23',\n",
       " 'cp26',\n",
       " 'cp28'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sys import getsizeof\n",
    "black_matches = brown_inverted_index[\"black\"]\n",
    "blue_matches = brown_inverted_index[\"blue\"]\n",
    "\n",
    "print(len(black_matches))\n",
    "print(len(blue_matches))\n",
    "\n",
    "black_matches & blue_matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Sets also provide simple ways to implement other kinds of boolean logics, like *or*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ca01',\n",
       " 'ca02',\n",
       " 'ca05',\n",
       " 'ca15',\n",
       " 'ca16',\n",
       " 'ca17',\n",
       " 'ca18',\n",
       " 'ca22',\n",
       " 'ca23',\n",
       " 'ca24',\n",
       " 'ca25',\n",
       " 'ca29',\n",
       " 'ca30',\n",
       " 'ca32',\n",
       " 'ca33',\n",
       " 'ca39',\n",
       " 'ca40',\n",
       " 'cb05',\n",
       " 'cb06',\n",
       " 'cb09',\n",
       " 'cb10',\n",
       " 'cb13',\n",
       " 'cb17',\n",
       " 'cb26',\n",
       " 'cb27',\n",
       " 'cc04',\n",
       " 'cc05',\n",
       " 'cc08',\n",
       " 'cc14',\n",
       " 'cc15',\n",
       " 'cd03',\n",
       " 'ce05',\n",
       " 'ce11',\n",
       " 'ce12',\n",
       " 'ce13',\n",
       " 'ce19',\n",
       " 'ce23',\n",
       " 'ce25',\n",
       " 'ce32',\n",
       " 'ce34',\n",
       " 'cf01',\n",
       " 'cf02',\n",
       " 'cf06',\n",
       " 'cf10',\n",
       " 'cf18',\n",
       " 'cf20',\n",
       " 'cf22',\n",
       " 'cf26',\n",
       " 'cf28',\n",
       " 'cf29',\n",
       " 'cf34',\n",
       " 'cf35',\n",
       " 'cf36',\n",
       " 'cf38',\n",
       " 'cf39',\n",
       " 'cf42',\n",
       " 'cf44',\n",
       " 'cg04',\n",
       " 'cg05',\n",
       " 'cg09',\n",
       " 'cg12',\n",
       " 'cg14',\n",
       " 'cg17',\n",
       " 'cg18',\n",
       " 'cg27',\n",
       " 'cg40',\n",
       " 'cg41',\n",
       " 'cg50',\n",
       " 'cg51',\n",
       " 'cg55',\n",
       " 'cg69',\n",
       " 'cg70',\n",
       " 'cg75',\n",
       " 'ch27',\n",
       " 'cj01',\n",
       " 'cj09',\n",
       " 'cj10',\n",
       " 'cj16',\n",
       " 'cj43',\n",
       " 'cj48',\n",
       " 'cj53',\n",
       " 'cj62',\n",
       " 'cj66',\n",
       " 'cj70',\n",
       " 'ck03',\n",
       " 'ck04',\n",
       " 'ck06',\n",
       " 'ck10',\n",
       " 'ck11',\n",
       " 'ck12',\n",
       " 'ck13',\n",
       " 'ck14',\n",
       " 'ck15',\n",
       " 'ck16',\n",
       " 'ck17',\n",
       " 'ck19',\n",
       " 'ck22',\n",
       " 'ck23',\n",
       " 'ck24',\n",
       " 'ck26',\n",
       " 'ck28',\n",
       " 'ck29',\n",
       " 'cl07',\n",
       " 'cl09',\n",
       " 'cl10',\n",
       " 'cl11',\n",
       " 'cl19',\n",
       " 'cl20',\n",
       " 'cl21',\n",
       " 'cl22',\n",
       " 'cm01',\n",
       " 'cm02',\n",
       " 'cm04',\n",
       " 'cn01',\n",
       " 'cn02',\n",
       " 'cn04',\n",
       " 'cn05',\n",
       " 'cn06',\n",
       " 'cn12',\n",
       " 'cn13',\n",
       " 'cn15',\n",
       " 'cn16',\n",
       " 'cn19',\n",
       " 'cn20',\n",
       " 'cn22',\n",
       " 'cn23',\n",
       " 'cn24',\n",
       " 'cn26',\n",
       " 'cn27',\n",
       " 'cn28',\n",
       " 'cn29',\n",
       " 'cp01',\n",
       " 'cp04',\n",
       " 'cp05',\n",
       " 'cp06',\n",
       " 'cp07',\n",
       " 'cp08',\n",
       " 'cp10',\n",
       " 'cp12',\n",
       " 'cp13',\n",
       " 'cp14',\n",
       " 'cp15',\n",
       " 'cp16',\n",
       " 'cp17',\n",
       " 'cp20',\n",
       " 'cp21',\n",
       " 'cp22',\n",
       " 'cp23',\n",
       " 'cp25',\n",
       " 'cp26',\n",
       " 'cp28',\n",
       " 'cp29',\n",
       " 'cr02',\n",
       " 'cr05',\n",
       " 'cr09'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "black_matches | blue_matches   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boolean *not* can be implemented by using set difference between the negated set and the set off all documents. E.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "345\n"
     ]
    }
   ],
   "source": [
    "all_documents = set(brown.fileids())\n",
    "print(len(all_documents - (black_matches | blue_matches)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will see this again in the next notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
