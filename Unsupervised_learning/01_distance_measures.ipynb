{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa10aa90",
   "metadata": {},
   "source": [
    "# 1: Distance measures\n",
    "\n",
    "### 1. Hamming distance \n",
    "\n",
    "Start by implementing the Hamming distance function below. The function takes two strings `s1` and `s2` as input and returns their Hamming distance. When implemented correctly, the output should be the following:\n",
    "\n",
    "```\n",
    "hamming(\"dog\", \"dgo\") == 2\n",
    "hamming(\"blouse\", \"house\") == 6\n",
    "```\n",
    "\n",
    "In case the input strings are of different lengths, we should pad the shorter string **at the end** with padding symbols `\"#\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f64304f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "def hamming(s1, s2):\n",
    "\n",
    "    if len(s1) < len(s2):\n",
    "        return hamming(s2,s1)\n",
    "    else:\n",
    "        s2 += \"#\" * (len(s1) - len(s2))\n",
    "        return sum([c1 != c2 for c1, c2 in zip(s1, s2)])\n",
    "    \n",
    "print(hamming(\"dog\",\"dgo\"))\n",
    "print(hamming(\"house\", \"blouse\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbac1ab",
   "metadata": {},
   "source": [
    "### 2. Comparing to minimum edit distance\n",
    "\n",
    "We'll then compare Hamming distance and minimum edit distance (also called Levensthein distance). Start by installing the module [Levensthein](https://maxbachmann.github.io/Levenshtein/installation.html).\n",
    "\n",
    "We can then apply the `Levenshtein.distance` function. It should give the following output:\n",
    "```\n",
    "Levenshtein.distance(\"dog\", \"dgo\") == 2\n",
    "Levenshtein.distance(\"blouse\", \"house\") == 2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6064080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "import Levenshtein\n",
    "\n",
    "print(Levenshtein.distance(\"dog\", \"dgo\"))\n",
    "print(Levenshtein.distance(\"blouse\", \"house\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c36496",
   "metadata": {},
   "source": [
    "### 3. Our own version of minimum edit distance\n",
    "\n",
    "We'll now implement our own version of the minimum edit distance algorithm `lev`. \n",
    "\n",
    "For input strings `\"house\"` and `\"blouse\"`, the skeleton code initializes a matrix:\n",
    "\n",
    "$${\\rm matrix} = \\begin{bmatrix}\n",
    "0 & 1 & 2 & 3 & 4 & 5\\\\\n",
    "1 & 0 & 0 & 0 & 0 & 0\\\\\n",
    "2 & 0 & 0 & 0 & 0 & 0\\\\\n",
    "3 & 0 & 0 & 0 & 0 & 0\\\\\n",
    "4 & 0 & 0 & 0 & 0 & 0\\\\\n",
    "5 & 0 & 0 & 0 & 0 & 0\\\\\n",
    "6 & 0 & 0 & 0 & 0 & 0\\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "The element `matrix[i,j]` corresponds to the substrings `s1[:i]` and `s2[:j]`. Now we fill in the entries in this matrix correctly using the update rule: \n",
    "\n",
    "1. Test if one of the strings is empty. In that case, we need to insert all symbols in the other string;\n",
    "2. Check if we can copy, i.e. the strings start with the same symbol. Otherwise, cost will be at least 1;\n",
    "3. Check which produces the lowest overall cost: substitution (or copying), insertion or deletion;\n",
    "4. Return lowest overall cost\n",
    "\n",
    "\n",
    "You should then return the last element of the last row of `matrix`. This is the minimum edit distance between `s1` and `s2`.\n",
    "\n",
    "The function should return the following result:\n",
    "\n",
    "```\n",
    "lev(\"dog\", \"dgo\") == 2\n",
    "lev(\"house\", \"blouse\") == 2\n",
    "lev(\"mitten\", \"sitting\") == 3\n",
    "```\n",
    "\n",
    "> **Hint:** If you encounter problems, it's a good idea to print matrix after the for loops and check that the entries in the matrix agree with your understanding of the update rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e01a518c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0\n",
      "2.0\n",
      "3.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def lev(s1, s2):\n",
    "    matrix = np.zeros((len(s1) + 1, len(s2) + 1))\n",
    "    \n",
    "    # We initialize the first row and column of the\n",
    "    # distance matrix\n",
    "    matrix[:,0] = np.arange(0,len(s1)+1)\n",
    "    matrix[0,:] = np.arange(0,len(s2)+1)\n",
    "    \n",
    "    for col in range(1,len(s2) + 1):\n",
    "        for row in range(1,len(s1) + 1):\n",
    "            # Please use the distance update rule from the lecture\n",
    "            # to fill in cell matrix[row][col]. Remember to check\n",
    "            # whether we're copying or substituting.\n",
    "            \n",
    "            subst_cost = 0 if s1[row-1] == s2[col-1] else 1 \n",
    "            matrix[row,col] = min(matrix[row-1,col] + 1,\n",
    "                                  matrix[row, col-1] + 1,\n",
    "                                  matrix[row-1, col-1] + subst_cost)\n",
    "\n",
    "    # You should now return the last entry in the last \n",
    "    # row. \n",
    "        \n",
    "    return matrix[-1,-1]\n",
    "\n",
    "print(lev(\"dog\", \"dgo\"))\n",
    "print(lev(\"house\", \"blouse\"))\n",
    "print(lev(\"mitten\", \"sitting\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a4c282",
   "metadata": {},
   "source": [
    "### 4. Comparing document vectors\n",
    "\n",
    "Let's start by compiling a few document vectors for Brown corpus categories. The Brown corpus contains text from 15 categories:\n",
    "\n",
    "```\n",
    "['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']\n",
    "```\n",
    "\n",
    "We'll need to encode these into ID numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da0182b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "cat2i = {cat:i for i, cat in enumerate(brown.categories())}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d32ab2c",
   "metadata": {},
   "source": [
    "We will first represent each category as a bag-of-words containing all the tokens from documents in that category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "555ca6c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /Users/lxy/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/lxy/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "[('said', 204), ('would', 189), ('one', 172), ('back', 158), ('could', 145), ('like', 139), ('man', 107), ('get', 99), ('two', 89), ('know', 86)]\n",
      "[('state', 196), ('year', 183), ('states', 180), ('may', 179), ('united', 155), ('new', 150), ('development', 125), ('one', 125), ('would', 120), ('made', 118)]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown,stopwords\n",
    "from collections import Counter\n",
    "\n",
    "nltk.download(\"brown\")\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "en_stopwords = stopwords.words(\"english\")\n",
    "\n",
    "def preprocess(words):\n",
    "    '''lower case and remove stopwords'''\n",
    "    return [word.lower() for word in words if word.lower() not in en_stopwords and word.isalpha()]\n",
    "\n",
    "raw_feature_dicts = []\n",
    "for category in brown.categories():\n",
    "    # Represent each category as a counter over word types\n",
    "    raw_feature_dicts.append(Counter(preprocess(brown.words(categories=category))))\n",
    "    \n",
    "print(len(raw_feature_dicts))\n",
    "print(raw_feature_dicts[cat2i[\"mystery\"]].most_common(10))\n",
    "print(raw_feature_dicts[cat2i[\"government\"]].most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b062554a",
   "metadata": {},
   "source": [
    "We will then use `DictVectorizer` from sklearn to transform our word counters into real-valued vectors.\n",
    "\n",
    "`DictVectorizer` will associate each word type with a unique dimension and and store the counts for this word type in  each category under this dimension. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fdd0d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "vectorizer = DictVectorizer()\n",
    "sparse_matrix = vectorizer.fit_transform(raw_feature_dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e686df",
   "metadata": {},
   "source": [
    "Let's print the shape of the resulting matrix. We have one row for each of the 15 categories and around 40k feature dimensions corresponding to word types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9848c2ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 40097)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cfa9b1",
   "metadata": {},
   "source": [
    "To use distance metrics, we'll need to convert this sparse matrix into an `np.array`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ffffdeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_matrix = sparse_matrix.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b100b9e",
   "metadata": {},
   "source": [
    "Let's now use `scipy.spatial.distance.cosine` to compare vectors for different categories. This function actually return the cosine **distance** rather than similarity. These are related via the formula:\n",
    "$$cosdist(x,y) = 1 - cossim(x,y)$$\n",
    "\n",
    "Print the cosine distance between the category *adventure* and all other categories in the Brown corpus. What is the closest category to *adventure*? Which one is furthest away?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d0a1510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adventure 0\n",
      "belles_lettres 0.31761452179089555\n",
      "editorial 0.4106633168160483\n",
      "fiction 0.11781160852800698\n",
      "government 0.6846370760021299\n",
      "hobbies 0.4495174612745898\n",
      "humor 0.24818366256735258\n",
      "learned 0.5719927326485756\n",
      "lore 0.32945460871737575\n",
      "mystery 0.11043616570333414\n",
      "news 0.3883890659297321\n",
      "religion 0.5140895334150573\n",
      "reviews 0.43939022838544683\n",
      "romance 0.12038926342301437\n",
      "science_fiction 0.29095478019263565\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "for cat in brown.categories():\n",
    "    print(cat, cosine(full_matrix[cat2i[\"adventure\"]], full_matrix[cat2i[cat]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48603ded",
   "metadata": {},
   "source": [
    "In the next notebook, we will cluster data points and for this purpose, we often need to know the distance between every pair of points, e.g. category vectors in our case. It is very slow to compute this individually for each pair. Instead we can use the [`scipy.spatial.distance.pdist`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.pdist.html) function. `pdist` returns a so called condensed distance matrix (which is an upper triangular matrix), you can convert this to a reqular square matrix using the function [`scipy.spatial.distance.squareform`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.squareform.html#scipy.spatial.distance.squareform).\n",
    "\n",
    "Read the documentation of the functions and use them to generate an array `m`, where `m[cat2i[\"adventure\"], [\"mystery\"]]` gives you the cosine distance between these two categories. \n",
    "\n",
    "Test a few categories to make sure that your results agree with the distances we computed using a loop above.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "299ad827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 15)\n",
      "0.11043616570333403\n",
      "0.6846370760021299\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "\n",
    "m = squareform(pdist(full_matrix, \"cosine\"))\n",
    "print(m.shape)\n",
    "print(m[cat2i[\"adventure\"], cat2i[\"mystery\"]])\n",
    "print(m[cat2i[\"adventure\"], cat2i[\"government\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e24dc55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
