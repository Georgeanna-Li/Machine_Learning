{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Information Retrieval II\n",
    "\n",
    "* Boolean filtering\n",
    "* Relevance ranking with tf-idf\n",
    "* Ranked search in EalsticSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Boolean Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The simplest kind of query involves looking for texts that contain particular words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "\n",
    "brown_files = {fileid:brown.words(fileid) for fileid in brown.fileids()}\n",
    "brown_files = {fileid:[w.lower() for w in words] for fileid, words in brown_files.items()}\n",
    "                       \n",
    "def get_texts_with_words(word1,word2):\n",
    "    '''returns a list of brown fileids that contain the provided words'''\n",
    "    texts = set()\n",
    "    for fileid, words in brown_files.items():\n",
    "        has_word1 = word1 in words \n",
    "        has_word2 = word2 in words \n",
    "        if has_word1 and has_word2:\n",
    "            texts.add(fileid)\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, we can look for texts in the Brown corpus that contain the words \"black\" and \"blue\". However, even for a corpus of only 500 texts, iterating over the texts is a bit slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ck06', 'ck13', 'cl10', 'cf36', 'ca25', 'cg50', 'cn28', 'cg27', 'cp26', 'cn19', 'cg40', 'ca18', 'ck15', 'cg41', 'ce23', 'ck10', 'ca33', 'cn20', 'cp05', 'cl21', 'cp01', 'cn15', 'cp28', 'cp23', 'cp21', 'ce25', 'cp15', 'cp04', 'cl19', 'cb13'}\n",
      "CPU times: user 35.3 s, sys: 605 ms, total: 35.9 s\n",
      "Wall time: 36.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for i in range(1000):\n",
    "    get_texts_with_words(\"black\",\"blue\")\n",
    "print(get_texts_with_words(\"black\",\"blue\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Searching through a corpus in response to a query is not practical. That's why we need an *inverted index*. We can implement this as a hash map (i.e. a Python dict) from the word to a set of document ids. Let's create this for the brown corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def create_inverted_index(nltk_corpus):\n",
    "    inverted_index = defaultdict(set)\n",
    "    for fileid in brown.fileids():\n",
    "        for word in brown.words(fileid):\n",
    "            inverted_index[word.lower()].add(fileid)\n",
    "    return inverted_index\n",
    "\n",
    "brown_inverted_index = create_inverted_index(brown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Finding the documents which contain one specific word is now very fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cj61', 'cb04', 'cf35', 'ck13', 'ch25', 'cf34', 'ce18', 'cj58', 'cn10', 'cp02', 'ch29', 'cp12', 'ce15', 'cl18', 'cg03', 'ca24', 'cn17', 'cp16', 'ce14', 'cc08', 'cp26', 'cb24', 'ck25', 'ck29', 'cj14', 'cf30', 'cg55', 'cr07', 'cn06', 'ca17', 'ca18', 'cn07', 'cf26', 'cn22', 'cg47', 'cc14', 'ch26', 'cj15', 'cl13', 'ca21', 'cf22', 'ck18', 'cn20', 'cn27', 'cn23', 'cp05', 'ca29', 'cp14', 'cn15', 'cg14', 'cc04', 'cg12', 'cn16', 'cf32', 'ce13', 'ck01', 'ch06', 'ck16', 'cn26', 'ca11', 'ce11', 'cp04', 'cl17', 'cl14', 'cb11', 'cb14', 'cp10', 'cb02', 'cg51'}\n",
      "CPU times: user 195 µs, sys: 57 µs, total: 252 µs\n",
      "Wall time: 238 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for i in range(1000):\n",
    "    brown_inverted_index[\"brown\"]\n",
    "    \n",
    "print(brown_inverted_index[\"brown\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can now use set operations to implement Boolean filtering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ck06', 'ck13', 'cl10', 'cf36', 'ca25', 'cg50', 'cn28', 'cp26', 'cg27', 'cn19', 'cg40', 'ca18', 'ck15', 'cg41', 'ce23', 'ck10', 'ca33', 'cn20', 'cp05', 'cl21', 'cp01', 'cn15', 'cp28', 'cp23', 'cp21', 'ce25', 'cp15', 'cp04', 'cl19', 'cb13'}\n",
      "CPU times: user 2.55 ms, sys: 142 µs, total: 2.69 ms\n",
      "Wall time: 2.66 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for i in range(1000):\n",
    "    set1 = brown_inverted_index[\"black\"]\n",
    "    set2 = brown_inverted_index[\"blue\"]\n",
    "    set1 & set2\n",
    "    \n",
    "print(set1 & set2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Sets also provide simple ways to implement other kinds of boolean logics, like *or*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cf18', 'cf35', 'ck13', 'cf02', 'cl20', 'ce32', 'ck11', 'cf28', 'cr02', 'ca24', 'ca25', 'cg50', 'cg40', 'cr05', 'cn05', 'ca17', 'cg75', 'cf39', 'cc14', 'cj48', 'cf38', 'cn13', 'cn29', 'cf22', 'cn23', 'ch27', 'cn12', 'ca30', 'cp20', 'ck16', 'cg04', 'cl19', 'cr09', 'ck14', 'cg51', 'cb26', 'cb05', 'cb09', 'cf34', 'cp25', 'cp29', 'cf36', 'cm01', 'cp16', 'ca32', 'cn28', 'cf06', 'ca39', 'ca18', 'cp22', 'cj62', 'cg41', 'cn22', 'cb27', 'ce23', 'cj70', 'cj53', 'ca33', 'cj66', 'ca15', 'cg17', 'cn27', 'ck04', 'cl21', 'ca22', 'cn15', 'cp21', 'cg09', 'ce25', 'cf29', 'cn26', 'cf10', 'ce11', 'cb06', 'cp15', 'ck22', 'cf44', 'cp10', 'cj09', 'cn24', 'ca16', 'ca05', 'cc05', 'cl10', 'ce34', 'cn01', 'cg18', 'cp26', 'cc08', 'cn19', 'ck29', 'cn04', 'ck15', 'cl09', 'ck24', 'cf01', 'ck10', 'ck26', 'cm04', 'cg05', 'ck12', 'cn20', 'cp01', 'cl22', 'cg12', 'ce13', 'cf42', 'ce05', 'cp04', 'ck28', 'cj10', 'ca01', 'cm02', 'cp06', 'ce12', 'ca02', 'ck06', 'ce19', 'ck19', 'ck23', 'cb17', 'cd03', 'cp12', 'cg69', 'cl07', 'cg27', 'cg55', 'cn06', 'cp07', 'cg70', 'cf26', 'cp08', 'cp13', 'cj43', 'cj16', 'ca40', 'ca23', 'cn02', 'cp05', 'cp14', 'ca29', 'cp28', 'cc15', 'cg14', 'cc04', 'cb10', 'cn16', 'ck03', 'cp23', 'ck17', 'cj01', 'cl11', 'cb13', 'cp17', 'cf20'}\n",
      "CPU times: user 4.56 ms, sys: 358 µs, total: 4.92 ms\n",
      "Wall time: 4.67 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for i in range(1000):\n",
    "    set1 = brown_inverted_index[\"black\"]\n",
    "    set2 = brown_inverted_index[\"blue\"]\n",
    "    set1 | set2\n",
    "    \n",
    "print(set1 | set2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boolean *not* can be implemented by using set difference between the negated set and the set off all documents. E.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cf25', 'cj51', 'cf47', 'cj11', 'cf13', 'cg74', 'cb15', 'cb18', 'ch11', 'cg66', 'cg32', 'cg67', 'cn10', 'ce18', 'cg33', 'cl20', 'ck11', 'cn08', 'cb01', 'cg19', 'cd08', 'cj77', 'cf03', 'cl23', 'cn17', 'ch17', 'cg21', 'cj14', 'cf30', 'cg07', 'ch19', 'cf12', 'cc01', 'ch09', 'cj38', 'ce36', 'cl02', 'ca13', 'cj64', 'cf22', 'cb25', 'cp18', 'ch27', 'cj80', 'cb19', 'ck01', 'cc06', 'ch22', 'ca11', 'cb12', 'cj21', 'cp27', 'cj19', 'ch21', 'cc07', 'cg73', 'cg63', 'ce10', 'cf08', 'cb04', 'cj07', 'cb05', 'cj52', 'cb09', 'cg53', 'ca37', 'cg38', 'cd01', 'ch23', 'cp02', 'cg08', 'cf04', 'cj73', 'cp16', 'cn25', 'ce16', 'cj35', 'ca39', 'cd16', 'ca06', 'cp22', 'ce21', 'cj72', 'cn22', 'ce22', 'ch26', 'cg23', 'cj70', 'ca07', 'ck18', 'ca15', 'ce01', 'cg45', 'cf24', 'cp19', 'ce31', 'cl03', 'cg06', 'cf29', 'cn26', 'cf10', 'cd02', 'cg71', 'ck22', 'cf09', 'ch20', 'ce20', 'cp10', 'cj44', 'cj09', 'ck27', 'cg54', 'ch25', 'cj08', 'cf16', 'cd11', 'cg62', 'ck21', 'cj74', 'ca31', 'cm03', 'ce35', 'cn01', 'cj79', 'cg16', 'ck20', 'cn21', 'cj04', 'cd17', 'cg61', 'cg56', 'cj27', 'cj42', 'cc03', 'cg20', 'ch05', 'cr01', 'ca42', 'cj13', 'cg43', 'ch01', 'cl13', 'cj15', 'cg05', 'cj31', 'ca41', 'cc13', 'cj75', 'ca08', 'ch28', 'ce07', 'ck07', 'cl06', 'cg10', 'ch10', 'cb21', 'cl17', 'cn11', 'cl12', 'cg25', 'ca01', 'cn18', 'cm02', 'cj12', 'ca43', 'cj55', 'ck19', 'ck23', 'ce15', 'cg26', 'cf11', 'cp11', 'cf15', 'cj71', 'cl04', 'ce30', 'cp03', 'cd04', 'cf19', 'cb24', 'cg30', 'cn14', 'cj06', 'cg58', 'cd12', 'cg22', 'ce29', 'cf43', 'ca23', 'cg15', 'cj33', 'ch12', 'cc15', 'cg14', 'cc09', 'cc04', 'cf40', 'cj76', 'cj03', 'cj32', 'cg48', 'cj78', 'cg49', 'cf20', 'ca34', 'ce03', 'cg35', 'cj59', 'cf18', 'cm06', 'cf27', 'cg11', 'cb16', 'ce32', 'ce06', 'cc16', 'cj24', 'cd13', 'cl18', 'cd06', 'cd15', 'cg64', 'cg60', 'cr05', 'ch16', 'cr07', 'cj34', 'cj29', 'cn07', 'cl16', 'cl05', 'cg46', 'cg47', 'cm05', 'cc14', 'cg31', 'ca21', 'cl15', 'ca30', 'cp20', 'ch15', 'cg42', 'ce27', 'cn03', 'cb02', 'ch18', 'cg51', 'ch06', 'cd09', 'cd05', 'cj61', 'ce26', 'cf07', 'ce09', 'ca14', 'ce33', 'cj58', 'ch29', 'cg36', 'ch24', 'cg72', 'cc02', 'cb22', 'cf45', 'cg03', 'cf14', 'cm01', 'cf06', 'ck25', 'ca04', 'cj63', 'cg28', 'cj62', 'cp09', 'ck05', 'cb03', 'cd07', 'cf31', 'cg17', 'ch03', 'cf41', 'cf33', 'cg65', 'cp24', 'cl01', 'cj56', 'cf32', 'cc10', 'cd14', 'cj36', 'cf44', 'cj49', 'cj69', 'ca12', 'cj28', 'ca09', 'cg13', 'cj30', 'ce08', 'cc17', 'cn24', 'ch13', 'ca44', 'cf05', 'cj68', 'ca16', 'cb08', 'cf17', 'ca05', 'ch14', 'cj23', 'cj47', 'cg68', 'cr06', 'cg24', 'cg44', 'cb23', 'cr08', 'ce14', 'cc08', 'cc12', 'cj57', 'ck29', 'ca38', 'cl08', 'cf46', 'ce04', 'cg52', 'cj41', 'ch30', 'cj37', 'cg02', 'ch02', 'ck09', 'cj25', 'cg34', 'cg12', 'ce13', 'cb20', 'cg57', 'ck08', 'ce05', 'cj17', 'ck02', 'cb14', 'ca10', 'cf21', 'cj65', 'cb07', 'cj22', 'cj40', 'ca02', 'ca03', 'cj18', 'cf37', 'cj39', 'cl24', 'ce19', 'ch04', 'ch08', 'cd10', 'ca28', 'cj67', 'ca36', 'ca27', 'cj46', 'cc11', 'cj02', 'cg37', 'ca26', 'cj60', 'cj05', 'cj26', 'ca20', 'cg39', 'cf48', 'cj45', 'cg59', 'ce24', 'cr04', 'ce02', 'ce28', 'cp13', 'cj20', 'cr03', 'cp14', 'ca19', 'cb10', 'cf23', 'cg29', 'cj50', 'cg01', 'ch07', 'ca35', 'cn09', 'cl14', 'cj54', 'cb11', 'ce17', 'ck28'}\n",
      "CPU times: user 24.4 ms, sys: 1.95 ms, total: 26.3 ms\n",
      "Wall time: 25.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for i in range(1000):\n",
    "    all_documents = set(brown.fileids())\n",
    "    has_black = brown_inverted_index[\"black\"]\n",
    "\n",
    "    not_black = all_documents - has_black\n",
    "    \n",
    "print(not_black)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relevance ranking with TF-IDF\n",
    "\n",
    "Relevance ranking means to order documents based on their relevance to a query. We can apply what we've already learned to built a vector space model for relevance ranking. \n",
    "\n",
    "* Building a document/term matrix for the corpus\n",
    "* Carrying out term weighting\n",
    "* Transfering the query into a compatible space\n",
    "* Identifying most relevant documents using cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 38738)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from collections import Counter\n",
    "from nltk.corpus import movie_reviews, stopwords\n",
    "\n",
    "en_stopwords = set(stopwords.words(\"english\"))\n",
    "\n",
    "raw_feature_dicts = []\n",
    "for document in movie_reviews.fileids():\n",
    "    raw_feature_dicts.append(Counter([word.lower() for word in movie_reviews.words(document) if word.isalpha() and word.lower() not in en_stopwords]))\n",
    "\n",
    "vectorizer = DictVectorizer()\n",
    "X = vectorizer.fit_transform(raw_feature_dicts)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's convert counts in our document vectors into TF-IDF weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 38738)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf = TfidfTransformer()\n",
    "X_tfidf = tfidf.fit_transform(X).toarray()\n",
    "print(X_tfidf.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can use our vectorizer to create a query vector. Note that vectorizer expects a list of dictionaries, so we need to put the query in a list to get around this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 38738)\n"
     ]
    }
   ],
   "source": [
    "def convert_query(text_query):\n",
    "    '''turn a text query string into a vector representation'''\n",
    "    words = Counter([word.lower() for word in text_query.split()])\n",
    "    return vectorizer.transform([words]).toarray()\n",
    "    \n",
    "query_vector = convert_query(\"chamber of secrets\")\n",
    "print(query_vector.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can then use the `cdist` function to compare the vectors for each document with the query and pick out the best one. Note that \"cosine\" refers to cosine **distance** here, so the best match is in fact the ones with **the lowest weight.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1863\n",
      "not since attending an ingmar bergman retrospective a few years ago have i seen a film as uncompromising in its portrayal of emotional truth as secrets & lies . like bergman , director mike leigh is interested in probing his characters ' inner depths through hypernaturally blunt confrontations . also like bergman , leigh engages in frequent closeups of his characters ' ravished and wracked faces . and the prominent mournfulness of a cello on the soundtrack recalls bergman ' s own use of a bach cello suite in an earlier film . all that is missing is a discussion of god . which is not to say that secrets & lies is nothing more than an homage to the swedish master . in fact , it is quite possible leigh had no such intentions in mind . nonetheless , what we get is so far removed from the average moviegoing experience -- even from the reason we go to the movies in the first place -- that it takes some effort to adjust to the film ' s rhythms . once the adjustment is made , however , there are great rewards . one such is the chance to see life on the screen as it really is . though leigh may have adopted some of bergman ' s stylistic touches , most obviously in an early scene of terse cross - cutting during a married couple ' s strained conversation , as well as in that somewhat obtrusive score , the overall feeling of the film is that it eschews any \" style \" at all . whereas bergman uses artifice as a tool to expose reality , leigh makes the camera a mere observer , almost as in a pbs documentary . the effect of this is to focus all of your attention on the actors . it is a tribute to everyone involved that , despite such scrutiny , only infrequently are we aware that anyone * is * acting . much has been made of brenda blethyn ' s performance , and rightly so , but it is only when you remind yourself that you are watching a fiction that you realize how good she is . there are a few missteps . for one , except for one scene ( tragicomic , as it happens ) , there is scant humor in the film . this leads to a certain monotonous tone throughout . and occasionally ( as with bergman ) the bluntness of the situations can seem forced . for all that , this longish film manages to keep hold of your attention . it is unfortunate that the audience for secrets & lies will most likely be limited to an intellectual elite , for there is nothing inherently intellectual about this film . in fact , it might easily resonate more strongly for millions of working class filmgoers who will likely never see it . there is even a sweet but significant irony in the film ' s unspoken take on race relations , something an american audience at least would do well to observe . nonetheless , secrets & lies is not for the faint of heart . though there is nothing physically horrific to make one squeamish , the exploration of common human frailty can be so raw and unsparing that it is tempting to turn from the screen . needless to say , it is also very depressing at times . but for many of us , of course , so is life . and though the film is too honest to tack on a phony happy ending , that same honesty allows it to admit that things can also get better .\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "import numpy as np\n",
    "\n",
    "distances = cdist(query_vector,X_tfidf,\"cosine\")\n",
    "doc_id = np.argmin(distances)\n",
    "print(doc_id)\n",
    "print(\" \".join(movie_reviews.words(movie_reviews.fileids()[doc_id])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranked retrieval with ElasticSearch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nbrown_index.document(BrownDocument)\\nbrown_index.create()\\n\\nfor fileid in brown.fileids():\\n    text = \" \".join(brown.words(fileid))\\n    genre = brown.categories(fileid)[0]\\n    doc = BrownDocument(text=text, genre=genre)\\n    doc.meta.id = fileid\\n    doc.save()\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from elasticsearch_dsl import Document, Text, Keyword, analyzer, tokenizer, Index\n",
    "from elasticsearch_dsl.connections import connections\n",
    "from nltk.corpus import brown\n",
    "\n",
    "connections.create_connection(hosts=['localhost'])\n",
    "\n",
    "brown_analyzer = analyzer('brown', tokenizer=\"whitespace\", filter=[\"lowercase\",\"stop\"])\n",
    "\n",
    "class BrownDocument(Document):\n",
    "    text = Text(analyzer=brown_analyzer)\n",
    "    genre = Keyword()\n",
    "    \n",
    "brown_index = Index(\"brown\")\n",
    "\n",
    "### We don't need to run this if Brown index already exists:\n",
    "\"\"\"\n",
    "brown_index.document(BrownDocument)\n",
    "brown_index.create()\n",
    "\n",
    "for fileid in brown.fileids():\n",
    "    text = \" \".join(brown.words(fileid))\n",
    "    genre = brown.categories(fileid)[0]\n",
    "    doc = BrownDocument(text=text, genre=genre)\n",
    "    doc.meta.id = fileid\n",
    "    doc.save()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default relevance ranking algorithm used in Elasticsearch, which is known as [Okapi BM25](https://en.wikipedia.org/wiki/Okapi_BM25) uses the vector space intuition and involves *tf-idf*, but it does not make use of dimensionality reduction. \n",
    "\n",
    "It works as follows: When the documents are indexed, statistics which allow for a quick *tf-idf* calculation for every term for every document in the corpus are collected. Then, when ranking a document given a query, a document is ranked by summing the *tf-idf* score of all the terms in the document that appear in the query. That is, if we have a Document D and a query Q with words $q_0\\ldots q_n$, the relevance score is:\n",
    "\n",
    "$$\\text{score}(D,Q) = \\sum_{i=1}^{n} \\hat{\\text{tf}}(q_i, D) \\cdot \\text{idf}(q_i)$$ \n",
    "\n",
    "In the typical context of relatively short queries, and in combination with techniques like stopword removal and the use for inverted indices for boolean filtering (with $O(1)$ lookup using our old friend the hash map!), this calculation is extremely efficient!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can actually look under the hood of Elasticsearch and see how specific relevance calculations are done. Let's create a new search query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch_dsl.query import Match\n",
    "\n",
    "s = brown_index.search()\n",
    "s = s.query(Match(text=\"black and blue\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that instead of individual words, this time we are matching a larger phrase. Under the hood, Elasticsearch uses its analyzer to generate tokens from our query, and identifies texts which at least one of the resulting terms. The order of terms doesn't matter, so, given that we have stopword removal, the above is therefore equivalent to `s.query(Match(text=\"black\") | Match(text=\"blue\"))`. For relevance ranking, any term which we are trying to match will be included as one of the query terms for the calculation of Okapi BM25. Let's execute the search and look at the scores of the top 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.9/site-packages/elasticsearch/connection/base.py:200: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.16/security-minimal-setup.html to enable security.\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Hit(brown/cp26): {'text': \"I was thinking of the heat and of water that morni...}>\n",
      "4.560643\n",
      "<Hit(brown/cg50): {'text': \"As he had done on his first Imperial sortie a year...}>\n",
      "4.493673\n",
      "<Hit(brown/ca33): {'text': \"At last the White House is going to get some much-...}>\n",
      "4.4184194\n",
      "<Hit(brown/cb13): {'text': \"Sizzling temperatures and hot summer pavements are...}>\n",
      "4.4184194\n",
      "<Hit(brown/ck10): {'text': \"That summer the gambling houses were closed , desp...}>\n",
      "4.4184194\n",
      "<Hit(brown/cl21): {'text': \"But the police have dropped the case . I want you ...}>\n",
      "4.4184194\n",
      "<Hit(brown/ck13): {'text': \"In the dim underwater light they dressed and strai...}>\n",
      "4.2184463\n",
      "<Hit(brown/cl10): {'text': \"`` Not since last night . I didn't think there was...}>\n",
      "4.176656\n",
      "<Hit(brown/ce23): {'text': \"Roy Mason is essentially a landscape painter whose...}>\n",
      "4.1663623\n",
      "<Hit(brown/cf36): {'text': \"It was John who found the lion tracks . He found t...}>\n",
      "4.0997477\n"
     ]
    }
   ],
   "source": [
    "response = s.execute()\n",
    "for hit in response:\n",
    "    print(hit)\n",
    "    print(hit.meta.score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see exactly how those numbers are being calculated, we have to use the low-level API. First, let's get the true query using `to_dict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': {'match': {'text': 'black and blue'}}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can pass that query to a special `explain` function available in the `elasticsearch` API, with the `id` and `body` keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_index': 'brown',\n",
       " '_type': '_doc',\n",
       " '_id': 'cp26',\n",
       " 'matched': True,\n",
       " 'explanation': {'value': 4.560643,\n",
       "  'description': 'sum of:',\n",
       "  'details': [{'value': 2.8859284,\n",
       "    'description': 'weight(text:black in 487) [PerFieldSimilarity], result of:',\n",
       "    'details': [{'value': 2.8859284,\n",
       "      'description': 'score(freq=7.0), computed as boost * idf * tf from:',\n",
       "      'details': [{'value': 2.2, 'description': 'boost', 'details': []},\n",
       "       {'value': 1.529856,\n",
       "        'description': 'idf, computed as log(1 + (N - n + 0.5) / (n + 0.5)) from:',\n",
       "        'details': [{'value': 108,\n",
       "          'description': 'n, number of documents containing term',\n",
       "          'details': []},\n",
       "         {'value': 500,\n",
       "          'description': 'N, total number of documents with field',\n",
       "          'details': []}]},\n",
       "       {'value': 0.8574569,\n",
       "        'description': 'tf, computed as freq / (freq + k1 * (1 - b + b * dl / avgdl)) from:',\n",
       "        'details': [{'value': 7.0,\n",
       "          'description': 'freq, occurrences of term within document',\n",
       "          'details': []},\n",
       "         {'value': 1.2,\n",
       "          'description': 'k1, term saturation parameter',\n",
       "          'details': []},\n",
       "         {'value': 0.75,\n",
       "          'description': 'b, length normalization parameter',\n",
       "          'details': []},\n",
       "         {'value': 1304.0,\n",
       "          'description': 'dl, length of field (approximate)',\n",
       "          'details': []},\n",
       "         {'value': 1358.844,\n",
       "          'description': 'avgdl, average length of field',\n",
       "          'details': []}]}]}]},\n",
       "   {'value': 1.6747147,\n",
       "    'description': 'weight(text:blue in 487) [PerFieldSimilarity], result of:',\n",
       "    'details': [{'value': 1.6747147,\n",
       "      'description': 'score(freq=1.0), computed as boost * idf * tf from:',\n",
       "      'details': [{'value': 2.2, 'description': 'boost', 'details': []},\n",
       "       {'value': 1.6470631,\n",
       "        'description': 'idf, computed as log(1 + (N - n + 0.5) / (n + 0.5)) from:',\n",
       "        'details': [{'value': 96,\n",
       "          'description': 'n, number of documents containing term',\n",
       "          'details': []},\n",
       "         {'value': 500,\n",
       "          'description': 'N, total number of documents with field',\n",
       "          'details': []}]},\n",
       "       {'value': 0.4621765,\n",
       "        'description': 'tf, computed as freq / (freq + k1 * (1 - b + b * dl / avgdl)) from:',\n",
       "        'details': [{'value': 1.0,\n",
       "          'description': 'freq, occurrences of term within document',\n",
       "          'details': []},\n",
       "         {'value': 1.2,\n",
       "          'description': 'k1, term saturation parameter',\n",
       "          'details': []},\n",
       "         {'value': 0.75,\n",
       "          'description': 'b, length normalization parameter',\n",
       "          'details': []},\n",
       "         {'value': 1304.0,\n",
       "          'description': 'dl, length of field (approximate)',\n",
       "          'details': []},\n",
       "         {'value': 1358.844,\n",
       "          'description': 'avgdl, average length of field',\n",
       "          'details': []}]}]}]}]}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "client = Elasticsearch()\n",
    "\n",
    "response = client.explain(\n",
    "    index=\"brown\",\n",
    "#my code here\n",
    "    id=\"cp26\",\n",
    "    body={'query': {'match': {'text': 'black and blue'}}}\n",
    "#my code here\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A summary of important observations from our low-level investigation of scoring Okapi BM25 scoring in Elasticsearch:\n",
    "\n",
    "* We can see the final score is *basically* a sum of tf-idf scores\n",
    "* However, there's a boosting term (indicating more important fields?) that defaults to 2.2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
