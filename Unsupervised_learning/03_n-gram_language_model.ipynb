{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35331854",
   "metadata": {},
   "source": [
    "# 3: N-gram language model\n",
    "\n",
    "In this notebook we'll build a bigram language model on Penn Treebank data.\n",
    "\n",
    "We start by importing the Penn treebank from nltk and split it into three lists: `train`, `dev` and `test`. \n",
    "\n",
    "We'll assign the first 80% of sentences to `train`, the next 10% into `dev` and the remaining 10% into `test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a3e5c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3131\n",
      "391\n",
      "392\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import treebank\n",
    "\n",
    "train = treebank.sents()[:int(0.8*len(treebank.sents()))]\n",
    "dev = treebank.sents()[int(0.8*len(treebank.sents())):int(0.9*len(treebank.sents()))]\n",
    "test = treebank.sents()[int(0.9*len(treebank.sents())):]\n",
    "\n",
    "print(len(train))\n",
    "print(len(dev))\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97eae26",
   "metadata": {},
   "source": [
    "Then, write a function `preprocess` which removes all trace tokens. These are tokens which contain the `\"*\"` character. Apply `preprocess` to `train`, `dev` and `test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd3aac69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sent):\n",
    "    return [tok for tok in sent if not \"*\" in tok]\n",
    "\n",
    "train = [preprocess(s) for s in train]\n",
    "dev = [preprocess(s) for s in dev]\n",
    "test = [preprocess(s) for s in test]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bd66c5",
   "metadata": {},
   "source": [
    "We now initialize an encoder which can encode word tokens into numbers. We'll use `sklearn.preprocessing.LabelEncoder` for this. \n",
    "\n",
    "You should generate a set `train_types` which contains every word type occurring in the training data. You should add three special symbols into `train_types`:\n",
    "\n",
    "* `<UNK>` the unknown symbol,\n",
    "* `<S>` the start of sentence symbol, and\n",
    "* `</S>` the end of sentence symbol\n",
    "\n",
    "Then initialize a `LabelEncoder` `word_enc` and fit it on `train_types`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6947c99b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "UNK=\"<UNK>\"\n",
    "START=\"<S>\"\n",
    "STOP=\"</S>\"\n",
    "\n",
    "train_types = set([tok for sent in train for tok in sent])\n",
    "train_types.add(UNK)\n",
    "train_types.add(START)\n",
    "train_types.add(STOP)\n",
    "\n",
    "word_enc = LabelEncoder()\n",
    "word_enc.fit(list(train_types))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbda4e3c",
   "metadata": {},
   "source": [
    "Before we can start training our language model, we'll need a function `encode_sent` which encodes a sentence into an array of feature numbers. \n",
    "\n",
    "`encode_sent` takes a list of tokens `s` as input, e.g. `[\"She\", \"works\", \"at\", \"the\", \"company\"]`.\n",
    "\n",
    "We replace every token which is not found in `train_types` with `UNK`. We also\n",
    "append `START` and `STOP` at the beginning and at end of the sentence respectively, before calling `word_encoder.transform` on the sentence.\n",
    "\n",
    "E.g.:\n",
    "\n",
    "```\n",
    "encode_sent(\"She works at the company\".split())\n",
    "array([  769,  3085, 10543,  4036,  9896,  4785,   768])\n",
    "```\n",
    "(note that the specific number values may differ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51aa5781",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sent(s):\n",
    "    s = [START] + [UNK if not tok in train_types else tok for tok in s] + [STOP]\n",
    "    return word_enc.transform(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca74813b",
   "metadata": {},
   "source": [
    "Now we initialize a numpy array `count_w1_w2` of size `len(train_type) x len(train_type)`. Initialize every element of the array to `1` (add one smoothing). \n",
    "\n",
    "We'll use this array to store counts for word pairs in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91900670",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "count_w1_w2 = np.ones((len(train_types), len(train_types)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8ff184",
   "metadata": {},
   "source": [
    "Now implement a function `train_lm`. It takes two arguments:\n",
    "\n",
    "* `data` a dataset, and\n",
    "* `count_w1_w2` an array to store word bigram counts\n",
    "\n",
    "Use should use the nltk function `bigrams` to iterate through the bigrams of every sentence in `data`. First pass the sentence through `encode_sent` and then call `bigrams` on the output.\n",
    "\n",
    "After populating `count_w1_w2` with bigram counts, we then generate a second `len(train_types) x 1` array `count_w1` which contains the row sums in `count_w1_w2`.\n",
    "\n",
    "Return `count_w1_w2` and `count_w1`.\n",
    "\n",
    "**Hint:** You can use `np.array.sum` to sum over the rows of `count_w1_w2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6014524d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import bigrams\n",
    "\n",
    "def train_lm(data, count_w1_w2):\n",
    "    # store all the bigrams\n",
    "    for s in train:\n",
    "        for w1, w2 in bigrams(encode_sent(s)):\n",
    "            count_w1_w2[w1,w2] += 1\n",
    "            \n",
    "    return count_w1_w2, count_w1_w2.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0498017",
   "metadata": {},
   "source": [
    "> Note that here the `count_w1_w2.sum(axis=1)` is the sum of how many times a word appears."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53347a5",
   "metadata": {},
   "source": [
    "Use `train_lm` to train `count_w1_w2` and `count_w1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a28b1128",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_w1_w2, count_w1 = train_lm(train,count_w1_w2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c9e734",
   "metadata": {},
   "source": [
    "Now, write a function `prob` which takes three arguments:\n",
    "\n",
    "* `sent` a sentence like `[\"I\", \"am\", \"Sam\"]` containing $n$ tokens,\n",
    "* `count_w1_w2` a table of word pair counts\n",
    "* `count_w1` a table of word counts\n",
    "\n",
    "The function should return the log probability of the sentence:\n",
    "\n",
    "$$\\log p(s) = \\log p(w_1|{\\rm START}) + \\Big( \\sum_{i=1?}^{n-1} \\log p(w_{i+1}|w_i) \\Big) + \\log p({\\rm STOP}|w_n)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24a33dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob(sent, count_w1_w2, count_w1):\n",
    "    \"\"\"returns the log probability of the sentence\"\"\"\n",
    "    log_prob = 0\n",
    "    for w1, w2 in bigrams(encode_sent(sent)):\n",
    "        log_prob += np.log(count_w1_w2[w1, w2]) - np.log(count_w1[w1])\n",
    "        # P(w2|w1) = count(w1, w2) / count(w1)\n",
    "    return log_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2448c67",
   "metadata": {},
   "source": [
    "Compare the probabilities of the sentences \"I like New York.\" and \". New I York like\". \n",
    "\n",
    "The language model actually scores the correct order higher than the incorrect one (remember that log probabilities are negative and closer to 0 means that the sentence is more likely)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1566873d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-37.632996193501334\n",
      "-56.16365143231954\n"
     ]
    }
   ],
   "source": [
    "print(prob(\"I like New York .\".split(), count_w1_w2, count_w1))\n",
    "print(prob(\". New I York like\".split(), count_w1_w2, count_w1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7234e903",
   "metadata": {},
   "source": [
    "Now write a function `log_pp` which takes as arguments:\n",
    "    \n",
    "* `data` a dataset (dev or test) of $k$ setences $S_i$\n",
    "* `count_w1_w2` a table of word pair counts\n",
    "* `count_w1` a table of word counts\n",
    "\n",
    "The funciton should return the log perplexity of the language model on the data:\n",
    "\n",
    "$$\\frac{-1}{|S_1| + ... + |S_k|} \\cdot \\sum_{i = 1}^{k} \\log p(S_k)$$\n",
    "\n",
    "Run `log_pp` on the development data. You should get around `8` as result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9108006c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.260413159592071"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def log_pp(data, count_w1_w2, count_w1):\n",
    "    \"\"\"returns the log perplexity of the language model\"\"\"\n",
    "    data_prob = 0\n",
    "    data_len = 0\n",
    "    for s in data:\n",
    "        data_prob += prob(s, count_w1_w2, count_w1)\n",
    "        data_len += len(s)\n",
    "    return -(1/data_len) * data_prob\n",
    "\n",
    "log_pp(dev, count_w1_w2, count_w1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de892c53",
   "metadata": {},
   "source": [
    "Run `log_pp` on the test set. The result should be around `8` again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bfeb440f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.33955878691596\n"
     ]
    }
   ],
   "source": [
    "print(log_pp(test, count_w1_w2, count_w1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512b409a",
   "metadata": {},
   "source": [
    "In this final partk, we will generate sentences from the language model.\n",
    "\n",
    "We'll start with a seed sentence `s = [START]` and add tokens to this sentence iteratively.\n",
    "\n",
    "At each step, we sample the next token from the conditional distribution:\n",
    "\n",
    "$$P(tok|w_n)$$\n",
    "\n",
    "where $w_n$ is the current final token in `s`.\n",
    "\n",
    "Start by encoding $s_n$ into an index number `w_ind` using `word_end`. Then form the conditional token distribution for the next token by normalizing the `w_ind`th row in `counts_w1_w2` with `counts_w1[w_ind]`.\n",
    "\n",
    "You should now initialize a parameter `accum = 0`. We'll use this parameter to keep track of the cumulative probability. You should also sample a random number `r` using `numpy.random.random()`.\n",
    "\n",
    "Then iterate over the indices `i` in `distr` updating the cumulative probability with `distr[i]`. When the cumulative probability grows over `r`, we've found the index of the next token `i`.\n",
    "\n",
    "Use `word_enc.inverse_transform` to transform `i` into a word token and append it to `s`.\n",
    "\n",
    "Continue until you sample the end of sequence token `STOP` or your sentence length is 20.\n",
    "\n",
    "Print a couple of sentences. The results will not be very impressive because the bigram language model is quite a weak language model. A trigram model would generate better outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6c4b069c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       ...,\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_w1_w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "84f3fd23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<S>', 'pick', 'mainly', 'Mercantile', '1,880', 'passenger', 'tire', 'unjust', 'Deere', 'mature', 'Gray', '3\\\\/4', '2005', 'chips', 'transaction', 'LANDOR', 'banks', 'speeches', 'HHS', 'Miguel', 'warranties']\n"
     ]
    }
   ],
   "source": [
    "s = [START]\n",
    "\n",
    "while s[-1] != STOP and len(s) < 21:\n",
    "    w_ind = word_enc.transform([s[-1]])[0]          # word_enc.transform return a list with one value\n",
    "    distr = count_w1_w2[w_ind] / count_w1[w_ind]     # calculate the probability\n",
    "    accum = 0\n",
    "    r = np.random.random()\n",
    "    for i, p in enumerate(distr):\n",
    "        accum += p\n",
    "        if r < accum:\n",
    "            tok = word_enc.inverse_transform([i])[0]\n",
    "            s.append(tok)\n",
    "            break\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4adb8f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
