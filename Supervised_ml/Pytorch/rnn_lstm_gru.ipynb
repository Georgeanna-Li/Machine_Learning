{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks \n",
    "\n",
    "- Learn about embedding layer\n",
    "- Introduce Recurrent Neural Networks (RNNs)\n",
    "- Implement RNN for sentiment analysis\n",
    "- Implement Long-Short Term Memories (LSTMs) for sentiment analysis\n",
    "- Implement Gated Recurrent Units (GRUs) for sentiment analysis\n",
    "\n",
    "### General\n",
    "- This notebook was last tested on Python 3.8, PyTorch 1.7.1 and TorchText 0.8.1\n",
    "- This notebook uses torchtext to process datasets\n",
    "\n",
    "### Getting Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# required imports\n",
    "import pandas as pd\n",
    "import torchtext\n",
    "from torchtext.legacy.data import Field, LabelField\n",
    "from torchtext.legacy.data import TabularDataset\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding Layer\n",
    "\n",
    "Embedding layer is the ubiquitous input layer of deep neural networks used in NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [``Embedding`` layer](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html#embedding) in Pytorch (that we used in the last week's tutorial on Word2vec) is a lookup table that is typically (in NLP) used to store word embeddings of a fixed vocabulary and word embedding size. Word embeddings can be retrieved from the lookup table by providing a list of word index as input to the layer. We need to know what the input and the output of this layer look like. Let's first look at a dummy example where we have two sentences ``x_1`` and ``x_2``. Let's assume we have the two sentences as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_1 = \"He is very nice\" # sentence 1\n",
    "x_2 = \"She is very kind\" # sentence 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convert the two sentences into indexes (each word is replaced with its index in the vocabulary).\n",
    "Let's assume our ``vocabulary size`` is set to 100. Remember, vocabulary size is a hyper-parameter.(It's because we can actually change this by putting limitations on the least frequency of words)\n",
    "\n",
    "\n",
    "Let's also store that ``vocabulary size`` in a variable ``VOCAB_SIZE`` now as we will need to pass it to the ``Embedding`` layer later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_1 = [1, 25, 40, 5]\n",
    "x_2 = [4, 25, 40, 99]\n",
    "VOCAB_SIZE = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Max sequence length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One last thing we need to think about is the ``length of each sequence``. The two examples above are nicely set to equal length = 4. This does not need to be the case, as we can have sequences of varying lengths. We will be passing a batch of sentences to Pytorch and the max sequence length will be set to the length of the longest sentence (after tokenization) in that batch. The rest of sentences (shorter ones) will be padded with zeros. \n",
    "\n",
    "Now, do we need to explicitly provide the max sequence length to Pytorch? And how do we know the max seq length for each batch, if different batches have sequences of varying lengths and each batch is set to the max sent in that batch? Well, rest assured, we don't really need to worry about that. Pytorch will assign a max seq length for each batch. We will be able to inspect the max seq length for a given batch using output of the ``Embedding`` layer. (We will see that soon)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Size of word vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``Embedding`` layer will give us a vector for each word in the vocabulary.\n",
    "\n",
    "Now, we will need to tell it what size we want for that vector. Popular values for a vector size are usually between 100-300 for many tasks (e.g., sentiment analysis). Let's set it to 300 dimensions. \n",
    "\n",
    "All words in the vocabulary will have the same embedding size. Let's put that hyper-parameter in a variable ``WORD_VEC_SIZE``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORD_VEC_SIZE= 300 # size of word embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to call the ``Embedding`` class to construct an embeddings tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of the embedding lookup table =  torch.Size([100, 300])\n",
      "input (word indices) tensor = \n",
      " tensor([[ 1, 25, 40,  5],\n",
      "        [ 4, 25, 40, 99]])\n",
      "input (word indices) shape =  torch.Size([2, 4])\n"
     ]
    }
   ],
   "source": [
    "# Constructing an embedding Layer:\n",
    "embedding = nn.Embedding(VOCAB_SIZE, WORD_VEC_SIZE)\n",
    "print(\"size of the embedding lookup table = \", embedding.weight.data.size())\n",
    "\n",
    "# let's create a sample input (word indices) to the embedding layer \n",
    "sample_input = torch.LongTensor([ x_1, x_2 ])\n",
    "print(\"input (word indices) tensor = \\n\", sample_input)\n",
    "print(\"input (word indices) shape = \", sample_input.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pass the input to the embedding layer and print the word embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word embeddings tensor = \n",
      " tensor([[[ 0.1901, -2.3038,  1.6044,  ...,  1.3484, -0.5816, -0.3130],\n",
      "         [-0.5178, -1.4132,  1.3756,  ..., -0.9488, -0.4345,  0.9516],\n",
      "         [-0.2492, -1.2017,  1.7837,  ...,  1.2783,  1.6616,  0.4356],\n",
      "         [ 0.6053, -1.2433, -0.1071,  ...,  1.0977,  0.8681, -0.0269]],\n",
      "\n",
      "        [[-0.3506, -0.1355,  0.9561,  ...,  1.1970,  1.2279,  1.1410],\n",
      "         [-0.5178, -1.4132,  1.3756,  ..., -0.9488, -0.4345,  0.9516],\n",
      "         [-0.2492, -1.2017,  1.7837,  ...,  1.2783,  1.6616,  0.4356],\n",
      "         [ 0.6506,  0.7046,  1.2624,  ...,  1.7854,  0.6201,  1.2388]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# let's pass the input to the embedding layer\n",
    "word_embeddings = embedding(sample_input)\n",
    "print(\"word embeddings tensor = \\n\", word_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Notice here each word in one sentence forms an embedding of (1,300) vector. Since we have 4 sentences in each document, the word_embedding size becomes (2,4,300)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print the shape of this tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word embeddings shape =  torch.Size([2, 4, 300])\n"
     ]
    }
   ],
   "source": [
    "print(\"word embeddings shape = \", word_embeddings.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each dimension can be interpreted as:\n",
    "- **First dimension:** (**2**,4,300): We have ``2 examples`` (that is, our ``x_1`` and ``x_2``). (Note: We will be passing a whole batch to the ``Embedding`` class and so this first dimension will be equal to the ``batch size``.\n",
    "- **Second dimension:** (2,**4**,300): For each of the two examples, we have a ``max sequence length`` = 4 (x_1 and x_2 each had 4 indexes).\n",
    "- **Third dimension:** (2,4,**300**): The ``word vector dimension`` is set to 300."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max sequence length: Another note"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall from above we mentioned Pytorch automatically infers the max sequence length for each batch. \n",
    "For the example above (as you can see from the second dimension returned by ``word_embeddings.size()``, Pytorch \n",
    "inferred the max seq length for this batch of two sentences is 4.\n",
    "\n",
    "Let's just adjust the second example, **adding two more words** (the string \"and kind\"). Note, both our ``VOCAB_SIZE`` and ``WORD_VEC_SIZE`` stay the same as before. We assign the word \"and\" an index of \"7\" and the word \"considerate\" an index of \"60\". \n",
    "\n",
    "Note that we have to pad the first example ``x_1`` with zeros (we will explicitly set the padding index to zero later when we define the embedding layer) in the end. (Try removing the zero padding. You will get an error.):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_1 = \"He is very nice\"\n",
    "x_2 = \"She is very kind and considerate\"\n",
    "x_1 = [1, 25, 40, 5, 0, 0]   # the zeros must be manually padded here\n",
    "x_2 = [4, 25, 40, 99, 7, 60]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create a new ``embedding`` layer by creating a new instance of the ``Embedding`` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of the embedding lookup table =  torch.Size([100, 300])\n"
     ]
    }
   ],
   "source": [
    "# constructing an embedding layer:\n",
    "padded_embedding = nn.Embedding(VOCAB_SIZE, WORD_VEC_SIZE, padding_idx=0)\n",
    "print(\"size of the embedding lookup table = \", padded_embedding.weight.data.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that **padded_embedding** embedding layer specifies the padding index (corresponds to an embedding initialized to all zeros). In this example, the zeroth index is dedicated for storing padding embedding (vector initialized to all zeros).\n",
    "\n",
    "Let's create sample input and pass it to the embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input (word indices) tensor = \n",
      " tensor([[ 1, 25, 40,  5,  0,  0],\n",
      "        [ 4, 25, 40, 99,  7, 60]])\n",
      "input (word indices) shape =  torch.Size([2, 6])\n",
      "tensor([[[-0.6576,  1.5036, -1.1379,  ..., -0.7042, -0.3216,  0.6035],\n",
      "         [ 2.0925, -0.2163,  1.0738,  ..., -1.3819, -1.5071, -2.0384],\n",
      "         [-0.3410,  0.9473,  0.2688,  ...,  0.7882,  1.2361,  0.7345],\n",
      "         [-0.8911,  0.1360, -0.1262,  ...,  0.1120,  0.4622, -1.0694],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 2.0051,  0.1782,  1.1753,  ...,  0.1490, -0.0251, -0.7231],\n",
      "         [ 2.0925, -0.2163,  1.0738,  ..., -1.3819, -1.5071, -2.0384],\n",
      "         [-0.3410,  0.9473,  0.2688,  ...,  0.7882,  1.2361,  0.7345],\n",
      "         [ 1.5075, -0.4779, -0.3377,  ...,  0.9493,  1.3547, -1.0004],\n",
      "         [ 0.5206, -0.6316,  0.4947,  ...,  0.5175, -0.6837,  0.3200],\n",
      "         [-0.1060, -2.1154, -1.3870,  ..., -0.9027, -0.4275,  0.6801]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# let's create a sample input\n",
    "sample_input = torch.LongTensor([ x_1, x_2 ])\n",
    "print(\"input (word indices) tensor = \\n\", sample_input)\n",
    "print(\"input (word indices) shape = \", sample_input.size())\n",
    "\n",
    "# let's retrieve the word embeddings by passing the sample input to the layer\n",
    "word_embeddings = padded_embedding(sample_input)\n",
    "print(word_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we inspect the shape of the new tensor ``word_embeddings``, we will see the second dimension now changed to 6, to match the max sequence length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 300])\n"
     ]
    }
   ],
   "source": [
    "print(word_embeddings.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does Pytorch initialize word vector dimensions/weights?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that Pytorch initializes the word vectors from a **normal distribution** $ \\mathcal{N}(0, 1) $. The word embedding weights are by default learnable parameters in Pytorch and so they will be adjusted during training. (Note: These weights can be initialized from an external word embedding tool such as [Word2vec](https://code.google.com/archive/p/word2vec/), [Fasttext](https://fasttext.cc/), or [Glove](https://nlp.stanford.edu/projects/glove/)). Also, the weights can be frozen (by setting ``embedding.weigh.required_grad`` flag to False), which is a reasonable option when initialized from an external tool. You can choose to keep learning them within the model with your training data). Below we show the ones initialized from a normal distribution by Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.7115,  1.1786, -0.3746,  ...,  0.4107,  1.3485,  0.3292],\n",
       "        [ 0.1901, -2.3038,  1.6044,  ...,  1.3484, -0.5816, -0.3130],\n",
       "        [ 0.7274, -0.2286,  1.8600,  ...,  0.1848,  2.1144,  0.3943],\n",
       "        ...,\n",
       "        [ 1.2256, -0.1416, -0.5852,  ..., -0.4734, -1.0416, -0.2759],\n",
       "        [ 0.7502, -0.0846, -2.0633,  ..., -0.6296, -0.6940,  0.2222],\n",
       "        [ 0.6506,  0.7046,  1.2624,  ...,  1.7854,  0.6201,  1.2388]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More information about the ``Embedding`` class can be found [here](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html) ([source code](https://pytorch.org/docs/stable/_modules/torch/nn/modules/sparse.html#Embedding))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks\n",
    "Recurrent Neural Networks (RNNs) are used to model sequences of arbitrary length (e.g., sequence of words in a sentence, sequence of sentences in a document, sequence of frames in a video). RNNs typically use their internal state (memory) to process sequence of inputs. \n",
    "\n",
    "At each time-step, RNNs output a prediction and hidden state, feeding its previous hidden state into each next step. RNNs are applied in a wide range of NLP applications:\n",
    "- language modeling, where RNN can condition on **all** previous words in the corpus unlike n-gram language model\n",
    "- text classification, where the states act as features (we will see sentiment analysis in this notebook)\n",
    "- machine translation, where a RNN is used to process a sentence in source language and another RNN is used to decode the sentence in target language (we will see this in the \"Machine Translation\" folder)\n",
    "- sequence labeling, where the states in RNN are used to predict a category for each item in the sequence \n",
    "\n",
    "Recommended reading for understanding the theory of RNNs: https://github.com/UBC-NLP/dlnlp2019/blob/master/slides/RNN.pdf \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grabbing few tweets using torchtext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us follow [**torchtext** tutorial](https://github.com/Georgeanna-Li/Machine_Learning/blob/master/Supervised_ml/Pytorch/torchtext_tutorial.ipynb) to read few tweets from the [sentiment analysis dataset](http://alt.qcri.org/semeval2016/task4/) used in the previous tutorial on feedforward neural networks. The preprocessed (tokenization, removing URLs, mentions, hashtags and so on) tweets are placed under ``data`` folder in three files as ``train.tsv``, ``dev.tsv`` and ``test.tsv``.  \n",
    "\n",
    "Let us view few tweets from ``train.tsv`` using pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dear &lt;&lt;&lt;MENTION&gt;&gt;&gt; the newooffice for mac is g...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;&lt;&lt;MENTION&gt;&gt;&gt; how about you make a system that...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i may be ignorant on this issue but should we ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>thanks to &lt;&lt;&lt;MENTION&gt;&gt;&gt; i just may be switchin...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>if i make a game as a &lt;&lt;&lt;HASHTAG&gt;&gt;&gt; universal ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  label\n",
       "0  dear <<<MENTION>>> the newooffice for mac is g...      2\n",
       "1  <<<MENTION>>> how about you make a system that...      2\n",
       "2  i may be ignorant on this issue but should we ...      2\n",
       "3  thanks to <<<MENTION>>> i just may be switchin...      2\n",
       "4  if i make a game as a <<<HASHTAG>>> universal ...      0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"./data/sentiment_analysis/train.tsv\", sep = '\\t', header=None, names=['tweet','label']) # the separator of tsv file is `\\t`\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We import the relevant packages, define the tokenizer and TorchText's fields.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import related packages\n",
    "import torchtext\n",
    "from torchtext.legacy.data import Field, LabelField\n",
    "from torchtext.legacy.data import TabularDataset\n",
    "\n",
    "# define the white space tokenizer to get tokens\n",
    "def tokenize_en(tweet):\n",
    "    \"\"\"\n",
    "    Tokenizes English tweet from a string into a list of strings (tokens)\n",
    "    \"\"\"\n",
    "    return tweet.strip().split()\n",
    "\n",
    "# define the TorchText's fields\n",
    "TEXT = Field(sequential=True, tokenize=tokenize_en, lower=True)\n",
    "LABEL = Field(sequential=False, unk_token = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To use the different splits (training, development and testing), we use `TabularDataset` class to load datasets.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test = TabularDataset.splits(\n",
    "    path=\"./data/sentiment_analysis/\", # the root directory where the data lies\n",
    "    train='train.tsv', validation=\"dev.tsv\", test=\"test.tsv\", # file names\n",
    "    format='tsv',\n",
    "    skip_header=False, # if your tsv file has a header, make sure to pass this to ensure it doesn't get proceesed as data!\n",
    "    fields=[('tweet', TEXT), ('label', LABEL)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Build our vocabulary to map words to integers.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train, min_freq=3) # builds vocabulary based on all the words that occur at least twice in the training set\n",
    "LABEL.build_vocab(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notice that this part is a false example of what will happen if we set `sort` as `False`.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initialize the iterators for the train, validation, and test data. Note that we set ``sort`` as `False` so as to not sort examples based on similar lengths which minimizes padding.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.legacy.data import Iterator, BucketIterator\n",
    "\n",
    "train_iter, val_iter, test_iter = BucketIterator.splits(\n",
    " (train, val, test), # we pass in the datasets we want the iterator to draw data from\n",
    " batch_sizes=(4,64,64),\n",
    " sort_key=lambda x: len(x.tweet), \n",
    " sort=False,\n",
    "# A key to use for sorting examples in order to batch together examples with similar lengths and minimize padding. \n",
    " sort_within_batch=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a batch of four examples and print them**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed tweets: \n",
      "0  sample: ['the', 'only', 'thing', 'that', 'i', 'am', 'afraid', 'of', 'if', 'bernie', 'sanders', 'is', 'president', 'he', 'may', 'die', 'in', 'office', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']  label: 1\n",
      "1  sample: ['iphone', 'launch', 'more', 'details', 'leak', 'ahead', 'of', \"apple's\", 'september', '<<<digit>>>', 'event', 'iphone', 'launch', 'on', 'wednesday', 'is', 'just', '<<<url>>>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']  label: 1\n",
      "2  sample: ['finishing', 'jurassic', 'park', 'for', 'like', 'the', 'time', 'tomorrow', 'film', 'class', 'is', 'gonna', 'have', 'a', 'lot', 'of', '<unk>', 'for', 'me', 'all', 'the', 'batman', 'films', 'star', 'wars']  label: 1\n",
      "3  sample: ['in', 'the', '<unk>', 'of', 'being', 'fail', 'and', 'balanced', 'may', 'i', 'present', 'the', '<<<mention>>>', 'galaxy', 'note', '<<<digit>>>', '<<<url>>>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']  label: 1\n"
     ]
    }
   ],
   "source": [
    "# create a single batch and terminate the loop\n",
    "for batch in train_iter:\n",
    "    tweets = batch.tweet\n",
    "    labels = batch.label\n",
    "    break  #we use first batch as an example.\n",
    "\n",
    "# print the four examples with padding and corresponding label\n",
    "print(\"processed tweets: \")\n",
    "for j in range(tweets.shape[1]): # sample loop\n",
    "    tokens = []\n",
    "    for i in range(tweets.shape[0]): # token loop\n",
    "        tokens.append(TEXT.vocab.itos[tweets[i,j]])\n",
    "    print(j,\" sample:\",tokens,\" label:\", labels[j].item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notice that the above part is not a good example where we set `sort` as `False`!**\n",
    "\n",
    "We see that there are a lot of paddings in those processed tweets. The reason is that we are not sorting sentences by length, therefore we put some sentences with different lengths together, which results in unnecessary paddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now we set ``sort`` as `True` so as to sort examples based on similar lengths which minimizes padding.**\n",
    "\n",
    "**Let us initialize the new iterators for the train, validation, and test data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, val_iter, test_iter = BucketIterator.splits(\n",
    " (train, val, test), # we pass in the datasets we want the iterator to draw data from\n",
    " batch_sizes=(4,64,64),\n",
    " sort_key=lambda x: len(x.tweet), \n",
    " sort=True,\n",
    "# A key to use for sorting examples in order to batch together examples with similar lengths and minimize padding. \n",
    " sort_within_batch=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let us pick up 4 tweets from the training set and convert them to tensors.**\n",
    "\n",
    "**Create a batch of four examples and print them**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 191,    4, 1273,   49],\n",
      "        [  14,   82,    2,    0],\n",
      "        [   2,   73,  215,   18],\n",
      "        [ 598,  145,   48,  215],\n",
      "        [  21,   21,   21,   21]])\n",
      "tensor([0, 1, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "# create a single batch and terminate the loop\n",
    "for batch in train_iter:\n",
    "    tweets = batch.tweet\n",
    "    labels = batch.label\n",
    "    print(tweets)\n",
    "    print(labels)\n",
    "    break  #we use first batch as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed tweets: \n",
      "0  sample: ['ihop', 'is', 'the', 'move', 'tomorrow']  label: 0\n",
      "1  sample: ['<<<mention>>>', 'make', 'david', 'beckham', 'tomorrow']  label: 1\n",
      "2  sample: ['bringing', 'the', 'bentley', 'out', 'tomorrow']  label: 0\n",
      "3  sample: ['new', '<unk>', 'with', 'bentley', 'tomorrow']  label: 0\n"
     ]
    }
   ],
   "source": [
    "# print the four examples with padding and corresponding label\n",
    "print(\"processed tweets: \")\n",
    "for j in range(tweets.shape[1]): # sample loop\n",
    "    tokens = []\n",
    "    for i in range(tweets.shape[0]): # token loop\n",
    "        tokens.append(TEXT.vocab.itos[tweets[i,j]])     # itos = index to string\n",
    "    print(j,\" sample:\",tokens,\" label:\", labels[j].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 191,    4, 1273,   49],\n",
       "        [  14,   82,    2,    0],\n",
       "        [   2,   73,  215,   18],\n",
       "        [ 598,  145,   48,  215],\n",
       "        [  21,   21,   21,   21]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a single hidden layer RNN\n",
    "\n",
    "PyTorch has ``torch.nn.RNN`` module that implements the vanilla (Elman) RNN with *tanh* or *ReLU* non-linearity. The documentation for this module is [here](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html?highlight=nn%20rnn#torch.nn.RNN). Let us use the sample batch of five examples created before to understand this module.\n",
    "\n",
    "Here we will represent the input tweet using a sequence of word embeddings (for each word present in the tweet). We will use ``torch.nn.Embedding`` layer to store word vectors corresponding to words in the vocabulary.\n",
    "\n",
    "Before implementing the embedding module for our usecase, let us compute the size of the word vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3333\n"
     ]
    }
   ],
   "source": [
    "# print the size of the word vocabulary\n",
    "VOCAB_SIZE = len(TEXT.vocab.stoi)\n",
    "print(VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 3333 unique words in the vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us implement the embedding module (whose underlying weight matrix shape is (``vocabulary size`` $\\times$ ``word embedding size``) for our usecase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lookup table shape =  torch.Size([3333, 300])\n"
     ]
    }
   ],
   "source": [
    "# set the word embedding size\n",
    "WORD_VEC_SIZE = 300\n",
    "\n",
    "# an Embedding module containing 300 dimensional tensor for each word in the vocabulary\n",
    "# Note, the parameters to Embedding class below are:\n",
    "# num_embeddings (int): size of the dictionary of embeddings\n",
    "# embedding_dim (int): the size of each embedding vector\n",
    "# For more details on Embedding class, see: https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/sparse.py\n",
    "embedding = nn.Embedding(VOCAB_SIZE, WORD_VEC_SIZE, sparse=True)\n",
    "print(\"lookup table shape = \", embedding.weight.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now feed the tensors of our sample batch to the embedding module and extract the sequence of word embeddings for each tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************** \n",
      " Word ids for the first batch (recall, it has 4 sentences, each column representing a sentence): \n",
      " tensor([[ 191,    4, 1273,   49],\n",
      "        [  14,   82,    2,    0],\n",
      "        [   2,   73,  215,   18],\n",
      "        [ 598,  145,   48,  215],\n",
      "        [  21,   21,   21,   21]]) \n",
      " **************************************************\n",
      "************************************************** \n",
      " Tweet input word embeddings size:  torch.Size([5, 4, 300]) \n",
      " **************************************************\n"
     ]
    }
   ],
   "source": [
    "# print tensor containing word ids for our batch\n",
    "print(\"*\"*50, \"\\n Word ids for the first batch (recall, it has 4 sentences, each column representing a sentence): \\n\", tweets.data, \"\\n\",\"*\"*50,)\n",
    "\n",
    "# feed the \"word ids\" tensor to the embedding module\n",
    "tweet_input_embeddings = embedding(tweets)\n",
    "\n",
    "# print the dimensions of the tweet_embeddings\n",
    "print(\"*\"*50, \"\\n Tweet input word embeddings size: \", tweet_input_embeddings.size(), \"\\n\",\"*\"*50,) \n",
    "# first dimension - sequence length: number of words per example (same across the whole batch, after padding) --> max_seq = 22\n",
    "# second dimension -  batch size / number of examples in the batch --> 4\n",
    "# third dimension - number of dimensions in the word vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the dimensions are not typical, because normally in a neural network we would have the first dimension as *batch size*, the second dimension as *sequence length*, and the third dimension as *number of dimensions in the word vector*. \n",
    "\n",
    "So we should always be careful about the dimensions.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's actually view the actual word embeddings tensor for this batch:\n",
    "\n",
    "- Take the first matrix for example, the first line is the *first token of the tweet 1*, and the second line is the *first token of the tweet 2*,...\n",
    "\n",
    "- For the second matrix, the first line is the *second token of the tweet 1*..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************** \n",
      " Embeddings for the first batch: \n",
      " tensor([[[ 0.0497, -0.6613,  0.8796,  ..., -0.4001,  1.3097, -0.4884],\n",
      "         [ 0.1035,  0.8628, -1.6050,  ..., -0.2178,  1.2235, -0.4696],\n",
      "         [ 0.2006,  0.0309,  0.6056,  ...,  0.2487,  0.3876,  1.3356],\n",
      "         [-1.2187,  0.3973, -0.8658,  ...,  1.3622,  0.8258,  0.1237]],\n",
      "\n",
      "        [[-0.2521,  0.4899,  0.2510,  ..., -0.2158,  0.9611, -1.4206],\n",
      "         [ 0.9853,  1.4709,  0.7310,  ..., -0.7995,  0.6693, -1.1515],\n",
      "         [ 0.2335, -0.0525,  0.1288,  ...,  0.4827,  0.1625,  1.3136],\n",
      "         [ 2.0688,  1.0594, -1.3368,  ...,  2.0239,  1.2129,  0.7414]],\n",
      "\n",
      "        [[ 0.2335, -0.0525,  0.1288,  ...,  0.4827,  0.1625,  1.3136],\n",
      "         [-1.7450, -0.9481,  1.0052,  ..., -0.1025, -1.1641, -1.5934],\n",
      "         [ 0.9274,  0.7615, -0.4993,  ..., -0.4135,  0.5053,  1.0365],\n",
      "         [ 1.0659, -0.4157, -0.1426,  ..., -0.3867,  1.8647,  0.0849]],\n",
      "\n",
      "        [[-0.0097, -0.2096, -0.0502,  ...,  1.7240, -1.2708,  0.5092],\n",
      "         [ 0.4210, -0.9114, -1.8391,  ...,  0.9806, -1.2720,  0.0070],\n",
      "         [-0.5046, -0.0532,  1.1351,  ..., -0.2897,  0.1450, -0.2530],\n",
      "         [ 0.9274,  0.7615, -0.4993,  ..., -0.4135,  0.5053,  1.0365]],\n",
      "\n",
      "        [[ 0.4481,  1.1751,  1.3331,  ..., -0.0131, -0.0657, -0.1492],\n",
      "         [ 0.4481,  1.1751,  1.3331,  ..., -0.0131, -0.0657, -0.1492],\n",
      "         [ 0.4481,  1.1751,  1.3331,  ..., -0.0131, -0.0657, -0.1492],\n",
      "         [ 0.4481,  1.1751,  1.3331,  ..., -0.0131, -0.0657, -0.1492]]],\n",
      "       grad_fn=<EmbeddingBackward0>) \n",
      " **************************************************\n"
     ]
    }
   ],
   "source": [
    "print(\"*\"*50, \"\\n Embeddings for the first batch: \\n\", tweet_input_embeddings, \"\\n\",\"*\"*50,) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we are seeing is the actual word vectors representing each of the 4 sentences (i.e., whole batch).\n",
    "This is dimension 2 in ``tweet_input_embeddings``. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_input_embeddings.size()[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned, ``max_seq length`` for this batch is ``5``, which is dimension 1 (indexed as 0 in Pytorch, similar to Python) \n",
    "in ``tweet_input_embeddings``: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_input_embeddings.size()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, dimension 3 in ``tweet_input_embeddings`` (indexed as 2) is the size of the word vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_input_embeddings.size()[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the vector for the ``first word`` in the ``first sentence`` in the batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 300])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_input_embeddings[:1, :1, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0497, -0.6613,  0.8796, -2.1471, -0.9065, -0.1080, -1.8510,\n",
       "           0.8044,  1.2097, -0.7341,  0.8311,  0.7651, -1.1590,  0.5495,\n",
       "           0.6600, -0.5957, -0.9349,  0.1479,  0.5176, -1.7738,  0.1265,\n",
       "          -1.3657, -0.1073, -2.2124, -0.6851,  0.9176,  0.2406,  0.5285,\n",
       "           0.4733, -1.1413,  0.3945,  0.6287,  0.0211,  0.4979, -1.3710,\n",
       "          -0.1173,  1.0671, -0.3403,  0.9572,  1.2945,  1.2748, -0.5902,\n",
       "           0.8075,  2.3248,  0.4592, -0.2317, -1.5478,  0.5803, -0.9309,\n",
       "           1.1187, -0.0721,  0.1367,  0.1587, -0.2785,  0.6543, -0.8597,\n",
       "          -0.9292,  0.9744,  0.7614, -0.1196, -1.2134,  0.6489, -0.4959,\n",
       "           1.9132,  0.1632,  0.5426,  1.2953,  1.6276,  1.5345,  2.0170,\n",
       "          -1.5114, -0.2768,  1.7806,  0.9310,  0.9351, -0.0390, -0.2544,\n",
       "           0.5106,  2.1638,  0.4476, -0.7431,  0.5342, -0.5459, -0.2583,\n",
       "           0.3176, -1.4729,  1.0111,  0.4005,  0.1107, -0.5714,  1.3365,\n",
       "           0.3088, -0.9442,  1.5314,  1.3513,  0.8487, -2.0272,  1.0614,\n",
       "           1.8746,  0.7373,  0.1480, -0.9680, -0.7397, -1.3238,  0.8176,\n",
       "           0.1684,  0.9922, -1.0861,  1.3252,  0.7376,  1.1608, -1.7706,\n",
       "          -0.5288,  0.4983,  1.2058,  0.5005, -0.6752,  0.8746, -1.1899,\n",
       "           0.0368,  0.6582, -1.0793,  0.9805,  0.7269,  1.4975, -0.1580,\n",
       "          -0.6158,  0.0174, -0.6225, -0.7865, -0.3093, -0.0513,  1.1087,\n",
       "           0.2112, -0.7079, -1.1057,  0.7921, -0.1898, -0.7521, -0.9248,\n",
       "           0.1147,  0.3479,  0.7752,  1.1533, -0.1973, -0.2044,  0.7937,\n",
       "          -0.4728,  1.0654,  0.9693, -0.1489,  1.6730,  0.3668, -1.6008,\n",
       "          -0.9359, -0.4660,  2.0154, -0.1268, -0.1771, -1.9661,  0.1053,\n",
       "          -0.1509, -0.8553,  0.8224,  0.0365,  0.4589,  1.2274, -1.5972,\n",
       "          -0.5379,  1.5980,  0.2026, -0.1739, -1.1401,  1.4618, -0.6968,\n",
       "           2.7479,  0.1964,  1.1798,  0.1047, -1.3657,  1.2171,  0.1775,\n",
       "          -2.0473,  0.5351, -1.5391, -2.1154,  0.0768, -0.4668,  1.2644,\n",
       "          -0.3978,  0.0710, -0.5467, -1.9413, -0.4317, -0.6236, -0.7009,\n",
       "          -0.8507,  1.0782, -0.5980, -2.4838,  0.7748,  0.9088, -0.5591,\n",
       "          -1.1193, -0.4923, -0.7621,  0.6329, -0.4891, -1.0933, -0.4077,\n",
       "           0.3448, -2.2770, -0.2680,  0.1018,  0.8772,  1.6257,  1.3852,\n",
       "           0.7546, -0.8234,  1.8842,  2.5768,  2.1083, -0.9925, -1.2748,\n",
       "          -1.9002,  1.2878,  0.8359, -0.4048,  0.2420,  1.1804,  1.6524,\n",
       "          -1.1350, -0.6766,  1.4391,  0.3842, -0.6893, -1.0283,  0.7274,\n",
       "           0.5873, -0.4592, -0.1157, -0.5337,  0.0418,  1.4994,  0.1679,\n",
       "           0.0563,  0.8079,  1.0203, -0.0816, -0.6630,  0.4871,  1.0607,\n",
       "           0.4514,  1.6272, -1.1066, -1.4144,  0.9336, -1.5918, -1.3348,\n",
       "          -2.1594,  0.4712,  0.1713,  0.6128, -0.9739, -0.9616, -0.7835,\n",
       "          -0.6263,  0.7497,  1.0595, -1.6561,  0.3399,  0.3763,  0.4121,\n",
       "          -0.2201,  1.8073,  0.3541,  0.3983,  0.6450, -2.1414,  0.0924,\n",
       "          -0.5200, -1.2707,  1.2154, -0.7300, -0.1284,  0.8624,  1.3381,\n",
       "           0.7302,  0.0381,  0.0040, -0.6247, -0.5475, -0.9952,  1.0248,\n",
       "           0.9220,  0.1400, -1.5502, -0.4001,  1.3097, -0.4884]]],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_input_embeddings[:1, :1, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the ``first 5 dimensions`` of that same ``first word`` of the ``first sentence``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0497, -0.6613,  0.8796, -2.1471, -0.9065]]],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_input_embeddings[:1, :1, :5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following shows you the ``first 5 dimensions`` of the ``first word`` from ``each of the 4 sentences``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0497, -0.6613,  0.8796, -2.1471, -0.9065],\n",
       "         [ 0.1035,  0.8628, -1.6050, -1.6371,  0.6035],\n",
       "         [ 0.2006,  0.0309,  0.6056, -0.3022,  0.0536],\n",
       "         [-1.2187,  0.3973, -0.8658, -0.2913,  1.1139]]],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_input_embeddings[:1, :, :5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following shows you the ``last 7 dimensions`` of the ``last word`` from ``the last sentence``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.3923, -0.4266, -0.0244,  0.4538, -0.0131, -0.0657, -0.1492]]],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_input_embeddings[-1:, -1:, -7:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be passing the sequence of word embeddings for each sentence in the batch as input to the RNN. But let's now define an RNN module first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(300, 50)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "define the RNN module\n",
    "\"\"\"\n",
    "# first input - number of dimensions for word vectors for a vector x (300, size of the word embedding)\n",
    "# second input - number of nodes in hidden state h_t (50, size of the hidden layer)\n",
    "# third input - number of recurrent layers (we set it to 1)\n",
    "rnn = nn.RNN(input_size=300, hidden_size=50, num_layers=1) # input_size, hidden_size, num_layers\n",
    "print(rnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now pass the ``tweet_input_embeddings`` (representations of words in our batch) to RNN. Before we do, we need to know RNN also *optionally* takes a parameter for the ``initial hidden state h0`` (that is, the hidden state we will input to the model before the forward propagation starts. If this vector is not explicitly specified, Pytorch will just initialize h0 to a tensor of zeros.)\n",
    "\n",
    "Let's construct an ``initial hidden state h0``. Pay attention to the shape of its tensor, and what each of the 3 parameters \n",
    "mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape as as expected:  torch.Size([1, 4, 50])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "hidden layer at time-step 0 (h_0)\n",
    "\"\"\"\n",
    "# first dimension - number of RNN layers (1)\n",
    "# second dimension - number of examples/sentences in a batch\n",
    "# third dimension - number of nodes in hidden layer (50, size of the hidden layer, that we specified as hidden_size in RNN construction)\n",
    "h0 = torch.randn(1, 4, 50)\n",
    "print(\"The shape as as expected: \", h0.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us feed both the hidden representation constructed above and tweet embeddings to our RNN model.\n",
    "We will get back two objects ``output`` and ``hn`` that we will need to understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "forward propagation over the RNN model\n",
    "\"\"\"\n",
    "output, hn = rnn(tweet_input_embeddings, h0) \n",
    "# h0 is optional input, defaults to tensor of 0's of apprpriate size (num_layers, batch, hidden_size) when not provided"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what is ``output``? Well, let's inspect its shape first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output size:  torch.Size([5, 4, 50])\n"
     ]
    }
   ],
   "source": [
    "# output = seq_len, batch, hidden_size (output features from last layer of RNN)\n",
    "print(\"output size: \", output.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what we need to know about ``output``:\n",
    "- The first dimension in the ``output`` tensor is the ``max_seq length`` (5). \n",
    "- The second dimension is ``batch_size`` (the number of examples/sentences in our batch = 4).\n",
    "- The third dimension is the ``size of nodes/units`` in our hidden layer (=50). \n",
    "\n",
    "What is the shape of `hn` (tensor containing the hidden state for t=max_seq_length) ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last hidden state size:  torch.Size([1, 4, 50])\n"
     ]
    }
   ],
   "source": [
    "# h_n = num_layers, batch, hidden_size (hidden state for t=seq_len or hidden state at last timestep)\n",
    "print(\"last hidden state size: \", hn.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what we need to know about ``hn``:\n",
    "- ``hn`` is a tensor of shape (num_layers, batch_size, hidden_size / number of hidden layer nodes) containing the hidden state for the last ``time step`` \n",
    "(``t = max_seq_length``)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can take the output representation for a tweet after processing the last token (t=seq_len or last timestep) and call the resulting representation as the tweet representation that **\"summarizes\" the information present** in the tweet. This tweet representation can further be used for a useful task like tweet classification (we will try out sentiment analysis later in this tutorial) by adding a classification module on top of the tweet representation.\n",
    "\n",
    "> This `h_n` is the recurrent hidden layer of RNN.\n",
    "\n",
    "Let us compute the final tweet representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet output embeddings size:  torch.Size([4, 50])\n"
     ]
    }
   ],
   "source": [
    "tweet_output_embeddings = output[-1,:,:] # -1 fetches the embeddings from the last timestep\n",
    "print(\"tweet output embeddings size: \", tweet_output_embeddings.size())\n",
    "# first dimension - number of tweets in the batch (4)\n",
    "# second dimension - number of features in hidden state h_t (50, size of the hidden layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1722,  0.9785, -0.4916,  0.8167,  0.9822, -0.8901,  0.7265,  0.9476,\n",
       "          0.6588,  0.0333,  0.6143, -0.7835, -0.3580, -0.4351,  0.7930,  0.1546,\n",
       "         -0.7270,  0.9206,  0.9844, -0.7004,  0.6254, -0.2912, -0.8590, -0.5337,\n",
       "          0.9171,  0.7269,  0.8216,  0.2037,  0.8335,  0.8913,  0.2106, -0.2162,\n",
       "         -0.8760,  0.9680, -0.9720, -0.7904, -0.1621, -0.4894, -0.9454, -0.9038,\n",
       "          0.8667, -0.8328,  0.3629, -0.3457,  0.7354, -0.6668,  0.5052, -0.9672,\n",
       "          0.4521,  0.4315],\n",
       "        [ 0.2722,  0.9633,  0.3640,  0.4598,  0.9462, -0.9340,  0.6756,  0.8687,\n",
       "          0.2608,  0.6020,  0.5295, -0.6816,  0.0441,  0.1948,  0.9107, -0.6932,\n",
       "         -0.9248,  0.9949,  0.9446, -0.8027,  0.3860, -0.1418, -0.5424, -0.9260,\n",
       "          0.8161,  0.8401,  0.9568,  0.4214,  0.6950,  0.9749, -0.3559,  0.3205,\n",
       "         -0.9784,  0.9413, -0.8600, -0.9490,  0.1046, -0.4076, -0.9377, -0.8248,\n",
       "          0.6900, -0.7823, -0.1246, -0.3968,  0.8810, -0.9490, -0.4256, -0.9937,\n",
       "          0.2663,  0.4070],\n",
       "        [ 0.0870,  0.9760,  0.3567,  0.6865,  0.9157, -0.8945,  0.6805,  0.9692,\n",
       "          0.8206,  0.2574,  0.7100, -0.6768,  0.0678, -0.2017,  0.9197, -0.6785,\n",
       "         -0.9144,  0.9814,  0.9701, -0.9037,  0.7972,  0.4286, -0.5634, -0.8166,\n",
       "          0.8691,  0.8234,  0.9444,  0.1600,  0.7557,  0.9257,  0.0842,  0.0338,\n",
       "         -0.9796,  0.8543, -0.9663, -0.9621, -0.3023, -0.2287, -0.9563, -0.8429,\n",
       "          0.6166, -0.8436, -0.2596, -0.2899,  0.6564, -0.8435, -0.1104, -0.9875,\n",
       "         -0.1748,  0.4481],\n",
       "        [ 0.2708,  0.9640,  0.1157,  0.0508,  0.9417,  0.0705,  0.4083,  0.9448,\n",
       "          0.6399,  0.1245,  0.5015, -0.6766,  0.0908, -0.7696,  0.8865, -0.3771,\n",
       "         -0.9576,  0.9934,  0.9615, -0.2788,  0.8926,  0.2889, -0.5345, -0.7386,\n",
       "          0.9474,  0.8214,  0.9581, -0.4807,  0.4347,  0.9356, -0.5006, -0.3934,\n",
       "         -0.9904,  0.8718, -0.9873, -0.4414,  0.6490, -0.2191, -0.6629, -0.8250,\n",
       "          0.6199, -0.7952, -0.1312, -0.5950,  0.9780, -0.9574,  0.0499, -0.9894,\n",
       "          0.2956, -0.2875]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_output_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayered RNN\n",
    "\n",
    "For some applications, we may need more than one hidden layer for RNN to model the information flow. Adding more layers only requires few changes.\n",
    "\n",
    "Firstly, we change the ``num_layers`` argument to reflect the number of layers we want during the RNN module definition (we will define two hidden layers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "define the RNN module\n",
    "\"\"\"\n",
    "# first input - number of dimesnions for word vectors for a vector x (300, size of the word embedding)\n",
    "# second input - number of nodes in hidden layer (50, size of the hidden layer)\n",
    "# third input - number of recurrent layers (we set it to 2)\n",
    "rnn = nn.RNN(input_size=300, hidden_size=50, num_layers=2) # input_size, hidden_size, num_layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to single layered RNN, Multilayered RNN module takes two inputs: the ``initial hidden state h0`` for each element in the batch (at ``time step t=0``) and the ``input features`` (``tweet_input_embeddings`` in our case).\n",
    "\n",
    "Let us construct the new initial hidden state for a 2 layered RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape as as expected:  torch.Size([2, 4, 50])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "hidden layer at time-step 0 (h_0)\n",
    "\"\"\"\n",
    "# first dimension - number of RNN layers (2)\n",
    "# second dimension - number of examples/sentences in a batch (4)\n",
    "# third dimension - number of nodes in hidden layer (50, size of the hidden layer)\n",
    "h0 = torch.randn(2, 4, 50)\n",
    "print(\"The shape as as expected: \", h0.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us feed both the hidden representation constructed above and tweet embeddings to our RNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 4, 300])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "forward propagation over the RNN model\n",
    "\"\"\"\n",
    "print(tweet_input_embeddings.shape)\n",
    "output, hn = rnn(tweet_input_embeddings, h0) # h0 is optional input, defaults to tensor of 0's when not provided"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``output`` tensor contains the output features $h_t$ from the last layer of the RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output size:  torch.Size([5, 4, 50])\n"
     ]
    }
   ],
   "source": [
    "# output = seq_len, batch, hidden_size (output features from last layer of RNN)\n",
    "print(\"output size: \", output.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``hn`` is a tensor of shape (num_layers, batch_size, hidden_size / number of nodes in a hidden layer) containing the hidden state for last time step ``t = max_seq_len`` for the ``2 layered RNN``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last hidden state size:  torch.Size([2, 4, 50])\n"
     ]
    }
   ],
   "source": [
    "# h_n = num_layers, batch, hidden_size (hidden state for t=seq_len or hidden state at last timestep)\n",
    "print(\"last hidden state size: \", hn.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building tweet representation\n",
    "\n",
    "Actually, `output` is tensor containing the output features (h_t) from the last layer of the RNN, for each t. Namely, `output` returns all the hidden states of all time steps from the last layer of the RNN. Hence, the last element of `output` is `h_n`. \n",
    "Let us print them out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last element of output:\n",
      " tensor([[-2.3755e-01,  2.0197e-01, -4.5273e-01,  1.9134e-01, -6.4322e-02,\n",
      "          5.7595e-01,  6.2657e-01,  8.2071e-02, -1.2430e-03, -7.1888e-01,\n",
      "         -2.1794e-01, -2.0135e-01,  3.5560e-02,  7.9951e-02,  4.5778e-01,\n",
      "          1.0291e-01,  3.5427e-01, -3.7307e-02,  6.1022e-01,  6.7984e-02,\n",
      "          3.8143e-01,  2.4028e-01,  6.5714e-01,  5.3988e-01, -8.7714e-02,\n",
      "          4.6788e-01, -4.8817e-01, -6.7583e-01, -3.3352e-02, -1.0845e-01,\n",
      "          5.9589e-02,  5.0715e-01,  5.4126e-01,  1.7227e-02, -5.2711e-02,\n",
      "          7.4660e-01,  4.2723e-01,  4.2414e-01, -4.2217e-01,  2.3839e-01,\n",
      "          1.7724e-03,  7.6602e-02,  2.5165e-01, -7.8974e-02,  7.2263e-01,\n",
      "          3.4349e-01, -3.0204e-01, -7.6930e-01,  2.5551e-01,  5.3589e-01],\n",
      "        [-3.3300e-01,  4.5231e-01, -4.6670e-01,  1.1326e-02,  6.5857e-02,\n",
      "          6.2729e-01,  5.3091e-01,  6.5794e-01, -9.8252e-02, -6.1484e-01,\n",
      "         -2.4441e-01,  1.0896e-01,  2.3490e-01,  7.0867e-01,  4.7210e-01,\n",
      "         -2.3280e-01,  1.2244e-01, -4.2946e-02,  1.6792e-01,  4.9052e-01,\n",
      "          2.8362e-02,  3.4233e-01,  4.0837e-01,  4.6657e-01, -3.1687e-01,\n",
      "          7.0701e-01, -5.1584e-01, -6.0896e-01, -2.8509e-01,  1.6474e-01,\n",
      "         -7.4472e-02,  2.3624e-01, -1.1786e-02, -5.2384e-02, -6.8104e-04,\n",
      "          6.4940e-01, -1.7958e-01,  5.1350e-01, -5.2036e-01,  4.6151e-02,\n",
      "          5.9862e-01, -2.9422e-01,  5.5103e-01,  3.1124e-01,  7.0385e-01,\n",
      "          5.8320e-01, -4.0571e-01, -5.1312e-01,  1.9708e-01, -1.8774e-01],\n",
      "        [-6.7041e-01,  4.2482e-01, -2.6845e-01, -1.9867e-01, -3.2618e-01,\n",
      "          2.0260e-01,  7.0133e-01,  3.5784e-01,  1.2306e-01, -8.3805e-01,\n",
      "         -8.2320e-02, -1.2601e-01,  4.4190e-01,  4.8115e-01,  1.7538e-01,\n",
      "         -1.0305e-01,  3.6130e-01, -1.2699e-01,  1.0642e-01,  3.1982e-01,\n",
      "          2.1850e-01,  1.9559e-02,  7.2681e-01,  3.5036e-01, -2.3172e-01,\n",
      "          5.8650e-01, -2.6681e-01, -5.5606e-01,  2.9385e-01,  1.7063e-01,\n",
      "          1.8966e-01,  6.3124e-01,  3.1036e-01,  5.8165e-01, -1.3471e-02,\n",
      "          3.8323e-01,  3.4544e-01,  6.0903e-02, -5.1586e-01,  5.4400e-01,\n",
      "         -1.9195e-01, -1.3759e-01,  4.2418e-01,  1.3318e-01,  8.4644e-01,\n",
      "          5.5223e-01, -5.6175e-02, -4.8764e-01,  2.6764e-01,  2.1014e-01],\n",
      "        [-4.7650e-01,  4.8709e-01, -5.1262e-01, -3.7099e-01, -4.7594e-01,\n",
      "          3.2689e-01,  3.0526e-01,  4.0924e-02,  1.3043e-01, -8.8481e-01,\n",
      "         -9.6120e-02, -3.0941e-01,  2.0621e-01,  6.2395e-01, -4.3945e-01,\n",
      "         -1.7857e-01,  5.7696e-01, -1.2495e-01,  8.0152e-02,  4.3409e-02,\n",
      "          1.9005e-01, -2.1860e-01,  8.9021e-01,  3.3221e-01, -3.3927e-01,\n",
      "          2.1426e-01,  2.2006e-01,  3.4911e-02,  6.9073e-01, -1.5469e-01,\n",
      "         -4.0268e-01,  7.7982e-01,  4.3477e-02,  2.4028e-01, -3.6808e-01,\n",
      "          3.9234e-01,  3.8841e-01,  4.3857e-01, -8.9986e-02,  6.1988e-01,\n",
      "         -3.4588e-02, -2.4765e-01, -1.1714e-01, -1.9676e-01,  8.6390e-01,\n",
      "          7.8199e-01,  1.2798e-01,  5.8882e-02,  5.6546e-01,  3.0255e-01]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(\"last element of output:\\n\", output[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then let us print out the last hidden state of last layer. \n",
    "Notice, we have two RNN layer, we only want to use the hidden state of last layer. \n",
    "You can find the these value are same as `output[-1]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last hidden state h_n:\n",
      " tensor([[-2.3755e-01,  2.0197e-01, -4.5273e-01,  1.9134e-01, -6.4322e-02,\n",
      "          5.7595e-01,  6.2657e-01,  8.2071e-02, -1.2430e-03, -7.1888e-01,\n",
      "         -2.1794e-01, -2.0135e-01,  3.5560e-02,  7.9951e-02,  4.5778e-01,\n",
      "          1.0291e-01,  3.5427e-01, -3.7307e-02,  6.1022e-01,  6.7984e-02,\n",
      "          3.8143e-01,  2.4028e-01,  6.5714e-01,  5.3988e-01, -8.7714e-02,\n",
      "          4.6788e-01, -4.8817e-01, -6.7583e-01, -3.3352e-02, -1.0845e-01,\n",
      "          5.9589e-02,  5.0715e-01,  5.4126e-01,  1.7227e-02, -5.2711e-02,\n",
      "          7.4660e-01,  4.2723e-01,  4.2414e-01, -4.2217e-01,  2.3839e-01,\n",
      "          1.7724e-03,  7.6602e-02,  2.5165e-01, -7.8974e-02,  7.2263e-01,\n",
      "          3.4349e-01, -3.0204e-01, -7.6930e-01,  2.5551e-01,  5.3589e-01],\n",
      "        [-3.3300e-01,  4.5231e-01, -4.6670e-01,  1.1326e-02,  6.5857e-02,\n",
      "          6.2729e-01,  5.3091e-01,  6.5794e-01, -9.8252e-02, -6.1484e-01,\n",
      "         -2.4441e-01,  1.0896e-01,  2.3490e-01,  7.0867e-01,  4.7210e-01,\n",
      "         -2.3280e-01,  1.2244e-01, -4.2946e-02,  1.6792e-01,  4.9052e-01,\n",
      "          2.8362e-02,  3.4233e-01,  4.0837e-01,  4.6657e-01, -3.1687e-01,\n",
      "          7.0701e-01, -5.1584e-01, -6.0896e-01, -2.8509e-01,  1.6474e-01,\n",
      "         -7.4472e-02,  2.3624e-01, -1.1786e-02, -5.2384e-02, -6.8104e-04,\n",
      "          6.4940e-01, -1.7958e-01,  5.1350e-01, -5.2036e-01,  4.6151e-02,\n",
      "          5.9862e-01, -2.9422e-01,  5.5103e-01,  3.1124e-01,  7.0385e-01,\n",
      "          5.8320e-01, -4.0571e-01, -5.1312e-01,  1.9708e-01, -1.8774e-01],\n",
      "        [-6.7041e-01,  4.2482e-01, -2.6845e-01, -1.9867e-01, -3.2618e-01,\n",
      "          2.0260e-01,  7.0133e-01,  3.5784e-01,  1.2306e-01, -8.3805e-01,\n",
      "         -8.2320e-02, -1.2601e-01,  4.4190e-01,  4.8115e-01,  1.7538e-01,\n",
      "         -1.0305e-01,  3.6130e-01, -1.2699e-01,  1.0642e-01,  3.1982e-01,\n",
      "          2.1850e-01,  1.9559e-02,  7.2681e-01,  3.5036e-01, -2.3172e-01,\n",
      "          5.8650e-01, -2.6681e-01, -5.5606e-01,  2.9385e-01,  1.7063e-01,\n",
      "          1.8966e-01,  6.3124e-01,  3.1036e-01,  5.8165e-01, -1.3471e-02,\n",
      "          3.8323e-01,  3.4544e-01,  6.0903e-02, -5.1586e-01,  5.4400e-01,\n",
      "         -1.9195e-01, -1.3759e-01,  4.2418e-01,  1.3318e-01,  8.4644e-01,\n",
      "          5.5223e-01, -5.6175e-02, -4.8764e-01,  2.6764e-01,  2.1014e-01],\n",
      "        [-4.7650e-01,  4.8709e-01, -5.1262e-01, -3.7099e-01, -4.7594e-01,\n",
      "          3.2689e-01,  3.0526e-01,  4.0924e-02,  1.3043e-01, -8.8481e-01,\n",
      "         -9.6120e-02, -3.0941e-01,  2.0621e-01,  6.2395e-01, -4.3945e-01,\n",
      "         -1.7857e-01,  5.7696e-01, -1.2495e-01,  8.0152e-02,  4.3409e-02,\n",
      "          1.9005e-01, -2.1860e-01,  8.9021e-01,  3.3221e-01, -3.3927e-01,\n",
      "          2.1426e-01,  2.2006e-01,  3.4911e-02,  6.9073e-01, -1.5469e-01,\n",
      "         -4.0268e-01,  7.7982e-01,  4.3477e-02,  2.4028e-01, -3.6808e-01,\n",
      "          3.9234e-01,  3.8841e-01,  4.3857e-01, -8.9986e-02,  6.1988e-01,\n",
      "         -3.4588e-02, -2.4765e-01, -1.1714e-01, -1.9676e-01,  8.6390e-01,\n",
      "          7.8199e-01,  1.2798e-01,  5.8882e-02,  5.6546e-01,  3.0255e-01]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(\"last hidden state h_n:\\n\", hn[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Notice that the last element of output is exactly the same as the last timestep of hidden layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us compute the final tweet representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet output embeddings size:  torch.Size([4, 50])\n"
     ]
    }
   ],
   "source": [
    "tweet_output_embeddings = output[-1,:,:] # -1 fetches the embeddings from the last timestep\n",
    "print(\"tweet output embeddings size: \", tweet_output_embeddings.size())\n",
    "# first dimension - number of tweets in the batch (4)\n",
    "# second dimension - number of features in hidden state h_t (50, size of the hidden layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.3755e-01,  2.0197e-01, -4.5273e-01,  1.9134e-01, -6.4322e-02,\n",
       "          5.7595e-01,  6.2657e-01,  8.2071e-02, -1.2430e-03, -7.1888e-01,\n",
       "         -2.1794e-01, -2.0135e-01,  3.5560e-02,  7.9951e-02,  4.5778e-01,\n",
       "          1.0291e-01,  3.5427e-01, -3.7307e-02,  6.1022e-01,  6.7984e-02,\n",
       "          3.8143e-01,  2.4028e-01,  6.5714e-01,  5.3988e-01, -8.7714e-02,\n",
       "          4.6788e-01, -4.8817e-01, -6.7583e-01, -3.3352e-02, -1.0845e-01,\n",
       "          5.9589e-02,  5.0715e-01,  5.4126e-01,  1.7227e-02, -5.2711e-02,\n",
       "          7.4660e-01,  4.2723e-01,  4.2414e-01, -4.2217e-01,  2.3839e-01,\n",
       "          1.7724e-03,  7.6602e-02,  2.5165e-01, -7.8974e-02,  7.2263e-01,\n",
       "          3.4349e-01, -3.0204e-01, -7.6930e-01,  2.5551e-01,  5.3589e-01],\n",
       "        [-3.3300e-01,  4.5231e-01, -4.6670e-01,  1.1326e-02,  6.5857e-02,\n",
       "          6.2729e-01,  5.3091e-01,  6.5794e-01, -9.8252e-02, -6.1484e-01,\n",
       "         -2.4441e-01,  1.0896e-01,  2.3490e-01,  7.0867e-01,  4.7210e-01,\n",
       "         -2.3280e-01,  1.2244e-01, -4.2946e-02,  1.6792e-01,  4.9052e-01,\n",
       "          2.8362e-02,  3.4233e-01,  4.0837e-01,  4.6657e-01, -3.1687e-01,\n",
       "          7.0701e-01, -5.1584e-01, -6.0896e-01, -2.8509e-01,  1.6474e-01,\n",
       "         -7.4472e-02,  2.3624e-01, -1.1786e-02, -5.2384e-02, -6.8104e-04,\n",
       "          6.4940e-01, -1.7958e-01,  5.1350e-01, -5.2036e-01,  4.6151e-02,\n",
       "          5.9862e-01, -2.9422e-01,  5.5103e-01,  3.1124e-01,  7.0385e-01,\n",
       "          5.8320e-01, -4.0571e-01, -5.1312e-01,  1.9708e-01, -1.8774e-01],\n",
       "        [-6.7041e-01,  4.2482e-01, -2.6845e-01, -1.9867e-01, -3.2618e-01,\n",
       "          2.0260e-01,  7.0133e-01,  3.5784e-01,  1.2306e-01, -8.3805e-01,\n",
       "         -8.2320e-02, -1.2601e-01,  4.4190e-01,  4.8115e-01,  1.7538e-01,\n",
       "         -1.0305e-01,  3.6130e-01, -1.2699e-01,  1.0642e-01,  3.1982e-01,\n",
       "          2.1850e-01,  1.9559e-02,  7.2681e-01,  3.5036e-01, -2.3172e-01,\n",
       "          5.8650e-01, -2.6681e-01, -5.5606e-01,  2.9385e-01,  1.7063e-01,\n",
       "          1.8966e-01,  6.3124e-01,  3.1036e-01,  5.8165e-01, -1.3471e-02,\n",
       "          3.8323e-01,  3.4544e-01,  6.0903e-02, -5.1586e-01,  5.4400e-01,\n",
       "         -1.9195e-01, -1.3759e-01,  4.2418e-01,  1.3318e-01,  8.4644e-01,\n",
       "          5.5223e-01, -5.6175e-02, -4.8764e-01,  2.6764e-01,  2.1014e-01],\n",
       "        [-4.7650e-01,  4.8709e-01, -5.1262e-01, -3.7099e-01, -4.7594e-01,\n",
       "          3.2689e-01,  3.0526e-01,  4.0924e-02,  1.3043e-01, -8.8481e-01,\n",
       "         -9.6120e-02, -3.0941e-01,  2.0621e-01,  6.2395e-01, -4.3945e-01,\n",
       "         -1.7857e-01,  5.7696e-01, -1.2495e-01,  8.0152e-02,  4.3409e-02,\n",
       "          1.9005e-01, -2.1860e-01,  8.9021e-01,  3.3221e-01, -3.3927e-01,\n",
       "          2.1426e-01,  2.2006e-01,  3.4911e-02,  6.9073e-01, -1.5469e-01,\n",
       "         -4.0268e-01,  7.7982e-01,  4.3477e-02,  2.4028e-01, -3.6808e-01,\n",
       "          3.9234e-01,  3.8841e-01,  4.3857e-01, -8.9986e-02,  6.1988e-01,\n",
       "         -3.4588e-02, -2.4765e-01, -1.1714e-01, -1.9676e-01,  8.6390e-01,\n",
       "          7.8199e-01,  1.2798e-01,  5.8882e-02,  5.6546e-01,  3.0255e-01]],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_output_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN for Sentiment Analysis\n",
    "\n",
    "In this section we will implement RNN for classifying the sentiment of the tweet (same task used in our previous feedforward neural networks tutorial).\n",
    "\n",
    "We will pick up most of the functions from our feedforward neural networks code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the necessary imports\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "\n",
    "# set the seed (for reproducibility)\n",
    "manual_seed = 123\n",
    "torch.manual_seed(manual_seed)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "if n_gpu > 0:\n",
    "  torch.cuda.manual_seed(manual_seed)\n",
    "\n",
    "# hyperparameters\n",
    "MAX_EPOCHS = 5 # number of passes over the training data\n",
    "LEARNING_RATE = 0.3 # learning rate for the weight update rule\n",
    "NUM_CLASSES = 3 # number of classes for the problem\n",
    "EMBEDDING_SIZE = 300 # size of the word embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define the full RNN model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> For the dimension in `nn.LogSoftmax(dim=1)`, we are doing normalization over the classes(the second dimension)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "create a model for RNN\n",
    "\"\"\"\n",
    "class RNNmodel(nn.Module):\n",
    "  \n",
    "  def __init__(self, embedding_size, vocab_size, output_size, hidden_size, num_layers):\n",
    "    # In the constructor we define the layers for our model\n",
    "    super(RNNmodel, self).__init__()\n",
    "    # word embedding lookup table\n",
    "    self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_size, sparse=True)\n",
    "    # core RNN module\n",
    "    self.rnn_layer = nn.RNN(input_size=embedding_size, hidden_size=hidden_size, num_layers=num_layers) \n",
    "    # activation function\n",
    "    self.activation_fn = nn.ReLU()\n",
    "    # classification related modules\n",
    "    self.linear_layer = nn.Linear(hidden_size, output_size) \n",
    "    self.softmax_layer = nn.LogSoftmax(dim=1)\n",
    "    self.debug = False\n",
    "  \n",
    "  def forward(self, x):\n",
    "    # In the forward function we define the forward propagation logic\n",
    "    if self.debug:\n",
    "        print(\"input word indices shape = \", x.size())\n",
    "    out = self.embedding(x)\n",
    "    if self.debug:\n",
    "        print(\"word embeddings shape = \", out.size())\n",
    "    out, _ = self.rnn_layer(out) # since we are not feeding h_0 explicitly, h_0 will be initialized to zeros by default\n",
    "    if self.debug:\n",
    "        print(\"RNN output (features from last layer of RNN for all timesteps) shape = \", out.size())\n",
    "    # classify based on the hidden representation after RNN processes the last token\n",
    "    out = out[-1]   # get the final output\n",
    "    if self.debug:\n",
    "        print(\"Tweet embeddings or RNN output (features from last layer of RNN for the last timestep only) shape = \", out.size())\n",
    "    out = self.activation_fn(out)\n",
    "    if self.debug:\n",
    "        print(\"ReLU output shape = \", out.size())\n",
    "    out = self.linear_layer(out)\n",
    "    if self.debug:\n",
    "        print(\"linear layer output shape = \", out.size())\n",
    "    out = self.softmax_layer(out) # accepts 2D or more dimensional inputs\n",
    "    if self.debug:\n",
    "        print(\"softmax layer output shape = \", out.size())\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some additional hyperparameters for RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters of RNN\n",
    "HIDDEN_SIZE = 50 # no. of units in the hidden layer\n",
    "NUM_LAYERS = 2 # no. of hidden layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rest of the pipeline looks similar to our feedforward neural networks code (except that we are using **torchtext** instead of **DataLoader**):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def train(loader):\n",
    "    total_loss = 0.0\n",
    "    # iterate throught the data loader\n",
    "    num_sample = 0\n",
    "    for batch in loader:\n",
    "        # load the current batch\n",
    "        batch_input = batch.tweet\n",
    "        batch_output = batch.label\n",
    "    \n",
    "        batch_input = batch_input.to(device)\n",
    "        batch_output = batch_output.to(device)\n",
    "        # forward propagation\n",
    "        # pass the data through the model\n",
    "        model_outputs = model(batch_input)\n",
    "\n",
    "        # compute the loss\n",
    "        cur_loss = criterion(model_outputs, batch_output)\n",
    "        total_loss += cur_loss.item()\n",
    "\n",
    "        # backward propagation (compute the gradients and update the model)\n",
    "        # clear the buffer\n",
    "        optimizer.zero_grad()\n",
    "        # compute the gradients\n",
    "        cur_loss.backward()\n",
    "        # update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        num_sample += batch_output.shape[0]\n",
    "    return total_loss/num_sample\n",
    "\n",
    "# evaluation logic based on classification accuracy\n",
    "def evaluate(loader):\n",
    "    all_pred=[]\n",
    "    all_label = []\n",
    "    with torch.no_grad(): # impacts the autograd engine and deactivate it. reduces memory usage and speeds up computation\n",
    "        for batch in loader:\n",
    "             # load the current batch\n",
    "            batch_input = batch.tweet\n",
    "            batch_output = batch.label\n",
    "\n",
    "            batch_input = batch_input.to(device)\n",
    "            # forward propagation\n",
    "            # pass the data through the model\n",
    "            model_outputs = model(batch_input)\n",
    "            # identify the predicted class for each example in the batch\n",
    "            probabilities, predicted = torch.max(model_outputs.cpu().data, 1)\n",
    "            # put all the true labels and predictions to two lists\n",
    "            all_pred.extend(predicted)\n",
    "            all_label.extend(batch_output)\n",
    "            \n",
    "    accuracy = accuracy_score(all_label, all_pred)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define the RNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "model = RNNmodel(EMBEDDING_SIZE, VOCAB_SIZE, NUM_CLASSES, HIDDEN_SIZE, NUM_LAYERS) \n",
    "model.to(device) # ship it to the right device\n",
    "\n",
    "# define the loss function (last node of the graph)\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us make a full forward propagation pass over a sample input batch to the RNN model. Closely pay attention to the shapes of intermediate layers (by turning on debug mode of the model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample input =  tensor([[ 191,    4, 1273,   49],\n",
      "        [  14,   82,    2,    0],\n",
      "        [   2,   73,  215,   18],\n",
      "        [ 598,  145,   48,  215],\n",
      "        [  21,   21,   21,   21]])\n",
      "sample output =  tensor([0, 1, 0, 0])\n",
      "input word indices shape =  torch.Size([5, 4])\n",
      "word embeddings shape =  torch.Size([5, 4, 300])\n",
      "RNN output (features from last layer of RNN for all timesteps) shape =  torch.Size([5, 4, 50])\n",
      "Tweet embeddings or RNN output (features from last layer of RNN for the last timestep only) shape =  torch.Size([4, 50])\n",
      "ReLU output shape =  torch.Size([4, 50])\n",
      "linear layer output shape =  torch.Size([4, 3])\n",
      "softmax layer output shape =  torch.Size([4, 3])\n",
      "model prediction shape =  torch.Size([4, 3])\n",
      "loss =  1.115674614906311\n"
     ]
    }
   ],
   "source": [
    "# turn on the debug mode\n",
    "model.debug = True\n",
    "\n",
    "# print the sample input batch and labels\n",
    "print(\"sample input = \", tweets)\n",
    "print(\"sample output = \", labels)\n",
    "\n",
    "# feed the batch as input to the RNN model\n",
    "model_prediction = model(tweets)\n",
    "print('model prediction shape = ', model_prediction.size())\n",
    "\n",
    "# feed the model prediction and labels to the loss function\n",
    "loss = criterion(model_prediction, labels)\n",
    "print(\"loss = \", loss.item())\n",
    "\n",
    "# turn off the debug mode (as we go for training from now)\n",
    "model.debug = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We need to create a new directory 'ckpt/' to store our model checkpoint.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists(\"./ckpt\"): # check if the directory doesn't exist already\n",
    "    os.mkdir(\"./ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let us perform the training. We will save our model and optimizer at end of each epoch.**\n",
    "\n",
    "\n",
    "You can find more information of saving and loading model [here](https://pytorch.org/tutorials/beginner/saving_loading_models.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.2506, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [2/5], Loss: 0.2480, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [3/5], Loss: 0.2483, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [4/5], Loss: 0.2495, Training Accuracy: 0.5153, Validation Accuracy: 0.4217\n",
      "Epoch [5/5], Loss: 0.2504, Training Accuracy: 0.5170, Validation Accuracy: 0.4217\n"
     ]
    }
   ],
   "source": [
    "# create an instance of SGD with required hyperparameters\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# start the training\n",
    "for epoch in range(MAX_EPOCHS):\n",
    "    # train the model for one pass over the data\n",
    "    train_loss = train(train_iter)  \n",
    "    # compute the training accuracy\n",
    "    train_acc = evaluate(train_iter)\n",
    "    # compute the validation accuracy\n",
    "    val_acc = evaluate(val_iter)\n",
    "    \n",
    "    # print the loss for every epoch\n",
    "    print('Epoch [{}/{}], Loss: {:.4f}, Training Accuracy: {:.4f}, Validation Accuracy: {:.4f}'.format(epoch+1, MAX_EPOCHS, train_loss, train_acc, val_acc))\n",
    "    \n",
    "    # save model, optimizer, and number of epoch to a dictionary\n",
    "    model_save = {\n",
    "            'epoch': epoch,  # number of epoch\n",
    "            'model_state_dict': model.state_dict(), # model parameters \n",
    "            'optimizer_state_dict': optimizer.state_dict(), # save optimizer \n",
    "            'loss': train_loss # training loss\n",
    "            }\n",
    "    \n",
    "    # use torch.save to store \n",
    "    torch.save(model_save, \"./ckpt/model_{}.pt\".format(epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We trained the network only for 5 epochs, but it already overfits on validation set after epoch 2. \n",
    "In the coming sessions, we will look at methods to ``regularize`` the network (this will help us deal with overfitting)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load model checkpoint** \n",
    "\n",
    "When we have a trained model checkpint, we can load it using `torch.load()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNNmodel(\n",
      "  (embedding): Embedding(3333, 300, sparse=True)\n",
      "  (rnn_layer): RNN(300, 50, num_layers=2)\n",
      "  (activation_fn): ReLU()\n",
      "  (linear_layer): Linear(in_features=50, out_features=3, bias=True)\n",
      "  (softmax_layer): LogSoftmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# define a new model\n",
    "model2 = RNNmodel(EMBEDDING_SIZE, VOCAB_SIZE, NUM_CLASSES, HIDDEN_SIZE, NUM_LAYERS) \n",
    "\n",
    "# load checkpoint \n",
    "checkpoint = torch.load(\"./ckpt/model_1.pt\") # loading the model obatined after 2nd epoch\n",
    "\n",
    "# assign the parameters of checkpoint to this new model\n",
    "model2.load_state_dict(checkpoint['model_state_dict'])\n",
    "model2.to(device)\n",
    "\n",
    "print(model2) # can be used for inference or for further training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRUs\n",
    "\n",
    "Gated Recurrent Units (GRUs) are a variant of RNNs that use more complex units for activation. They are created to have more persistent memory thereby making them easier for RNNs to capture long-term dependencies.\n",
    "\n",
    "GRU is defined by ``torch.nn.GRU`` module and its documentation can be fetched [here](https://pytorch.org/docs/stable/generated/torch.nn.GRU.html#gru). Now let us define the GRU module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "define the GRU module\n",
    "\"\"\"\n",
    "# first input - number of word vector dimensions/embeddings\n",
    "# second input - number of nodes in hidden layer (50, size of the hidden layer)\n",
    "# third input - number of recurrent layers (2)\n",
    "gru_rnn = nn.GRU(input_size=300, hidden_size=50, num_layers=2) # input_size, hidden_size, num_layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to RNN, GRU module takes two inputs: *the initial hidden state for each element in the batch* (t=0) and the *input features* (``tweet_input_embeddings`` in our case).\n",
    "\n",
    "Let us feed both the initial hidden state and tweet embeddings to our GRU model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "forward propagation over the GRU model\n",
    "\"\"\"\n",
    "output, hn = gru_rnn(tweet_input_embeddings, h0) # h0 is optional input, defaults to tensor of 0's when not provided"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``output`` tensor contains the output features $h_t$ from the last layer of the GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output size:  torch.Size([5, 4, 50])\n"
     ]
    }
   ],
   "source": [
    "# output = seq_len, batch, hidden_size (output features from last layer of GRU)\n",
    "print(\"output size: \", output.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``hn`` is a tensor of shape (num_layers, batch_size, hidden_size / number of nodes in a hidden layer) containing the hidden state for last time step ``t = max_seq_len`` for the ``2 layered RNN``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last hidden state size:  torch.Size([2, 4, 50])\n"
     ]
    }
   ],
   "source": [
    "# h_n = num_layers, batch, hidden_size (hidden state for t=seq_len or hidden state at last timestep)\n",
    "print(\"last hidden state size: \", hn.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to RNN, you can compute the final tweet representation (representation from last hidden state for each tweet) as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet output embeddings size:  torch.Size([4, 50])\n"
     ]
    }
   ],
   "source": [
    "tweet_output_embeddings = output[-1,:,:] # -1 fetches the embeddings from the last timestep\n",
    "print(\"tweet output embeddings size: \", tweet_output_embeddings.size())\n",
    "# first dimension - number of tweets in the batch (5)\n",
    "# second dimension - number of features in hidden state h_t (20, size of the hidden layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTMs\n",
    "\n",
    "Long short-term memory (LSTMs) are a variant of RNNs that use more complex units for activation. Similar to the spirit of GRU, they are created to have more persistent memory thereby making them easier for RNNs to capture long-term dependencies.\n",
    "\n",
    "LSTM is defined by ``torch.nn.LSTM`` module and its documentation can be fetched [here](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#lstm). Now let us define the LSTM module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "define the LSTM module\n",
    "\"\"\"\n",
    "# first input - number of features in x (300, size of the word embedding)\n",
    "# second input - number of number of nodes in a hidden layer (50)\n",
    "# third input - number of recurrent layers (2)\n",
    "lstm_rnn = nn.LSTM(input_size=300, hidden_size=50, num_layers=2) # input_size, hidden_size, num_layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike RNN and GRU, LSTM module takes **three inputs**: the **initial hidden state** for each element in the batch (t=0), the **input features** (tweet_input_embeddings in our case) and the **initial cell state** for each element in the batch.\n",
    "\n",
    "Let us construct the initial cell state (this construction is similar to that of initial hidden state)\n",
    "> This cell state is somewhat related to how much a cell has to forget."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "cell state at time-step 0 (h_0)\n",
    "\"\"\"\n",
    "# first dimension - number of LSTM layers (2)\n",
    "# second dimension - batch_size (# of tweets/examples/sentences)\n",
    "# third dimension - hidden_size / number of nodes in a hidden layer (50)\n",
    "c0 = torch.randn(2, 4, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us feed the initial hidden state, initial cell state and tweet embeddings to our LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "forward propagation over the LSTM model\n",
    "\"\"\"\n",
    "output, (hn, cn) = lstm_rnn(tweet_input_embeddings, (h0, c0)) # h0 and c0 is optional input, defaults to tensor of 0's when not provided"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``output`` tensor contains the output features $h_t$ from the last layer of the LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output size:  torch.Size([5, 4, 50])\n"
     ]
    }
   ],
   "source": [
    "# output = seq_len, batch_size, hidden_size (output features from last layer of LSTM)\n",
    "print(\"output size: \", output.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``hn`` is a tensor of shape (num_layers, batch, hidden_size) containing the hidden state for t = seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last hidden state size:  torch.Size([2, 4, 50])\n"
     ]
    }
   ],
   "source": [
    "# h_n = num_layers, batch, hidden_size (hidden state for t=seq_len or hidden state at last timestep)\n",
    "print(\"last hidden state size: \", hn.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``cn`` is a tensor of shape (num_layers, batch, hidden_size) containing the cell state for t = seq_len."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last cell state size:  torch.Size([2, 4, 50])\n"
     ]
    }
   ],
   "source": [
    "# c_n = num_layers, batch_size, hidden_size (cell state for t=seq_len or cell state at last timestep)\n",
    "print(\"last cell state size: \", cn.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to RNN and GRU, you can compute the final tweet representation (representation from last hidden state for each tweet) as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet output embeddings size:  torch.Size([4, 50])\n"
     ]
    }
   ],
   "source": [
    "tweet_output_embeddings = output[-1,:,:] # -1 fetches the embeddings from the last timestep\n",
    "print(\"tweet output embeddings size: \", tweet_output_embeddings.size())\n",
    "# first dimension - number of tweets in the batch (4)\n",
    "# second dimension - number of features in hidden state h_t (50, size of the hidden layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3d597f4c481aa0f25dceb95d2a0067e73c0966dcbd003d741d821a7208527ecf"
  },
  "kernelspec": {
   "display_name": "Python (ctrl_transformers)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
