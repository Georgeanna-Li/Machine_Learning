{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLjpbXW5DRBB"
      },
      "source": [
        "# Seq2Seq model and Evaluation metric - Machine Translation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yMxWc16DRBE"
      },
      "source": [
        "### Tutorial Topics\n",
        "- Machine Translation:\n",
        "    - Seq2Seq model\n",
        "    - Evaluation metric\n",
        "\n",
        "### Software Requirements\n",
        "- Python (>=3.6)\n",
        "- PyTorch (>=1.2.0) \n",
        "- Jupyter (latest)\n",
        "- torchtext\n",
        "- NLTK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YoUOTMQF4_QQ",
        "outputId": "65933d0d-619d-4d0e-e79e-b08aff711835"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4a1pOgPENwx"
      },
      "source": [
        "## Seq2Seq model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tk6h6fp2DRBP"
      },
      "source": [
        "In this tutorial, we will introduce a neural network to translate French sentence to English sentence.\n",
        "\n",
        "We will introduce a important architecture in machine translation: [sequence to sequence network](http://arxiv.org/abs/1409.3215), in which two recurrent neural networks work together to transform one sequence (e.g., sentence) to another. An encoder network condenses an input sequence into a **single vector**, and a decoder network unfolds that vector into a new sequence in target language."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpeU8nufDRBP"
      },
      "source": [
        "# Sequence to Sequence Learning\n",
        "\n",
        "A [Sequence to Sequence network](http://arxiv.org/abs/1409.3215), or seq2seq network, or [Encoder Decoder network](https://arxiv.org/pdf/1406.1078v3.pdf), is a model consisting of two separate RNNs called the **`encoder`** and **`decoder`**. The `encoder` reads an input sequence one token at a time, and outputs a vector at each step. The final output of the encoder is kept as the **context** vector. In classification task, we use this **context** vector as the \"summarization\" of input sequence. In seq2seq model, the decoder uses this context vector as the initial state to generate translation. We will discuss the details in the later section.  \n",
        "\n",
        "![](https://i.imgur.com/tVtHhNp.png)\n",
        "\n",
        " Picture Courtesy: https://i.imgur.com/tVtHhNp.png\n",
        " \n",
        "When using a single RNN, there is a one-to-one relationship between `inputs` and `outputs`. But there are not directly one-to-one relationship between source language and target language. \n",
        "\n",
        "Consider a simple sentence \"`Je ne suis pas le chat noir\"` &rarr; \"`I am not the black cat`\". Many of the words have a pretty direct translation, like \"chat\" &rarr; \"cat\". However the differing grammars cause words to be in different orders, e.g. \"chat noir\" and \"black cat\". There is also the \"ne ... pas\" &rarr; \"not\" construction that makes the two sentences have different lengths.\n",
        "\n",
        "With the seq2seq model, by encoding many source inputs into one vector, and decoding from one vector into many target outputs, we are freed from the constraints of sequence order and length. The encoded sequence is represented by a single vector which is a $N$ dimensional representation. In an ideal case, this vector can be considered as the `\"summarization\"` of the sequence.\n",
        "\n",
        "The flow of rest of this tutorial is as follows:\n",
        "1. Preparing data\n",
        "2. Encoder\n",
        "3. Decoder\n",
        "4. Seq2seq\n",
        "5. Training the model\n",
        "6. Loading the trained model checkpoint\n",
        "7. Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9VR524FDRBQ"
      },
      "source": [
        "### Required imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-D5ocUsBDRBR"
      },
      "outputs": [],
      "source": [
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "import time\n",
        "import datetime\n",
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
        "import torchtext\n",
        "from torchtext.legacy import data\n",
        "import spacy\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxjuYtN0DRBU"
      },
      "source": [
        "Here we will also define a constant to decide whether to use the GPU (with CUDA specifically) or the CPU. \n",
        "\n",
        "If you don't have a GPU, set this as CPU. Later when we create tensors, this variable will be used to decide whether we keep them on CPU or move them to GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZONroUKDRBV",
        "outputId": "4c0cef3e-44bd-4a74-fc3f-8e9139128d58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmhYeZCdDRBY"
      },
      "source": [
        "## 1. Preparing Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nh7pBWq_DRBa"
      },
      "source": [
        "***Define tokenizers:***\n",
        "we create the tokenizers. A tokenizer is used to turn a string containing a sentence into a list of individual tokens.\n",
        "\n",
        "`spaCy` has model for each language (\"fr\" for French and \"en\" for English) which need to be loaded so we can access the tokenizer of each model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9DyXb3eDRBa"
      },
      "source": [
        "***Note***: the models must first be downloaded using the following on the command line:\n",
        "\n",
        "```\n",
        "python -m spacy download en_core_web_sm\n",
        "python -m spacy download fr_core_news_sm\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQbDo_trz-yX",
        "outputId": "8347f49b-67ea-4792-e861-b91edc598cfe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en_core_web_sm==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz (12.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.0 MB 2.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.6)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.9.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.21.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.6)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.63.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (57.4.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.11.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2021.10.8)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "Collecting fr_core_news_sm==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-2.2.5/fr_core_news_sm-2.2.5.tar.gz (14.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 14.7 MB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from fr_core_news_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (57.4.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (2.0.6)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (0.9.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (3.0.6)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.21.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (4.63.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.0.6)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->fr_core_news_sm==2.2.5) (4.11.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->fr_core_news_sm==2.2.5) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->fr_core_news_sm==2.2.5) (3.7.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_sm==2.2.5) (2021.10.8)\n",
            "Building wheels for collected packages: fr-core-news-sm\n",
            "  Building wheel for fr-core-news-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fr-core-news-sm: filename=fr_core_news_sm-2.2.5-py3-none-any.whl size=14727025 sha256=a76a0fad8c6f9050b255c35380b6ce28c31b3bee0118e872c78193c2b23007e9\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-n02dqoj2/wheels/c9/a6/ea/0778337c34660027ee67ef3a91fb9d3600b76777a912ea1c24\n",
            "Successfully built fr-core-news-sm\n",
            "Installing collected packages: fr-core-news-sm\n",
            "Successfully installed fr-core-news-sm-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('fr_core_news_sm')\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download fr_core_news_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "htgoVt6HDRBb"
      },
      "outputs": [],
      "source": [
        "import fr_core_news_sm\n",
        "import en_core_web_sm\n",
        "\n",
        "spacy_fr = fr_core_news_sm.load()\n",
        "spacy_en = en_core_web_sm.load()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoOzzEjE3LLa"
      },
      "source": [
        "Next, we create the tokenizer functions. These functions can be provided to TorchText and will take in the sentence as a string and return the sentence as a list of tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5i5mYv6h3L7B"
      },
      "outputs": [],
      "source": [
        "def tokenize_fr(text):\n",
        "    \"\"\"\n",
        "    Tokenizes French text from a string into a list of strings (tokens)\n",
        "    \"\"\"\n",
        "    return [tok.text for tok in spacy_fr.tokenizer(text)]\n",
        "\n",
        "def tokenize_en(text):\n",
        "    \"\"\"\n",
        "    Tokenizes English text from a string into a list of strings (tokens)\n",
        "    \"\"\"\n",
        "    return [tok.text for tok in spacy_en.tokenizer(text)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXWp5zbi3PhB"
      },
      "source": [
        "`TorchText`'s Fields handle how data should be processed. You can read all of the possible arguments [here](https://github.com/pytorch/text/blob/master/torchtext/data/field.py#L61).\n",
        "\n",
        "We set the tokenize argument to the corresponding tokenization function for each, with French being the `SRC` (source) field and English being the `TRG` (target) field. The field also appends the \"start of sequence\" (\\<sos\\>) and \"end of sequence\" (\\<eos\\>) tokens via the `init_token` and `eos_token` arguments, and converts all words to lowercase."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFRXhTeU3OIM"
      },
      "outputs": [],
      "source": [
        "SRC = data.Field(tokenize = tokenize_fr, \n",
        "            init_token = '<sos>', \n",
        "            eos_token = '<eos>', \n",
        "            lower = True)\n",
        "TRG = data.Field(tokenize = tokenize_en, \n",
        "            init_token = '<sos>', \n",
        "            eos_token = '<eos>', \n",
        "            lower = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnOUGpiM3YCm"
      },
      "source": [
        "Next, we load the train, validation and test data.\n",
        "\n",
        "The dataset we'll be using is the [Multi30k](https://github.com/multi30k/dataset) dataset. This is a dataset with ~30,000 parallel English, French and German sentences. The length of sentence is around 12 words. You can find more information in [WMT18](http://www.statmt.org/wmt18/multimodal-task.html). This corpus was officially split to Training (29,000 sentences), Validation (1,014 sentences), and multiple Test sets. We provide Test 2016 (1,000 sentences). \n",
        "\n",
        "The raw dataset is extracted to three `.tsv` files. Each file includes two column, 'English' and 'French'. We use `torchtext.legacy.data.TabularDataset` to load these tsv files. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "opWSiWmN3XKt"
      },
      "outputs": [],
      "source": [
        "train, val, test = data.TabularDataset.splits(\n",
        "    path='./drive/MyDrive/Colab Notebooks/eng-fre/', train='train_eng_fre.tsv',validation='val_eng_fre.tsv', test='test_eng_fre.tsv', \n",
        "    format='tsv', skip_header=True, fields=[('TRG', TRG), ('SRC', SRC)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1WwLAWb5XPt"
      },
      "source": [
        "We can double check that we've loaded the right number of examples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lALu0PWW4J4W",
        "outputId": "ed02a524-88ee-47a1-beda-fcdcb46904e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training examples: 29000\n",
            "Number of validation examples: 1014\n",
            "Number of testing examples: 1000\n"
          ]
        }
      ],
      "source": [
        "print(f\"Number of training examples: {len(train.examples)}\")\n",
        "print(f\"Number of validation examples: {len(val.examples)}\")\n",
        "print(f\"Number of testing examples: {len(test.examples)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkAicvV95kAE"
      },
      "source": [
        "We can also print out an example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1VIBOMQr5X7Q",
        "outputId": "53e4224a-48b1-4961-ed04-5e7138fe45ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'TRG': ['two', 'young', ',', 'white', 'males', 'are', 'outside', 'near', 'many', 'bushes', '.'], 'SRC': ['deux', 'jeunes', 'hommes', 'blancs', 'sont', 'dehors', 'près', 'de', 'buissons', '.']}\n"
          ]
        }
      ],
      "source": [
        "print(vars(train.examples[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJtbuYXD5ltS",
        "outputId": "ece85f0d-9d26-4b52-d23b-d17205c7c6f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'TRG': ['an', 'older', ',', 'overweight', 'man', 'flips', 'a', 'pancake', 'while', 'making', 'breakfast', '.'], 'SRC': ['un', 'homme', 'âgé', 'en', 'surpoids', 'fait', 'sauter', 'une', 'crêpe', 'en', 'préparant', 'le', 'petit', 'déjeuner', '.']}\n"
          ]
        }
      ],
      "source": [
        "print(vars(val.examples[100]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soAYLiNT51fe"
      },
      "source": [
        "Next, we'll build the vocabulary for the source and target languages. \n",
        "\n",
        "The vocabulary is used to associate each unique token with an index and this is used to build a one-hot encoding for each token. The vocabularies of the source and target languages have some minimal overlap (e.g., `critique` and `genre` are used in similar context in both languages)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAC6IHWm51k4"
      },
      "source": [
        "Using the `min_freq` argument, we only allow tokens that appear at least 2 times to appear in our vocabulary. Tokens that appear only once are converted into an `<unk>` (unknown) token.\n",
        "\n",
        "It is important to note that your vocabulary should only be built from the `training set` and not the `validation/test set`. This prevents **\"information leakage\"** into your model, giving you artifically inflated validation/test scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zkltTmyK5o8s"
      },
      "outputs": [],
      "source": [
        "TRG.build_vocab(train,min_freq=2)\n",
        "SRC.build_vocab(train,min_freq=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Bg8RlqE58yw",
        "outputId": "644cba61-34ec-496b-a061-6f5e4f89f227"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique tokens in source (fr) vocabulary: 6462\n",
            "Unique tokens in target (en) vocabulary: 5893\n"
          ]
        }
      ],
      "source": [
        "print(f\"Unique tokens in source (fr) vocabulary: {len(SRC.vocab)}\")\n",
        "print(f\"Unique tokens in target (en) vocabulary: {len(TRG.vocab)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-orLmAvC51oi"
      },
      "source": [
        "`TRG.vocab.stoi` is the dictionary of word to index. For example, the index of `<pad>` is 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0cjuor56E5y",
        "outputId": "ba460845-a018-42b9-ca86-2bbd8914326f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        }
      ],
      "source": [
        "print(TRG.vocab.stoi['<pad>'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEePNI326JF1"
      },
      "source": [
        "The final step of preparing the data is to create the `iterators` to generate batches. These can be iterated on to return a batch of data. The text of both source and target text will be converted to two sequence of corresponding indexes, using the vocabularies.\n",
        "\n",
        "\n",
        "We also need to define a `torch.device`. This indicate whether the input `tensors` should be sent to `GPU` or not. We already defined the `device` variable before. \n",
        "\n",
        "Finally, the output of the iterator will be `padded`. \n",
        "\n",
        "We use a `BucketIterator` to creates batches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exU8AMSw7arW"
      },
      "outputs": [],
      "source": [
        "train_iter, val_iter, test_iter = data.BucketIterator.splits(\n",
        "    (train, val, test), # we pass in the datasets we want the iterator to draw data from\n",
        "    batch_sizes=(16, 256, 256), device = device,\n",
        "    sort_key=lambda x: len(x.SRC), # the BucketIterator needs to be told what function it should use to group the data.\n",
        "    sort_within_batch=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPpt9Fgb7glp"
      },
      "source": [
        "Each batch will include two **tensors**: tensor of source language and tensor of target language. The size of each tensor is **[max_length, batch_size]**. Each example is already **padded** within batch dynamically."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZonpfR9l7c2c",
        "outputId": "9a00cc05-8046-4604-a075-01569db4bcce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor size of source language: torch.Size([15, 16])\n",
            "tensor size of target language: torch.Size([17, 16])\n",
            "the tensor of first example in target language: tensor([  2,   4,   9,  13,   4, 763,  10,   8,   7,  88,  13,   4, 482, 363,\n",
            "         51,   5,   3], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "# batch example of training data\n",
        "for batch in train_iter:\n",
        "    src = batch.SRC\n",
        "    trg = batch.TRG\n",
        "    print('tensor size of source language:', src.shape)\n",
        "    print('tensor size of target language:', trg.shape)\n",
        "    print('the tensor of first example in target language:', trg[:,0])\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df3gTAlzvO1l"
      },
      "source": [
        "We save our Fields for reproducibility. Ensure you create a direction `ckpt` in your drive before running the following code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pHw0elIXvM-S"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "with open(\"./drive/MyDrive/Colab Notebooks/ckpt/TRG.Field\",\"wb\") as f:\n",
        "     pickle.dump(TRG,f)\n",
        "\n",
        "with open(\"./drive/MyDrive/Colab Notebooks/ckpt/SRC.Field\",\"wb\") as f:\n",
        "     pickle.dump(SRC,f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J73SrWVN7nU4"
      },
      "source": [
        "## Building the Seq2Seq Model\n",
        "\n",
        "## 2. Encoder\n",
        "\n",
        "![](https://pytorch.org/tutorials/_images/seq2seq.png)\n",
        "\n",
        "First, we'll build the encoder model that encodes the French sentence. We use a single layer `Uni-directional LSTM`.\n",
        "\n",
        "Similar to the classifiction task (covered in DSCI 572), we only pass the output of embedding layer to the LSTM layer. The LSTM layer returns `outputs`, `hidden` and `cell`. The `hidden` is the final hidden state of LSTM layer (t=seq_len). The `cell` is the final cell state of the LSTM layer (t=seq_len). `hidden` and `cell` can be considered as the **context** representation of source language. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xY9pEqwL7iKs"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, enc_hid_dim,n_layers, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.emb_dim = emb_dim\n",
        "        self.enc_hid_dim = enc_hid_dim\n",
        "        self.dropout = dropout\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        self.lstm = nn.LSTM(emb_dim, enc_hid_dim, n_layers, dropout=dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, src):\n",
        "        \n",
        "        #src = [src len, batch size]\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        \n",
        "        #embedded = [src len, batch size, emb dim]\n",
        "        \n",
        "        outputs, (hidden, cell) = self.lstm(embedded)\n",
        "       \n",
        "        # outputs are always from the top hidden layer, if bidirectional outputs are concatenated.\n",
        "        # outputs shape [sequence_length, batch_size, hidden_dim * num_directions]\n",
        "        # hidden is of shape [num_layers * num_directions, batch_size, hidden_size]\n",
        "        # cell is of shape [num_layers * num_directions, batch_size, hidden_size]\n",
        "        \n",
        "        return hidden, cell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gV9fweNEKNcp"
      },
      "source": [
        "## 3. Decoder\n",
        "\n",
        "![](https://pytorch.org/tutorials/_images/seq2seq.png)\n",
        "\n",
        "Next up is the decoder. Decoder is a `uni-directional LSTM`.\n",
        "\n",
        "\n",
        "At time step $t$, the input of decoder LSTM is embeded word vector of $t$th target word , $y_t$, the previous decoder hidden state, $h_{t-1}$, and the previous decoder hidden cell, $c_{t-1}$.\n",
        "\n",
        "$$h_t, c_t = \\text{DecoderLSTM}(y_t, (h_{t-1}, c_{t-1}))$$\n",
        "\n",
        "Specially, we will use the last `hidden state` and `cell state` of the encoder LSTM as the initial states of decoder LSTM (i.e., $h_{0}, c_{0}$) rather than randomly initialize them. \n",
        "\n",
        "We then pass hidden state of LSTM layer, $h_t$, through the linear layer, $f$, to make a prediction of the next word in the target sentence, $\\hat{y}_{t+1}$. \n",
        "\n",
        "$$\\hat{y}_{t+1} = f(h_t)$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bDL3CtVjKKS3"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, dec_hid_dim, n_layers, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.emb_dim = emb_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.dec_hid_dim = dec_hid_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        self.lstm = nn.LSTM(emb_dim, dec_hid_dim, n_layers, dropout=dropout)\n",
        "        self.fc_out = nn.Linear(dec_hid_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, input, hidden, cell):\n",
        "             \n",
        "        # input is of shape [batch_size]\n",
        "        # hidden is of shape [n_layer * num_directions, batch_size, hidden_size]\n",
        "        # cell is of shape [n_layer * num_directions, batch_size, hidden_size]\n",
        "        \n",
        "        input = input.unsqueeze(0)\n",
        "        \n",
        "        # input shape is [1, batch_size]. reshape is needed rnn expects a rank 3 tensors as input.\n",
        "        # so reshaping to [1, batch_size] means a batch of batch_size each containing 1 index.\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        \n",
        "        #embedded = [1, batch size, emb dim]    \n",
        "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
        "        \n",
        "        # output shape is [sequence_len, batch_size, hidden_dim * num_directions]\n",
        "        # hidden shape is [num_layers * num_directions, batch_size, hidden_dim]\n",
        "        # cell shape is [num_layers * num_directions, batch_size, hidden_dim]\n",
        "\n",
        "        # sequence_len and num_directions will always be 1 in the decoder.\n",
        "        # output shape is [1, batch_size, hidden_dim]\n",
        "        # hidden shape is [num_layers, batch_size, hidden_dim]\n",
        "        # cell shape is [num_layers, batch_size, hidden_dim]\n",
        "        \n",
        "        prediction = self.fc_out(hidden.squeeze(0)) # linear expects as rank 2 tensor as input\n",
        "        # predicted shape is [batch_size, output_dim]\n",
        "        \n",
        "        return prediction, hidden, cell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5sxYN1dYIrs"
      },
      "source": [
        "## 4. Seq2Seq\n",
        "\n",
        "\n",
        "![](https://pytorch.org/tutorials/_images/seq2seq.png)\n",
        "\n",
        "The `encoder` returns both the final `hidden state` and `cell state` to be used as the initial `hidden state` and `cell state` for the `decoder`.\n",
        "\n",
        "Briefly going over all of the steps:\n",
        "- the `outputs` tensor is created to hold all predictions, $\\hat{Y} = \\{\\hat{y_0}, \\hat{y_1} ... \\hat{y_t}\\}$;\n",
        "- the source sequence, $X = \\{x_0,x_1,..., x_t\\}$, is fed into the encoder to receive last hidden state, $h^{Encoder}_t$, and last cell state $c^{Encoder}_t$;\n",
        "- the initial decoder hidden state is set to be the $h^{Encoder}_t$, and the initial decoder cell state is set to be the $c^{Encoder}_t$. (i.e., $h^{Decoder}_0$ = $h^{Encoder}_t$; $c^{Decoder}_0$ = $c^{Encoder}_t$);\n",
        "- we use a batch of `<sos>` tokens as the first `input` (i.e., $y_1$);\n",
        "- we then decode within a loop:\n",
        "\n",
        " for i in range(1,t): t is the maximal length of target language\n",
        "  - inserting the input token $y_i$, previous hidden state, $h^{Decoder}_{i-1}$, and previous cell state, $c^{Decoder}_{i-1}$, into the decoder;\n",
        "  - receiving a prediction, $\\hat{y}_{i+1}$, which is the most likely output sequence, a new hidden state, $h^{Decoder}_{i}$, and a new cell state, $c^{Decoder}_{i}$;\n",
        "  - we then decide if we are going to **teacher force** or not, setting the next input as appropriate, that is, if teacher forcing is on, the next input will be the gold token from the previous timestep, otherwise, the next input will be the predicted token from the previous timestep."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qu6YDrp0Y2Rc"
      },
      "outputs": [],
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    ''' This class contains the implementation of complete sequence to sequence network.\n",
        "    It uses to encoder to produce the context vectors.\n",
        "    It uses the decoder to produce the predicted target sentence.\n",
        "    Args:\n",
        "        encoder: A Encoder class instance.\n",
        "        decoder: A Decoder class instance.\n",
        "    '''\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
        "        # src is of shape [src_sequence_len, batch_size]\n",
        "        # trg is of shape [targ_sequence_len, batch_size]\n",
        "        # if teacher_forcing_ratio is 0.5 we use ground-truth inputs 50% of time and 50% time we use decoder outputs.\n",
        "\n",
        "        batch_size = trg.shape[1]\n",
        "        max_len = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "\n",
        "        # to store the outputs of the decoder\n",
        "        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)\n",
        "\n",
        "        # context vector, last hidden and cell state of encoder to initialize the decoder\n",
        "        hidden, cell = self.encoder(src)\n",
        "\n",
        "        # first input to the decoder is the <sos> tokens\n",
        "        input = trg[0, :]\n",
        "\n",
        "        for t in range(1, max_len):\n",
        "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
        "            outputs[t] = output\n",
        "            # pick a random number between 0 to ratio and decide whether to teacher force\n",
        "            # if the ratio is 1.0, use_teacher_force is always 1 \n",
        "            # if the ratio is 0.0, use_teacher_force is always 0\n",
        "            # if the ration is 0.4, use_teacher_force is 1 for 40% of the time (on an average)\n",
        "            use_teacher_force = random.random() < teacher_forcing_ratio \n",
        "            top1 = output.max(1)[1]\n",
        "            # decide the next token based on use_teacher_force]\n",
        "            # if teacher forcing is on, the next input will be the gold token from the previous timestep\n",
        "            # otherwise, the next input will be the predicted token from the previous timestep.\n",
        "            input = (trg[t] if use_teacher_force else top1) \n",
        "\n",
        "        # outputs is of shape [sequence_len, batch_size, output_dim]\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3NXkVmygfhv"
      },
      "source": [
        "## 5. Training the Seq2Seq Model\n",
        "We instantiate our encoder, decoder and seq2seq model (placing it on the GPU if we have one). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_ZqMyitGX-5",
        "outputId": "cf72e0fa-b627-4736-c48f-ea4442fa0ee5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ]
        }
      ],
      "source": [
        "INPUT_DIM = len(SRC.vocab) # tokens in source vocabulary\n",
        "OUTPUT_DIM = len(TRG.vocab) # tokens in target vocabulary\n",
        "\n",
        "# hyperparameters\n",
        "ENC_EMB_DIM = 256 # encoder embedding size\n",
        "DEC_EMB_DIM = 256 # decoder embedding size\n",
        "ENC_HID_DIM = 512 # encoder hidden size\n",
        "DEC_HID_DIM = 512 # decoder hidden size\n",
        "ENC_DROPOUT = 0.5 # dropout for encoder\n",
        "DEC_DROPOUT = 0.3 # dropout for decoder\n",
        "N_LAYERS = 1 # number of LSTM layers\n",
        "LEARNING_RT = 0.001 # learning rate\n",
        "\n",
        "# model\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, DEC_HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
        "model = Seq2Seq(enc, dec, device).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMIV3Tnsh3lU"
      },
      "source": [
        "We use a simplified version of the **weight initialization scheme**. Here, we will initialize all biases to zero and all weights from $\\mathcal{N}(0, 0.01)$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5tsPrp-GZmh",
        "outputId": "c04bcbfd-d153-4d7c-cc1f-eb9c21c987d8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(6462, 256)\n",
              "    (lstm): LSTM(256, 512, dropout=0.5)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (embedding): Embedding(5893, 256)\n",
              "    (lstm): LSTM(256, 512, dropout=0.3)\n",
              "    (fc_out): Linear(in_features=512, out_features=5893, bias=True)\n",
              "    (dropout): Dropout(p=0.3, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ],
      "source": [
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        if 'weight' in name:\n",
        "            nn.init.zeros_(param.data)\n",
        "        else:\n",
        "            nn.init.constant_(param.data, 0)\n",
        "            \n",
        "model.apply(init_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fMQZFQTiCjx"
      },
      "source": [
        "Calculate the number of parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9w79-_fiByo",
        "outputId": "b04189e7-5de3-4cc6-cdde-a12f8f9594b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 9,339,909 trainable parameters\n"
          ]
        }
      ],
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9aNXTTZiHts"
      },
      "source": [
        "Create an optimizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zkdoXq8aiEoF"
      },
      "outputs": [],
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr = LEARNING_RT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMICykcSiN_k"
      },
      "source": [
        "Initialize the loss function. The pad token needs to be ignored."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xbjJMMSZiL7e",
        "outputId": "802c70c0-1858-4193-9ae4-883212dda4f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<pad> token index:  1\n"
          ]
        }
      ],
      "source": [
        "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
        "print('<pad> token index: ',TRG_PAD_IDX)\n",
        "## we will ignor the pad token in true target set\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eB5clLvrieXF"
      },
      "source": [
        "### Testing Model With a Single Batch\n",
        "We will run the model with first training batch to test our code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4yA7zAVBidXt",
        "outputId": "1b3d8b40-3eb1-4ff4-eacd-dac930aa1fe4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.5426, device='cuda:0', grad_fn=<DivBackward0>)\n"
          ]
        }
      ],
      "source": [
        "clip = 1\n",
        "model.train()\n",
        "\n",
        "for i, batch in enumerate(train_iter):\n",
        "    \n",
        "    # read the source sentence and target sentence\n",
        "    src = batch.SRC\n",
        "    trg = batch.TRG\n",
        "\n",
        "    # clear the gradient buffer\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # forward pass\n",
        "    output = model(src, trg)\n",
        "    #trg = [trg len, batch size]\n",
        "    #output = [trg len, batch size, output dim]\n",
        "\n",
        "    output_dim = output.shape[-1]\n",
        "\n",
        "\n",
        "    # we are reshaping to feed `output` and `trg` into the loss function\n",
        "    output = output[1:].view(-1, output_dim)\n",
        "    trg = trg[1:].view(-1)  # infered one dimension size (like shape(-1))\n",
        "\n",
        "    #output = [(trg len - 1) * batch size, output dim]\n",
        "    #trg = [(trg len - 1) * batch size]\n",
        "    \n",
        "    # compute the loss\n",
        "    loss = criterion(output, trg)\n",
        "    \n",
        "    # compute the gradients\n",
        "    loss.backward()\n",
        "\n",
        "    # clip the gradients to prevent gradient explosion problem\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "    \n",
        "    # update the parameters\n",
        "    optimizer.step()\n",
        "\n",
        "    print(loss/src.shape[1])\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tl_MI5-Hlp60"
      },
      "source": [
        "## Fully training process\n",
        "If we test our code successfully. We will start the fully training loop as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dg1-WRQ3lqRA"
      },
      "outputs": [],
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for i, batch in enumerate(iterator):\n",
        "        \n",
        "        src = batch.SRC\n",
        "        trg = batch.TRG\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output = model(src, trg)\n",
        "        \n",
        "        #trg = [trg len, batch size]\n",
        "        #output = [trg len, batch size, output dim]\n",
        "        \n",
        "        output_dim = output.shape[-1]\n",
        "        \n",
        "        output = output[1:].view(-1, output_dim)\n",
        "        trg = trg[1:].view(-1)\n",
        "        \n",
        "        # loss function works only with 2d logits, 1d targets\n",
        "        # so flatten the trg, output tensors. Ignore the <sos> token\n",
        "        # trg shape should be [(sequence_len - 1) * batch_size]\n",
        "        # output shape should be [(sequence_len - 1) * batch_size, output_dim]\n",
        "        \n",
        "        loss = criterion(output, trg)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSQfQpQclx1C"
      },
      "source": [
        "...and the evaluation loop, remembering to set the model to `eval` mode and turn off teaching forcing (i.e., teach forcing rate = 0)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HIy-h7i5lwd4"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i, batch in enumerate(iterator):\n",
        "\n",
        "            src = batch.SRC\n",
        "            trg = batch.TRG\n",
        "\n",
        "            output = model(src, trg, 0) # turn off teacher forcing\n",
        "\n",
        "            #trg = [trg len, batch size]\n",
        "            #output = [trg len, batch size, output dim]\n",
        "\n",
        "            output_dim = output.shape[-1]\n",
        "            \n",
        "            output = output[1:].view(-1, output_dim)\n",
        "            trg = trg[1:].view(-1)\n",
        "\n",
        "            #trg = [(trg len - 1) * batch size]\n",
        "            #output = [(trg len - 1) * batch size, output dim]\n",
        "\n",
        "            loss = criterion(output, trg)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUeJF_N1NnCD"
      },
      "source": [
        "Count the running time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H3rv8T7Vly1c"
      },
      "outputs": [],
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQ8OyS2aNs3_"
      },
      "source": [
        "## Training model. \n",
        "\n",
        "We will train the model for 10 epochs. At the end of each epoch, we will save a checkpoint and evaluate on the development set. We will print out the loss and perplexity of train and dev set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oaDCA9HOl4XP",
        "outputId": "6d5175c7-1156-400d-ce93-63ebef49c458"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01 | Time: 0m 54s\n",
            "\tTrain Loss: 7.775 | Train PPL: 2380.632\n",
            "\t Val. Loss: 6.975 |  Val. PPL: 1069.662\n",
            "Epoch: 02 | Time: 0m 55s\n",
            "\tTrain Loss: 6.424 | Train PPL: 616.720\n",
            "\t Val. Loss: 5.916 |  Val. PPL: 370.798\n",
            "Epoch: 03 | Time: 0m 55s\n",
            "\tTrain Loss: 5.687 | Train PPL: 294.931\n",
            "\t Val. Loss: 5.459 |  Val. PPL: 234.816\n",
            "Epoch: 04 | Time: 0m 54s\n",
            "\tTrain Loss: 5.440 | Train PPL: 230.340\n",
            "\t Val. Loss: 5.355 |  Val. PPL: 211.663\n",
            "Epoch: 05 | Time: 0m 55s\n",
            "\tTrain Loss: 5.392 | Train PPL: 219.726\n",
            "\t Val. Loss: 5.332 |  Val. PPL: 206.786\n",
            "Epoch: 06 | Time: 0m 55s\n",
            "\tTrain Loss: 5.379 | Train PPL: 216.819\n",
            "\t Val. Loss: 5.321 |  Val. PPL: 204.669\n",
            "Epoch: 07 | Time: 0m 55s\n",
            "\tTrain Loss: 5.373 | Train PPL: 215.428\n",
            "\t Val. Loss: 5.316 |  Val. PPL: 203.503\n",
            "Epoch: 08 | Time: 0m 55s\n",
            "\tTrain Loss: 5.369 | Train PPL: 214.650\n",
            "\t Val. Loss: 5.312 |  Val. PPL: 202.813\n",
            "Epoch: 09 | Time: 0m 54s\n",
            "\tTrain Loss: 5.367 | Train PPL: 214.203\n",
            "\t Val. Loss: 5.310 |  Val. PPL: 202.386\n",
            "Epoch: 10 | Time: 0m 55s\n",
            "\tTrain Loss: 5.366 | Train PPL: 213.920\n",
            "\t Val. Loss: 5.308 |  Val. PPL: 202.029\n"
          ]
        }
      ],
      "source": [
        "N_EPOCHS = 10\n",
        "CLIP = 1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss = train(model, train_iter, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, val_iter, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    # Create checkpoint at end of each epoch\n",
        "    state_dict_model = model.state_dict() \n",
        "    state = {\n",
        "        'epoch': epoch,\n",
        "        'state_dict': state_dict_model,\n",
        "        'optimizer': optimizer.state_dict()\n",
        "        }\n",
        "\n",
        "    torch.save(state, \"./drive/MyDrive/Colab Notebooks/ckpt/seq2seq_\"+str(epoch+1)+\".pt\")\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7oPEKhFwIR3"
      },
      "source": [
        "## 6. Load Checkpoint\n",
        "We will use the best model for the following process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDDP7hB8wZId"
      },
      "source": [
        "Load the saved TRG and SRC fields:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3kAeYOfCwW_U"
      },
      "outputs": [],
      "source": [
        "with open(\"./drive/MyDrive/Colab Notebooks/ckpt/TRG.Field\",\"rb\") as f:\n",
        "     TRG_saved = pickle.load(f)\n",
        "\n",
        "with open(\"./drive/MyDrive/Colab Notebooks/ckpt/SRC.Field\",\"rb\") as f:\n",
        "     SRC_saved = pickle.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGMLkVAzzLIW"
      },
      "source": [
        "Load trained model to `model_best` and put model on device."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCp-dPjIzP3p",
        "outputId": "cb93240f-9fd7-4b9e-ced3-4ebf56ec1ec4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ]
        }
      ],
      "source": [
        "INPUT_DIM = len(SRC_saved.vocab)\n",
        "OUTPUT_DIM = len(TRG_saved.vocab)\n",
        "ENC_EMB_DIM = 256\n",
        "DEC_EMB_DIM = 256\n",
        "ENC_HID_DIM = 512\n",
        "DEC_HID_DIM = 512\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.3\n",
        "N_LAYERS = 1\n",
        "LEARNING_RT = 0.001\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, DEC_HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
        "\n",
        "model_best = Seq2Seq(enc, dec, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wBmOZKhlzP6x"
      },
      "outputs": [],
      "source": [
        "model_best.load_state_dict(torch.load('./drive/MyDrive/Colab Notebooks/ckpt/seq2seq_10.pt')['state_dict'])\n",
        "model_best = model_best.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fR52wcB71MTN"
      },
      "source": [
        "Pre-process source language and get input tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2B2UZ-Aj0_7P",
        "outputId": "1ebedffd-7050-4b37-f75a-aa6d6fdd3c51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "src_token: ['une', 'femme', 'avec', 'un', 'gros', 'sac', 'passe', 'par', 'une', 'porte', '.']\n",
            "shape of source language:  torch.Size([13, 1])\n"
          ]
        }
      ],
      "source": [
        "model_best.eval()\n",
        "src_token = SRC_saved.preprocess('Une femme avec un gros sac passe par une porte.')\n",
        "print(\"src_token:\", src_token)\n",
        "src_tensor = SRC_saved.process([src_token],device=device)\n",
        "print(\"shape of source language: \", src_tensor.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4x0XCWoY1zPZ"
      },
      "source": [
        "We assume this is a test sample so we don't have the gold target text. We create a placeholder for target language which only includes 64 `<pad>` tokens (i.e., the maximal length of our translated generation is 64)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AplyDtKz1KS9",
        "outputId": "eaedd7a7-afda-4274-a976-6bb3b7b7c1b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of target language:  torch.Size([66, 1])\n"
          ]
        }
      ],
      "source": [
        "trg_token = ['<pad>']*64\n",
        "trg_tensor = TRG_saved.process([trg_token],device=device)\n",
        "print(\"shape of target language: \", trg_tensor.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hAg8yOj61HKL",
        "outputId": "5294cf03-e4b6-4e34-efc6-4f137a45ce03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of output translate:  torch.Size([65, 1, 5893])\n"
          ]
        }
      ],
      "source": [
        "output = model_best(src_tensor, trg_tensor, teacher_forcing_ratio = 0.0)\n",
        "output_dim = output.shape[-1]\n",
        "# get translation results, we ignore first token <sos> in both translation and target sentences. \n",
        "# output_translate = [(trg len - 1), batch, output dim] output dim is size of target vocabulary. \n",
        "output_translate = output[1:]\n",
        "print(\"shape of output translate: \", output_translate.size())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9iSIpaV5bzr"
      },
      "source": [
        "Detach the source input tensor to CPU device because our following process will operate on CPU. Then, squeeze the shape of `output_translate` to `[(trg len - 1),output dim]`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KtXb6Cz75C09",
        "outputId": "1414245b-11b4-484b-e942-50ef1a573a31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "token indices in source language:  [  2   6  19  15   4 185 150 211  64   6 109   5   3]\n",
            "shape of logits for each prediction token in target language torch.Size([65, 5893])\n"
          ]
        }
      ],
      "source": [
        "source_language_token_ids = src_tensor[:,0].cpu().numpy()\n",
        "print(\"token indices in source language: \", source_language_token_ids)\n",
        "translation_logit = output_translate[:,0,:].squeeze(1)\n",
        "print(\"shape of logits for each prediction token in target language\", translation_logit.size())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80F7qWb05Prb",
        "outputId": "a89f0664-2a76-45ec-87ab-464eefec88d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of unnormalized logits corresponding to the top prediction for each prediction token =  torch.Size([65, 1])\n",
            "token indices in target language:  [4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
            " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4]\n"
          ]
        }
      ],
      "source": [
        "# Choose top 1 word from decoder's output, we get the probability and index of the word\n",
        "prob, token_id = translation_logit.data.topk(1)\n",
        "print(\"shape of unnormalized logits corresponding to the top prediction for each prediction token = \", prob.size())\n",
        "target_language_token_ids_along_with_pad = token_id.squeeze(1).cpu().numpy()\n",
        "print(\"token indices in target language: \", target_language_token_ids_along_with_pad) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Ns0cqJA1G3X"
      },
      "source": [
        "To get the translation in text, we will use a dictionary to convert index to word.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_tawtQC4XP8",
        "outputId": "b6915991-bf1d-42cd-9fca-9678f74ca26d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source language: ['<sos>', 'une', 'femme', 'avec', 'un', 'gros', 'sac', 'passe', 'par', 'une', 'porte', '.']\n",
            "Our model translation:  a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a\n",
            "Gold translation:  a woman with a large purse is walking by a gate.\n"
          ]
        }
      ],
      "source": [
        "# get source langauge in text\n",
        "src_language_token_strs = []\n",
        "for i in source_language_token_ids:\n",
        "    if i == SRC.vocab.stoi['<eos>']:\n",
        "        break\n",
        "    else:\n",
        "        token = SRC.vocab.itos[i]\n",
        "        src_language_token_strs.append(token)\n",
        "print(\"Source language:\", src_language_token_strs)\n",
        "\n",
        "# get machine translation in text\n",
        "trans_language = []\n",
        "for i in target_language_token_ids_along_with_pad:\n",
        "    if i == TRG.vocab.stoi['<eos>']:\n",
        "        break\n",
        "    else:\n",
        "        token = TRG.vocab.itos[i]\n",
        "        trans_language.append(token)\n",
        "print(\"Our model translation: \",  ' '.join(trans_language))\n",
        "print(\"Gold translation: \", \"a woman with a large purse is walking by a gate.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86njWz6Z_m1a"
      },
      "source": [
        "## 7. Evaluation \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1WSy4bF_pm8"
      },
      "source": [
        "### Evaluation with perplexity\n",
        "\n",
        "[Perplexity](https://en.wikipedia.org/wiki/Perplexity) is an information theoretic measurement of how well probability model predicts a sample. Low perplexity values indicate a better fit.\n",
        "\n",
        "In language modeling, we build a language generation system with state distribution `q` which tries to mimic the real-world language with the state distribution `p` as much as possible. However, in practice we do not know the exact `p`, we only know $\\tilde{p}$  which is a sampled distribution (i.e., our dataset) from the real-world system. We use a perplexity with cross entropy $PPL( \\tilde{p} ,q)$. Perplexity is defined as: \n",
        "\n",
        "$PPL( \\tilde{p} ,q) = e^{Entropy[\\tilde{p},q]}$\n",
        "\n",
        "where $Entropy[\\tilde{p},q]$ is the `CrossEntropy` loss of our translation model, q is our translation system.\n",
        "We can directly use `math.exp(train_loss)` get the perplexity of our model. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFxWpf6R_gHo"
      },
      "source": [
        "### Evaluation with BLEU score\n",
        "\n",
        "The Bilingual Evaluation Understudy Score, or BLEU for short, is a metric for evaluating a generated sentence to a reference sentence.\n",
        "\n",
        "NLTK provides the [sentence_bleu()](http://www.nltk.org/api/nltk.translate.html#nltk.translate.bleu_score.sentence_bleu) function for evaluating a candidate sentence against one or more reference sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uli2x7KsCUZZ"
      },
      "outputs": [],
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6U9KrtSN1YT"
      },
      "source": [
        " Let's take a look at a simple example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbrmfOKRN9BI",
        "outputId": "83eac9f3-f051-46d9-b818-74d55783d465"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU score: 1.0\n"
          ]
        }
      ],
      "source": [
        "# two references for one document\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "references = [[['this', 'is', 'a', 'test'], ['this', 'is' 'test']]]\n",
        "candidates = [['this', 'is', 'a', 'test']]\n",
        "score = corpus_bleu(references, candidates)\n",
        "print(\"BLEU score:\",score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MOjyg8AOAHP"
      },
      "source": [
        "By default, the sentence_bleu() and corpus_bleu() scores calculate the cumulative 4-gram BLEU score, also called **BLEU-4**. The weights for the BLEU-4 are 1/4 (25%) or 0.25 for each of the 1-gram, 2-gram, 3-gram and 4-gram scores. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xU-NaJr0N9wf",
        "outputId": "a692567b-c0aa-4077-8934-1a4f894ad6b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokens:\t ['a', 'woman', 'with', 'a', 'large', 'purse', 'is', 'walking', 'by', 'a', 'gate', '.']\n",
            "Our model translation:\t ['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a']\n",
            "BLEU score: 0.463502386414385\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ]
        }
      ],
      "source": [
        "# now, use BLEU evaluate our translation\n",
        "\n",
        "# tokenize our golden sentence\n",
        "tokens = TRG.preprocess(\"a woman with a large purse is walking by a gate.\")\n",
        "print(\"tokens:\\t\", tokens)\n",
        "references = [[tokens]]\n",
        "print(\"Our model translation:\\t\", trans_language)\n",
        "our_translation = [trans_language]\n",
        "\n",
        "score = corpus_bleu(references, our_translation) # same as corpus_bleu(reference, our_translation, weights=(0.25, 0.25, 0.25, 0.25))\n",
        "print(\"BLEU score:\",score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NX7COgIhOPnt"
      },
      "source": [
        "The cumulative and individual 1-gram BLEU use the same weights, e.g. (1, 0, 0, 0). The 2-gram weights assign a 50% to each of 1-gram and 2-gram and the 3-gram weights are 33% for each of the 1, 2 and 3-gram scores.\n",
        "\n",
        "Let’s make this concrete by calculating the cumulative scores for BLEU-1, BLEU-2, BLEU-3 and BLEU-4:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9OZ3SxrOQCb",
        "outputId": "d2985dbd-91d3-46ac-dd92-87403b29fc45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cumulative 1-gram: 0.046154\n",
            "Cumulative 2-gram: 0.214834\n",
            "Cumulative 3-gram: 0.362400\n",
            "Cumulative 4-gram: 0.463502\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ]
        }
      ],
      "source": [
        "# cumulative BLEU scores\n",
        "print('Cumulative 1-gram: %f' % corpus_bleu(references, our_translation, weights=(1, 0, 0, 0)))\n",
        "print('Cumulative 2-gram: %f' % corpus_bleu(references, our_translation, weights=(0.5, 0.5, 0, 0)))\n",
        "print('Cumulative 3-gram: %f' % corpus_bleu(references, our_translation, weights=(0.33, 0.33, 0.33, 0)))\n",
        "print('Cumulative 4-gram: %f' % corpus_bleu(references, our_translation, weights=(0.25, 0.25, 0.25, 0.25)))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is an integrated `inference` function which is used as an overall evaluation of BLEU score for any given model with a corpus."
      ],
      "metadata": {
        "id": "_p9u44lrIoM2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def inference(model, file_name, src_vocab, trg_vocab, attention=False, max_trg_len = 64):\n",
        "    '''\n",
        "    Function for translation inference\n",
        "\n",
        "    Input: \n",
        "    model: translation model;\n",
        "    file_name: the directoy of test file that the first column is target reference, and the second column is source language;\n",
        "    trg_vocab: Target torchtext Field\n",
        "    attention: the model returns attention weights or not.\n",
        "    max_trg_len: the maximal length of translation text (optinal), default = 64\n",
        "\n",
        "    Output:\n",
        "    Corpus BLEU score.\n",
        "    '''\n",
        "    from nltk.translate.bleu_score import corpus_bleu\n",
        "    from nltk.translate.bleu_score import sentence_bleu\n",
        "    from torchtext.legacy.data import TabularDataset\n",
        "    from torchtext.legacy.data import Iterator\n",
        "\n",
        "    # convert index to text string\n",
        "    def convert_itos(convert_vocab, token_ids):\n",
        "        list_string = []\n",
        "        for i in token_ids:\n",
        "            if i == convert_vocab.vocab.stoi['<eos>']:\n",
        "                break\n",
        "            else:\n",
        "                token = convert_vocab.vocab.itos[i]\n",
        "                list_string.append(token)\n",
        "        return list_string\n",
        "\n",
        "    test = TabularDataset(\n",
        "      path=file_name, # the root directory where the data lies\n",
        "      format='tsv',\n",
        "      skip_header=True, # if your tsv file has a header, make sure to pass this to ensure it doesn't get proceesed as data!\n",
        "      fields=[('TRG', trg_vocab), ('SRC', src_vocab)])\n",
        "\n",
        "    test_iter = Iterator(\n",
        "    dataset = test, # we pass in the datasets we want the iterator to draw data from\n",
        "    sort = False,batch_size=128,\n",
        "    sort_key=None,\n",
        "    shuffle=False,\n",
        "    sort_within_batch=False,\n",
        "    device = device,\n",
        "    train=False\n",
        "    )\n",
        "  \n",
        "    model.eval()\n",
        "    all_trg = []\n",
        "    all_translated_trg = []\n",
        "\n",
        "    TRG_PAD_IDX = trg_vocab.vocab.stoi[trg_vocab.pad_token]\n",
        "\n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i, batch in enumerate(test_iter):\n",
        "\n",
        "            src = batch.SRC\n",
        "            #src = [src len, batch size]\n",
        "\n",
        "            trg = batch.TRG\n",
        "            #trg = [trg len, batch size]\n",
        "\n",
        "            batch_size = trg.shape[1]\n",
        "\n",
        "            # create a placeholder for traget language with shape of [max_trg_len, batch_size] where all the elements are the index of <pad>. Then send to device\n",
        "            trg_placeholder = torch.Tensor(max_trg_len, batch_size)\n",
        "            trg_placeholder.fill_(TRG_PAD_IDX)\n",
        "            trg_placeholder = trg_placeholder.long().to(device)\n",
        "            if attention == True:\n",
        "                output,_ = model(src, trg_placeholder, 0) #turn off teacher forcing\n",
        "            else:\n",
        "                #original \n",
        "                #output,_ = model(src, trg_placeholder, 0) #turn off teacher forcing\n",
        "                \n",
        "                # update:\n",
        "                output = model(src, trg_placeholder, 0) #turn off teacher forcing\n",
        "            # get translation results, we ignor first token <sos> in both translation and target sentences. \n",
        "            # output_translate = [(trg len - 1), batch, output dim] output dim is size of target vocabulary.\n",
        "            output_translate = output[1:]\n",
        "            # store gold target sentences to a list \n",
        "            all_trg.append(trg[1:].cpu())\n",
        "\n",
        "            # Choose top 1 word from decoder's output, we get the probability and index of the word\n",
        "            prob, token_id = output_translate.data.topk(1)\n",
        "            translation_token_id = token_id.squeeze(2).cpu()\n",
        "\n",
        "            # store gold target sentences to a list \n",
        "            all_translated_trg.append(translation_token_id)\n",
        "      \n",
        "    all_gold_text = []\n",
        "    all_translated_text = []\n",
        "    for i in range(len(all_trg)): \n",
        "        cur_gold = all_trg[i]\n",
        "        cur_translation = all_translated_trg[i]\n",
        "        for j in range(cur_gold.shape[1]):\n",
        "            gold_convered_strings = convert_itos(trg_vocab,cur_gold[:,j])\n",
        "            trans_convered_strings = convert_itos(trg_vocab,cur_translation[:,j])\n",
        "\n",
        "            all_gold_text.append(gold_convered_strings)\n",
        "            all_translated_text.append(trans_convered_strings)\n",
        "\n",
        "    corpus_all_gold_text = [[item] for item in all_gold_text]\n",
        "    corpus_bleu_score = corpus_bleu(corpus_all_gold_text, all_translated_text)  \n",
        "    return corpus_bleu_score"
      ],
      "metadata": {
        "id": "O40g6OOzAvSz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inference(model_best, './drive/MyDrive/Colab Notebooks/eng-fre/test_eng_fre.tsv', SRC, TRG, attention=False, max_trg_len = 64)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7NdU7r3_AxBd",
        "outputId": "c8d9152e-e730-4414-a174-32f5974c5010"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4024087109973494"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTBzai_C9tN0"
      },
      "source": [
        "## Reference \n",
        "* https://pytorch.org/docs/stable/nn.html\n",
        "* https://github.com/bentrevett/pytorch-seq2seq/blob/master/1%20-%20Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb\n",
        "* https://arxiv.org/abs/1409.3215\n",
        "* https://github.com/graviraja/seq2seq\n",
        "* https://github.com/eladhoffer/seq2seq.pytorch\n",
        "* https://github.com/spro/practical-pytorch/tree/master/seq2seq-translation\n",
        "* http://mlexplained.com/2018/02/08/a-comprehensive-tutorial-to-torchtext/\n",
        "* https://machinelearningmastery.com/calculate-bleu-score-for-text-python/\n",
        "* https://leimao.github.io/blog/Entropy-Perplexity/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7lQdLyN6977"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "seq2seq_tutorial copy1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}